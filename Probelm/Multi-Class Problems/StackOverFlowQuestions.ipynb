{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "id": "EmUJw6UBmXAY",
    "outputId": "6588db99-dafc-4db6-a5c5-057c27487629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: iterative-stratification in c:\\users\\clive\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.1.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\clive\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from iterative-stratification) (0.23.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\clive\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from iterative-stratification) (1.4.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\clive\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from iterative-stratification) (1.18.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\clive\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->iterative-stratification) (2.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\clive\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->iterative-stratification) (0.13.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\clive\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "efc9ed37385c41d566636af65781dc63725909c7",
    "colab_type": "text",
    "id": "VIivi6XPGwu7"
   },
   "source": [
    "# Multi-label text classification with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "colab_type": "code",
    "id": "xTJVS1DtGwu9",
    "outputId": "1d5a5fb5-f955-45ad-a161-6c103e1af098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'df_questions', 'StackOverFlowQuestions.ipynb', 'Tags.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score,hamming_loss,accuracy_score\n",
    "\n",
    "import os\n",
    "print(os.listdir())\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-Bfvjk9lszl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "colab_type": "code",
    "id": "NFmHV5RUSOvz",
    "outputId": "c48c3faa-d8f8-48c2-9da0-441fd85c0090"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tags</th>\n",
       "      <th>procesed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010-07-19T19:14:44Z</td>\n",
       "      <td>272</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>Last year, I read a blog post from Brendan O'C...</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>[machine-learning]</td>\n",
       "      <td>the two cultur statist machin learn last year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2010-07-19T19:24:36Z</td>\n",
       "      <td>4</td>\n",
       "      <td>Forecasting demographic census</td>\n",
       "      <td>What are some of the ways to forecast demograp...</td>\n",
       "      <td>Forecasting demographic census What are some o...</td>\n",
       "      <td>[forecasting]</td>\n",
       "      <td>forecast demograph censu what way forecast de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2010-07-19T19:25:39Z</td>\n",
       "      <td>208</td>\n",
       "      <td>Bayesian and frequentist reasoning in plain En...</td>\n",
       "      <td>How would you describe in plain English the ch...</td>\n",
       "      <td>Bayesian and frequentist reasoning in plain En...</td>\n",
       "      <td>[bayesian]</td>\n",
       "      <td>bayesian frequentist reason plain english how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2010-07-19T19:28:44Z</td>\n",
       "      <td>138</td>\n",
       "      <td>What is the meaning of p values and t values i...</td>\n",
       "      <td>After taking a statistics course and then tryi...</td>\n",
       "      <td>What is the meaning of p values and t values i...</td>\n",
       "      <td>[hypothesis-testing, t-test, p-value, interpre...</td>\n",
       "      <td>what mean valu valu statist test after take s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2010-07-19T19:31:47Z</td>\n",
       "      <td>58</td>\n",
       "      <td>Examples for teaching: Correlation does not me...</td>\n",
       "      <td>There is an old saying: \"Correlation does not ...</td>\n",
       "      <td>Examples for teaching: Correlation does not me...</td>\n",
       "      <td>[correlation]</td>\n",
       "      <td>exampl teach correl causat there correl causa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  OwnerUserId          CreationDate  Score  \\\n",
       "0   6          5.0  2010-07-19T19:14:44Z    272   \n",
       "1  21         59.0  2010-07-19T19:24:36Z      4   \n",
       "2  22         66.0  2010-07-19T19:25:39Z    208   \n",
       "3  31         13.0  2010-07-19T19:28:44Z    138   \n",
       "4  36          8.0  2010-07-19T19:31:47Z     58   \n",
       "\n",
       "                                               Title  \\\n",
       "0  The Two Cultures: statistics vs. machine learn...   \n",
       "1                     Forecasting demographic census   \n",
       "2  Bayesian and frequentist reasoning in plain En...   \n",
       "3  What is the meaning of p values and t values i...   \n",
       "4  Examples for teaching: Correlation does not me...   \n",
       "\n",
       "                                                Body  \\\n",
       "0  Last year, I read a blog post from Brendan O'C...   \n",
       "1  What are some of the ways to forecast demograp...   \n",
       "2  How would you describe in plain English the ch...   \n",
       "3  After taking a statistics course and then tryi...   \n",
       "4  There is an old saying: \"Correlation does not ...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  The Two Cultures: statistics vs. machine learn...   \n",
       "1  Forecasting demographic census What are some o...   \n",
       "2  Bayesian and frequentist reasoning in plain En...   \n",
       "3  What is the meaning of p values and t values i...   \n",
       "4  Examples for teaching: Correlation does not me...   \n",
       "\n",
       "                                                Tags  \\\n",
       "0                                 [machine-learning]   \n",
       "1                                      [forecasting]   \n",
       "2                                         [bayesian]   \n",
       "3  [hypothesis-testing, t-test, p-value, interpre...   \n",
       "4                                      [correlation]   \n",
       "\n",
       "                                       procesed_text  \n",
       "0   the two cultur statist machin learn last year...  \n",
       "1   forecast demograph censu what way forecast de...  \n",
       "2   bayesian frequentist reason plain english how...  \n",
       "3   what mean valu valu statist test after take s...  \n",
       "4   exampl teach correl causat there correl causa...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start from here\n",
    "import pickle as pk\n",
    "\n",
    "df_questions = pk.load(open('df_questions','rb'))\n",
    "\n",
    "df_questions.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4vFZcgPiSTCH"
   },
   "source": [
    "# Post Processing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. when word len < 2 is removed it produces the best results with accuracy of 50 for train and 48 for test \n",
    "2. when word len < 3 is remove the accuracy reduces to train acc. = 48 and test acc = 45\n",
    "3. when word len < 4 is removed the accuracy reduces to train acc = 46 and test acc = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "vmhsEmZJSV7F",
    "outputId": "68877748-0c1f-4fcc-92bf-64b4eb6def76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2715669351211727\n"
     ]
    }
   ],
   "source": [
    "# need to do post processing \n",
    "\n",
    "counter_words2remove = 0\n",
    "counter_total_words = 0\n",
    "for text in df_questions['procesed_text']:\n",
    "\n",
    "  for words in text.split():\n",
    "\n",
    "    if len(words) <= 2:\n",
    "      counter_words2remove += 1\n",
    "\n",
    "    counter_total_words += 1  \n",
    "\n",
    "print(counter_words2remove /  counter_total_words)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "70_1qN6hSWRu",
    "outputId": "f3a3a760-e157-4504-e9c4-78da9b6a4983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of words 8446993\n",
      "no of words 6153069\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def post_process(x):\n",
    "  ls = [] \n",
    "  for word in x.split():\n",
    "\n",
    "      if len(word) >2:\n",
    "        ls.append(word)\n",
    "\n",
    "  return ' '.join(ls)\n",
    "\n",
    "\n",
    "\n",
    "df_questions['post_processed_text'] = df_questions['procesed_text'].map(post_process)\n",
    "\n",
    "#printing the word reduction\n",
    "counter_total_words = 0\n",
    "\n",
    "for text in df_questions['procesed_text']:\n",
    "\n",
    "  for words in text.split():\n",
    "\n",
    "    counter_total_words += 1  \n",
    "\n",
    "print('no of words',counter_total_words)\n",
    "\n",
    "counter_total_words = 0\n",
    "\n",
    "for text in df_questions['post_processed_text']:\n",
    "\n",
    "  for words in text.split():\n",
    "\n",
    "\n",
    "    counter_total_words += 1  \n",
    "\n",
    "print('no of words',counter_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6dZ8EEFoSWOW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RAMiHh6VSIlw"
   },
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CaqPCwoH4BoT"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "# for labels \n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(df_questions.Tags)\n",
    "Y = multilabel_binarizer.transform(df_questions.Tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm \n",
    "\n",
    "# def is_same(x1,x2):\n",
    "    \n",
    "#     return sum(x1 == x2) == 100\n",
    "\n",
    "\n",
    "\n",
    "# for index,samples in tqdm.tqdm(enumerate(Y)):\n",
    "    \n",
    "#     count = []\n",
    "#     indexs = []\n",
    "#     for idx,x1 in enumerate(Y):\n",
    "        \n",
    "#         if is_same(x1,samples) == True:\n",
    "#             indexs.append(idx)\n",
    "#             count.append(True)\n",
    "    \n",
    "    \n",
    "#     print(f'sample {index + 1} is has {sum(count)-1} copies')\n",
    "#     print(indexs)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating class weights\n",
    "\n",
    "df_tags = pd.read_csv('Tags.csv')\n",
    "\n",
    "num_classes = 100\n",
    "grouped_tags = df_tags.groupby(\"Tag\").size().reset_index(name='count')\n",
    "most_common_tags = grouped_tags.nlargest(num_classes, columns=\"count\")\n",
    "df_tags.Tag = df_tags.Tag.apply(lambda tag : tag if tag in most_common_tags.Tag.values else None)\n",
    "\n",
    "df_tags = df_tags.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152913\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>count</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r</td>\n",
       "      <td>13236</td>\n",
       "      <td>11.552811</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>regression</td>\n",
       "      <td>10959</td>\n",
       "      <td>13.953189</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>machine-learning</td>\n",
       "      <td>6089</td>\n",
       "      <td>25.112991</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>time-series</td>\n",
       "      <td>5559</td>\n",
       "      <td>27.507285</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>probability</td>\n",
       "      <td>4217</td>\n",
       "      <td>36.261086</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>nonlinear-regression</td>\n",
       "      <td>514</td>\n",
       "      <td>297.496109</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>cox-model</td>\n",
       "      <td>510</td>\n",
       "      <td>299.829412</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>monte-carlo</td>\n",
       "      <td>504</td>\n",
       "      <td>303.398810</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>proportion</td>\n",
       "      <td>503</td>\n",
       "      <td>304.001988</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>algorithms</td>\n",
       "      <td>500</td>\n",
       "      <td>305.826000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Tag  count  class_weight  class_id\n",
       "0                      r  13236     11.552811        74\n",
       "1             regression  10959     13.953189        79\n",
       "2       machine-learning   6089     25.112991        41\n",
       "3            time-series   5559     27.507285        98\n",
       "4            probability   4217     36.261086        71\n",
       "..                   ...    ...           ...       ...\n",
       "95  nonlinear-regression    514    297.496109        58\n",
       "96             cox-model    510    299.829412        17\n",
       "97           monte-carlo    504    303.398810        52\n",
       "98            proportion    503    304.001988        72\n",
       "99            algorithms    500    305.826000         0\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = most_common_tags['count'].sum()\n",
    "print(total_count)\n",
    "\n",
    "def get_class_weight(x):\n",
    "\n",
    "    return 1 / (x /  total_count)\n",
    "\n",
    "\n",
    "most_common_tags['class_weight'] = most_common_tags['count'].map(get_class_weight)\n",
    "\n",
    "most_common_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = dict(most_common_tags[['Tag','class_weight']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ErUb6WtmSzuR"
   },
   "source": [
    "# Text Encoding \n",
    "# Text Encoding\n",
    "1. Using (1,2),(2,2),(1,3), ngrams doesnt help \n",
    "2. using  use_idf=1,smooth_idf=1,sublinear_tf=True increase accuracy by 2%\n",
    "3. using max features = 20000 helps reduce \n",
    "  the same effect can be observed using min_df = 2\n",
    "      and max_df = 0.95 (meaning ignore terms that occur in less than 1 doucment and terms that occure in more than 95% of the documents.\n",
    "4. max_features also help in reducing overfitting\n",
    "5. processed_text has a accuracy of 48 \n",
    "5. post_processed_text with n == 3 has a accuracy of 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xPYI1XPStAm"
   },
   "outputs": [],
   "source": [
    "#tfidf \n",
    "# tfv = TfidfVectorizer(min_df=9,max_df=0.70,  max_features=None, \n",
    "#             strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "#             ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "#             stop_words = 'english')\n",
    "\n",
    "#remove words that have appeat in less than 2 documents and appear in more that 95 % \n",
    "\n",
    "tfv = TfidfVectorizer(use_idf=1,smooth_idf=1,sublinear_tf=True,ngram_range = (1,1),\n",
    "#                       min_df = 2,max_df = 0.95,\n",
    "                      max_features = 50000\n",
    "                      \n",
    "                     )\n",
    "words = tfv.fit(df_questions['post_processed_text'])\n",
    "X = tfv.transform(df_questions['post_processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "EM-FEOH_TiKO",
    "outputId": "a7ace1dc-b63b-451e-f4c1-9f312d4bff51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<76365x50000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3410862 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "8NLiB6eqTnhV",
    "outputId": "ad6eaf81-70d2-4715-db52-3c236759f908"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76365, 100)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "\n",
    "# sample_weights = compute_sample_weight(class_weight='balanced', y=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMDv27ZlUG3S"
   },
   "source": [
    "# Splitting Train Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "colab_type": "code",
    "id": "CATk4_RLUGCx",
    "outputId": "5a131f50-7184-4f4e-cbb3-2e6c0fd7eae0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((53455, 50000), (53455, 100))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "trainx,testx,trainy,testy = train_test_split(X,Y,test_size = 0.3,shuffle = True)\n",
    "\n",
    "trainx.shape,trainy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<53455x50000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2637505 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSixU3ErS6k7"
   },
   "source": [
    "# ML model\n",
    "## SGDClassifier with loss = 'log' \n",
    "(using logistic regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDClassifier?\n",
    "hing loss give 30 acc\n",
    "log with l1  38 acc\n",
    "log with l1 svd 45 val acc and 53 train acc (ignore this one)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "cEPXRtNCS5zF",
    "outputId": "70b4ce29-2044-4040-cc64-fe77e14aa4b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "#include \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "estimator = SGDClassifier(\n",
    "                          alpha = 0.00002,\n",
    "                          n_jobs= -1,\n",
    "                          random_state = 909,\n",
    "                          loss = 'log',\n",
    "                          penalty = 'l1',\n",
    "#                           learning_rate = 'invscaling',\n",
    "#                           eta0 = 500,\n",
    "#                           power_t = 0.5,\n",
    "                          \n",
    "                          shuffle = True,\n",
    "                          early_stopping=True,\n",
    "                          \n",
    "                         )\n",
    "clf  = OneVsRestClassifier(estimator,n_jobs= -1)\n",
    "\n",
    "clf.fit(trainx,trainy)\n",
    "\n",
    "\n",
    "print(int(time.time() - t1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i0ztp7FSUTEr"
   },
   "source": [
    "Evaluating the Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "inAmSteDTmd_",
    "outputId": "98fa104f-d4e2-4f5a-805a-7a97c10d86d7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.015188289215227762\n",
      "test loss: 0.015897861195984287\n",
      "train score: 0.4993370867583851\n",
      "test score: 0.476710439355191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,hamming_loss,accuracy_score\n",
    "\n",
    "pred_train = clf.predict(trainx)\n",
    "pred_test = clf.predict(testx)\n",
    "\n",
    "# pred_test_proba = clf.predict_proba(testx)\n",
    "\n",
    "\n",
    "print('train loss:',hamming_loss(trainy,pred_train))\n",
    "print('test loss:',hamming_loss(testy,pred_test))\n",
    "\n",
    "\n",
    "print('train score:',f1_score(trainy,pred_train,average  = 'micro'))\n",
    "print('test score:',f1_score(testy,pred_test,average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del clf\n",
    "# del pred_test_proba\n",
    "# del pred_train\n",
    "# del pred_test\n",
    "del X,Y,trainx,testx\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true :  [('anova',)]\n",
      "pred :  [('hypothesis-testing', 't-test')]\n",
      "**********\n",
      "true :  [('bayesian', 'conditional-probability', 'probability')]\n",
      "pred :  [('conditional-probability', 'probability')]\n",
      "**********\n",
      "true :  [('machine-learning', 'mathematical-statistics', 'sampling', 'statistical-significance')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('data-visualization',)]\n",
      "pred :  [('data-visualization',)]\n",
      "**********\n",
      "true :  [('r', 'regression')]\n",
      "pred :  [('generalized-linear-model', 'r')]\n",
      "**********\n",
      "true :  [('statistical-significance',)]\n",
      "pred :  [('statistical-significance',)]\n",
      "**********\n",
      "true :  [('feature-selection', 'regression', 'residuals')]\n",
      "pred :  [('regression',)]\n",
      "**********\n",
      "true :  [('econometrics', 'self-study')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('correlation', 'r', 'time-series')]\n",
      "pred :  [('regression',)]\n",
      "**********\n",
      "true :  [('regression',)]\n",
      "pred :  [('regression',)]\n",
      "**********\n",
      "true :  [('regression', 'regression-coefficients', 'self-study')]\n",
      "pred :  [('regression',)]\n",
      "**********\n",
      "true :  [('logistic',)]\n",
      "pred :  [('logistic',)]\n",
      "**********\n",
      "true :  [('mixed-model', 'r', 'time-series')]\n",
      "pred :  [('mixed-model', 'r')]\n",
      "**********\n",
      "true :  [('clustering',)]\n",
      "pred :  [('r',)]\n",
      "**********\n",
      "true :  [('confidence-interval',)]\n",
      "pred :  [('confidence-interval',)]\n",
      "**********\n",
      "true :  [('bayesian', 'conditional-probability')]\n",
      "pred :  [('bayesian',)]\n",
      "**********\n",
      "true :  [('binary-data', 'r', 'regression', 'statistical-significance')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('binary-data', 'classification', 'cross-validation', 'svm')]\n",
      "pred :  [('classification', 'cross-validation')]\n",
      "**********\n",
      "true :  [('r', 'regression')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('data-mining',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('data-visualization', 'nonparametric', 'repeated-measures')]\n",
      "pred :  [('mean', 'nonparametric')]\n",
      "**********\n",
      "true :  [('multilevel-analysis', 'multiple-regression', 'spss')]\n",
      "pred :  [('multilevel-analysis', 'repeated-measures', 'spss')]\n",
      "**********\n",
      "true :  [('covariance',)]\n",
      "pred :  [('regression',)]\n",
      "**********\n",
      "true :  [('estimation',)]\n",
      "pred :  [('hypothesis-testing', 'panel-data')]\n",
      "**********\n",
      "true :  [('inference', 'maximum-likelihood')]\n",
      "pred :  [('maximum-likelihood',)]\n",
      "**********\n",
      "true :  [('classification', 'dataset', 'machine-learning', 'variance')]\n",
      "pred :  [('machine-learning',)]\n",
      "**********\n",
      "true :  [('spss',)]\n",
      "pred :  [('spss',)]\n",
      "**********\n",
      "true :  [('regression',)]\n",
      "pred :  [('regression',)]\n",
      "**********\n",
      "true :  [('correlation',)]\n",
      "pred :  [('correlation',)]\n",
      "**********\n",
      "true :  [('categorical-data', 'probability')]\n",
      "pred :  [('probability',)]\n",
      "**********\n",
      "true :  [('hypothesis-testing',)]\n",
      "pred :  [('chi-squared',)]\n",
      "**********\n",
      "true :  [('distributions', 'standard-error')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('hypothesis-testing', 'inference', 'self-study', 'statistical-significance')]\n",
      "pred :  [('statistical-significance',)]\n",
      "**********\n",
      "true :  [('correlation', 'hypothesis-testing', 'multivariate-analysis')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('categorical-data', 'least-squares', 'regression', 'self-study')]\n",
      "pred :  [('categorical-data', 'least-squares', 'multiple-regression', 'regression')]\n",
      "**********\n",
      "true :  [('model-selection', 'regression', 'svm')]\n",
      "pred :  [('regression', 'svm')]\n",
      "**********\n",
      "true :  [('distributions', 'probability', 'python')]\n",
      "pred :  [('probability',)]\n",
      "**********\n",
      "true :  [('statistical-significance',)]\n",
      "pred :  [('hypothesis-testing',)]\n",
      "**********\n",
      "true :  [('time-series',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('mean', 't-test')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('variance',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('data-visualization',)]\n",
      "pred :  [('data-visualization',)]\n",
      "**********\n",
      "true :  [('time-series',)]\n",
      "pred :  [('time-series',)]\n",
      "**********\n",
      "true :  [('data-visualization', 'distributions', 'r')]\n",
      "pred :  [('data-visualization',)]\n",
      "**********\n",
      "true :  [('prediction',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('machine-learning', 'r')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('mixed-model', 'repeated-measures', 'time-series')]\n",
      "pred :  [('mixed-model', 'spss')]\n",
      "**********\n",
      "true :  [('optimization',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('deep-learning', 'r')]\n",
      "pred :  [('random-forest',)]\n",
      "**********\n",
      "true :  [('generalized-linear-model', 'poisson', 'residuals')]\n",
      "pred :  [('generalized-linear-model', 'poisson')]\n",
      "**********\n",
      "true :  [('anova', 'spss')]\n",
      "pred :  [('anova', 'repeated-measures', 'spss')]\n",
      "**********\n",
      "true :  [('forecasting', 'generalized-linear-model', 'regression')]\n",
      "pred :  [('r',)]\n",
      "**********\n",
      "true :  [('machine-learning', 'neural-networks')]\n",
      "pred :  [('deep-learning', 'machine-learning', 'neural-networks')]\n",
      "**********\n",
      "true :  [('r', 'time-series')]\n",
      "pred :  [('arima', 'forecasting', 'r', 'time-series')]\n",
      "**********\n",
      "true :  [('regression', 'spss')]\n",
      "pred :  [('spss',)]\n",
      "**********\n",
      "true :  [('machine-learning', 'matlab', 'regression', 'stochastic-processes')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('generalized-linear-model',)]\n",
      "pred :  [('generalized-linear-model', 'logistic', 'r', 'random-forest')]\n",
      "**********\n",
      "true :  [('anova', 'interaction', 'nonparametric', 't-test')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('multilevel-analysis',)]\n",
      "pred :  [('multilevel-analysis', 'regression', 'stata')]\n",
      "**********\n",
      "true :  [('model', 'optimization', 'self-study')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('expected-value',)]\n",
      "pred :  [('self-study',)]\n",
      "**********\n",
      "true :  [('variance',)]\n",
      "pred :  [('variance',)]\n",
      "**********\n",
      "true :  [('r', 'regression')]\n",
      "pred :  [('categorical-data', 'survival')]\n",
      "**********\n",
      "true :  [('bayesian',)]\n",
      "pred :  [('bayesian',)]\n",
      "**********\n",
      "true :  [('clustering', 'survival')]\n",
      "pred :  [('survival',)]\n",
      "**********\n",
      "true :  [('conditional-probability', 'regression')]\n",
      "pred :  [('regression', 'residuals')]\n",
      "**********\n",
      "true :  [('missing-data', 'r', 'random-effects-model')]\n",
      "pred :  [('r',)]\n",
      "**********\n",
      "true :  [('bayesian', 'self-study')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('pca',)]\n",
      "pred :  [('pca',)]\n",
      "**********\n",
      "true :  [('time-series',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('nonparametric',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('logistic',)]\n",
      "pred :  [('logistic',)]\n",
      "**********\n",
      "true :  [('autocorrelation', 'r', 'time-series')]\n",
      "pred :  [('autocorrelation', 'time-series')]\n",
      "**********\n",
      "true :  [('missing-data',)]\n",
      "pred :  [('missing-data',)]\n",
      "**********\n",
      "true :  [('stochastic-processes',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('logistic', 'model', 'regression')]\n",
      "pred :  [('r',)]\n",
      "**********\n",
      "true :  [('normal-distribution',)]\n",
      "pred :  [('covariance',)]\n",
      "**********\n",
      "true :  [('correlation', 'normal-distribution')]\n",
      "pred :  [('correlation',)]\n",
      "**********\n",
      "true :  [('interpretation',)]\n",
      "pred :  [('interpretation', 'poisson', 'regression')]\n",
      "**********\n",
      "true :  [('machine-learning',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('mean', 'sampling', 'standard-error')]\n",
      "pred :  [('standard-error',)]\n",
      "**********\n",
      "true :  [('r',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('bayesian', 'binomial', 'confidence-interval')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('correlation',)]\n",
      "pred :  [('correlation',)]\n",
      "**********\n",
      "true :  [('autocorrelation', 'time-series')]\n",
      "pred :  [('autocorrelation', 'time-series')]\n",
      "**********\n",
      "true :  [('r', 'time-series')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('feature-selection', 'pca')]\n",
      "pred :  [('pca',)]\n",
      "**********\n",
      "true :  [('logistic',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('distributions', 'hypothesis-testing', 'multivariate-analysis', 'nonparametric')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('linear-model', 'multiple-regression')]\n",
      "pred :  [('regression',)]\n",
      "**********\n",
      "true :  [('classification', 'logistic', 'regression')]\n",
      "pred :  [('logistic',)]\n",
      "**********\n",
      "true :  [('interpretation',)]\n",
      "pred :  [('r',)]\n",
      "**********\n",
      "true :  [('covariance', 'maximum-likelihood', 'optimization', 'standard-error')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('data-mining', 'r', 'time-series')]\n",
      "pred :  [('r', 'time-series')]\n",
      "**********\n",
      "true :  [('model-selection',)]\n",
      "pred :  [('logistic', 'model-selection')]\n",
      "**********\n",
      "true :  [('correlation', 'p-value')]\n",
      "pred :  [('correlation',)]\n",
      "**********\n",
      "true :  [('feature-selection', 'r')]\n",
      "pred :  [('r',)]\n",
      "**********\n",
      "true :  [('anova', 'experiment-design', 'r')]\n",
      "pred :  [('anova',)]\n",
      "**********\n",
      "true :  [('bootstrap', 'r')]\n",
      "pred :  [('r',)]\n",
      "**********\n",
      "true :  [('probability',)]\n",
      "pred :  [('probability',)]\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    print('true : ',multilabel_binarizer.inverse_transform(np.array([testy[i]])))\n",
    "    print('pred : ',multilabel_binarizer.inverse_transform(np.array([pred_test[i]])))\n",
    "    \n",
    "    \n",
    "    print(\"**********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('              precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '           0       0.27      0.02      0.04       143\\n'\n",
      " '           1       0.80      0.57      0.67       797\\n'\n",
      " '           2       0.72      0.54      0.62       251\\n'\n",
      " '           3       0.74      0.45      0.56       206\\n'\n",
      " '           4       0.77      0.61      0.68       767\\n'\n",
      " '           5       0.40      0.19      0.26       157\\n'\n",
      " '           6       0.70      0.35      0.47       293\\n'\n",
      " '           7       0.88      0.75      0.81       236\\n'\n",
      " '           8       0.70      0.34      0.46       182\\n'\n",
      " '           9       0.58      0.22      0.32       510\\n'\n",
      " '          10       0.79      0.55      0.65       404\\n'\n",
      " '          11       0.64      0.38      0.47       854\\n'\n",
      " '          12       0.84      0.66      0.74       571\\n'\n",
      " '          13       0.61      0.12      0.21       272\\n'\n",
      " '          14       0.73      0.50      0.60       509\\n'\n",
      " '          15       0.70      0.51      0.59       858\\n'\n",
      " '          16       0.58      0.33      0.42       181\\n'\n",
      " '          17       0.71      0.49      0.58       151\\n'\n",
      " '          18       0.78      0.56      0.65       405\\n'\n",
      " '          19       0.70      0.12      0.21       282\\n'\n",
      " '          20       0.59      0.27      0.37       307\\n'\n",
      " '          21       0.73      0.42      0.53       455\\n'\n",
      " '          22       0.42      0.02      0.04       236\\n'\n",
      " '          23       0.63      0.28      0.39       197\\n'\n",
      " '          24       0.51      0.20      0.29      1060\\n'\n",
      " '          25       0.45      0.10      0.16       289\\n'\n",
      " '          26       0.50      0.14      0.22       440\\n'\n",
      " '          27       0.61      0.27      0.37       178\\n'\n",
      " '          28       0.77      0.28      0.41       253\\n'\n",
      " '          29       0.79      0.50      0.62       183\\n'\n",
      " '          30       0.68      0.32      0.43       301\\n'\n",
      " '          31       0.73      0.47      0.57       413\\n'\n",
      " '          32       0.66      0.36      0.46       492\\n'\n",
      " '          33       0.60      0.15      0.24       185\\n'\n",
      " '          34       0.58      0.30      0.39      1154\\n'\n",
      " '          35       0.48      0.07      0.12       179\\n'\n",
      " '          36       0.76      0.39      0.51       307\\n'\n",
      " '          37       0.47      0.12      0.19       276\\n'\n",
      " '          38       0.45      0.09      0.15       265\\n'\n",
      " '          39       0.25      0.02      0.04       298\\n'\n",
      " '          40       0.79      0.59      0.67      1062\\n'\n",
      " '          41       0.60      0.33      0.42      1845\\n'\n",
      " '          42       0.38      0.01      0.01       565\\n'\n",
      " '          43       0.67      0.38      0.48       274\\n'\n",
      " '          44       0.79      0.52      0.63       385\\n'\n",
      " '          45       0.76      0.51      0.61       188\\n'\n",
      " '          46       0.37      0.04      0.07       267\\n'\n",
      " '          47       0.70      0.35      0.47       186\\n'\n",
      " '          48       0.70      0.51      0.59       575\\n'\n",
      " '          49       0.00      0.00      0.00       168\\n'\n",
      " '          50       0.63      0.18      0.28       246\\n'\n",
      " '          51       0.00      0.00      0.00       306\\n'\n",
      " '          52       0.80      0.38      0.52       154\\n'\n",
      " '          53       0.78      0.36      0.49       174\\n'\n",
      " '          54       0.73      0.27      0.40       250\\n'\n",
      " '          55       0.55      0.16      0.24       585\\n'\n",
      " '          56       0.50      0.15      0.23       326\\n'\n",
      " '          57       0.88      0.68      0.76       591\\n'\n",
      " '          58       0.50      0.06      0.11       162\\n'\n",
      " '          59       0.66      0.28      0.39       275\\n'\n",
      " '          60       0.54      0.27      0.36       648\\n'\n",
      " '          61       0.55      0.21      0.30       300\\n'\n",
      " '          62       0.71      0.40      0.51       209\\n'\n",
      " '          63       0.73      0.51      0.60       170\\n'\n",
      " '          64       0.53      0.06      0.11       319\\n'\n",
      " '          65       0.76      0.53      0.62       287\\n'\n",
      " '          66       0.89      0.79      0.84       433\\n'\n",
      " '          67       0.53      0.26      0.35       188\\n'\n",
      " '          68       0.63      0.44      0.52       240\\n'\n",
      " '          69       0.23      0.04      0.07       255\\n'\n",
      " '          70       0.37      0.03      0.05       380\\n'\n",
      " '          71       0.63      0.27      0.38      1295\\n'\n",
      " '          72       0.59      0.24      0.34       157\\n'\n",
      " '          73       0.59      0.33      0.43       265\\n'\n",
      " '          74       0.73      0.46      0.57      4006\\n'\n",
      " '          75       0.52      0.20      0.29       178\\n'\n",
      " '          76       0.83      0.73      0.78       259\\n'\n",
      " '          77       0.43      0.08      0.13       205\\n'\n",
      " '          78       0.71      0.25      0.37       315\\n'\n",
      " '          79       0.68      0.44      0.53      3292\\n'\n",
      " '          80       0.33      0.07      0.11       191\\n'\n",
      " '          81       0.71      0.41      0.52       420\\n'\n",
      " '          82       0.56      0.37      0.45       187\\n'\n",
      " '          83       0.51      0.21      0.29       256\\n'\n",
      " '          84       0.46      0.15      0.22       400\\n'\n",
      " '          85       0.62      0.16      0.26      1106\\n'\n",
      " '          86       0.60      0.24      0.34       235\\n'\n",
      " '          87       0.69      0.47      0.56       390\\n'\n",
      " '          88       0.58      0.30      0.40       265\\n'\n",
      " '          89       0.65      0.27      0.38       170\\n'\n",
      " '          90       0.65      0.51      0.57       208\\n'\n",
      " '          91       0.54      0.16      0.24       763\\n'\n",
      " '          92       0.57      0.20      0.30       160\\n'\n",
      " '          93       0.56      0.31      0.40       194\\n'\n",
      " '          94       0.82      0.70      0.75       305\\n'\n",
      " '          95       0.89      0.67      0.76       390\\n'\n",
      " '          96       0.51      0.16      0.24       452\\n'\n",
      " '          97       0.62      0.14      0.22       148\\n'\n",
      " '          98       0.79      0.55      0.65      1655\\n'\n",
      " '          99       0.64      0.32      0.43       473\\n'\n",
      " '\\n'\n",
      " '   micro avg       0.69      0.36      0.47     45850\\n'\n",
      " '   macro avg       0.62      0.32      0.40     45850\\n'\n",
      " 'weighted avg       0.64      0.36      0.45     45850\\n'\n",
      " ' samples avg       0.49      0.39      0.41     45850\\n')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\clive\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pprint\n",
    "pprint.pprint(classification_report(testy,pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\clive\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:71: FutureWarning: Pass shuffle=True, random_state=909 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.01814390071479067\n",
      "test loss: 0.020321528241814393\n",
      "train score: 0.48119532381775915\n",
      "test score: 0.4272864232541652\n",
      "train loss: 0.018249990164063423\n",
      "test loss: 0.020386980533469115\n",
      "train score: 0.47961587650609167\n",
      "test score: 0.4283863032353006\n",
      "train loss: 0.018166444322373066\n",
      "test loss: 0.020504469735753948\n",
      "train score: 0.47899036730284483\n",
      "test score: 0.4234189340331993\n"
     ]
    }
   ],
   "source": [
    "#using different method of train test split.\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(shuffle = True,random_state = 909,n_splits = 3)\n",
    "\n",
    "estimator = SGDClassifier(n_jobs= -1,\n",
    "                          random_state = 909,\n",
    "                          loss = 'log',\n",
    "                          penalty = 'l1',\n",
    "#                           l1_ratio = 0.5,\n",
    "                    \n",
    "                          shuffle = False,\n",
    "                          early_stopping=True)\n",
    "\n",
    "clf  = OneVsRestClassifier(estimator,n_jobs= -1)\n",
    "\n",
    "\n",
    "for train_index, test_index in mskf.split(X_std, Y):\n",
    "#    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "     trainx,testx = X_std[train_index],X_std[test_index]\n",
    "     trainy,testy = Y[train_index],Y[test_index] \n",
    "        \n",
    "\n",
    "     clf.fit(trainx,trainy)\n",
    "        \n",
    "        \n",
    "     pred_train = clf.predict(trainx)\n",
    "     pred_test = clf.predict(testx)\n",
    "\n",
    "     print('train loss:',hamming_loss(trainy,pred_train))\n",
    "     print('test loss:',hamming_loss(testy,pred_test))\n",
    "\n",
    "\n",
    "     print('train score:',f1_score(trainy,pred_train,average  = 'micro'))\n",
    "     print('test score:',f1_score(testy,pred_test,average = 'micro'))   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Doq00LHmt0QX"
   },
   "source": [
    "\n",
    "## SGDClassifier with loss = hinge (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "fBBSnObbtoUY",
    "outputId": "72220526-10d2-45da-f3da-7d369a4af1f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.547489166259766\n",
      "train loss: 0.015608081563932279\n",
      "test loss: 0.016456569183762548\n",
      "train score: 0.41289432759360767\n",
      "test score: 0.37297099521021826\n",
      "train score: 0.25845991352825093\n",
      "test score: 0.22647095028689115\n"
     ]
    }
   ],
   "source": [
    "#include \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "estimator = SGDClassifier(n_jobs= -1)\n",
    "\n",
    "clf  = OneVsRestClassifier(estimator,n_jobs= -1)\n",
    "\n",
    "clf.fit(trainx,trainy)\n",
    "\n",
    "\n",
    "pred_train = clf.predict(trainx)\n",
    "pred_test = clf.predict(testx)\n",
    "\n",
    "\n",
    "print(time.time() - t1)\n",
    "\n",
    "print('train loss:',hamming_loss(trainy,pred_train))\n",
    "print('test loss:',hamming_loss(testy,pred_test))\n",
    "\n",
    "# print('train score:',accuracy_score(trainy,pred_train))\n",
    "# print('test score:',accuracy_score(testy,pred_test))\n",
    "\n",
    "print('train score:',f1_score(trainy,pred_train,average  = 'micro'))\n",
    "print('test score:',f1_score(testy,pred_test,average = 'micro'))\n",
    "\n",
    "\n",
    "print('train score:',f1_score(trainy,pred_train,average  = 'macro'))\n",
    "print('test score:',f1_score(testy,pred_test,average = 'macro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RSZebsN589QB"
   },
   "source": [
    "# Using loss = modified_huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "ZP0g9CwHtoPe",
    "outputId": "ee8170b6-4968-4b17-f385-982f126a3578",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.406828165054321\n",
      "train loss: 0.015546721541483491\n",
      "test loss: 0.016058489742470535\n",
      "train score : 0.9844532784585165\n",
      "test score : 0.9839415102575295\n",
      "train score: 0.46370377062615753\n",
      "test score: 0.44471277205904547\n",
      "true :  [('deep-learning', 'machine-learning')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('machine-learning', 'neural-networks', 'terminology')]\n",
      "pred :  [('neural-networks',)]\n",
      "**********\n",
      "true :  [('correlation', 'hypothesis-testing')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('r', 'time-series')]\n",
      "pred :  [('forecasting', 'r')]\n",
      "**********\n",
      "true :  [('maximum-likelihood', 'self-study')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('standard-deviation',)]\n",
      "pred :  [('standard-deviation',)]\n",
      "**********\n",
      "true :  [('machine-learning', 'neural-networks', 'r')]\n",
      "pred :  [('r',)]\n",
      "**********\n",
      "true :  [('regression',)]\n",
      "pred :  [('regression',)]\n",
      "**********\n",
      "true :  [('panel-data',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('correlation', 'expected-value', 'mathematical-statistics', 'probability')]\n",
      "pred :  [('covariance',)]\n",
      "**********\n",
      "true :  [('goodness-of-fit', 'time-series')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('probability',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('distributions', 'probability', 'sampling')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('poisson', 'r', 'regression')]\n",
      "pred :  [('poisson', 'regression')]\n",
      "**********\n",
      "true :  [('machine-learning', 'pdf')]\n",
      "pred :  [('distributions',)]\n",
      "**********\n",
      "true :  [('data-transformation',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('regression',)]\n",
      "pred :  [('regression', 'residuals')]\n",
      "**********\n",
      "true :  [('dataset', 'feature-selection', 'machine-learning')]\n",
      "pred :  [('machine-learning',)]\n",
      "**********\n",
      "true :  [('probability',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('statistical-significance', 'variance')]\n",
      "pred :  [('forecasting',)]\n",
      "**********\n",
      "true :  [('bayesian', 'references')]\n",
      "pred :  [('bayesian',)]\n",
      "**********\n",
      "true :  [('t-test',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('t-test',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('interpretation', 'r')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('nonparametric', 'r')]\n",
      "pred :  [('nonparametric', 'r')]\n",
      "**********\n",
      "true :  [('econometrics', 'regression')]\n",
      "pred :  [('regression',)]\n",
      "**********\n",
      "true :  [('self-study', 'statistical-significance')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('repeated-measures',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('categorical-data', 'hypothesis-testing', 'probability', 'self-study')]\n",
      "pred :  [('self-study',)]\n",
      "**********\n",
      "true :  [('distributions', 'outliers', 'regression')]\n",
      "pred :  [('regression',)]\n",
      "**********\n",
      "true :  [('modeling', 'predictive-models', 'self-study')]\n",
      "pred :  [('logistic',)]\n",
      "**********\n",
      "true :  [('machine-learning',)]\n",
      "pred :  [('classification',)]\n",
      "**********\n",
      "true :  [('distributions', 'probability')]\n",
      "pred :  [('probability',)]\n",
      "**********\n",
      "true :  [('machine-learning',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('logistic',)]\n",
      "pred :  [('logistic', 'stata')]\n",
      "**********\n",
      "true :  [('maximum-likelihood', 'standard-error')]\n",
      "pred :  [('maximum-likelihood',)]\n",
      "**********\n",
      "true :  [('clustering',)]\n",
      "pred :  [('clustering',)]\n",
      "**********\n",
      "true :  [('feature-selection', 'matlab')]\n",
      "pred :  [('feature-selection', 'matlab')]\n",
      "**********\n",
      "true :  [('feature-selection',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('logistic',)]\n",
      "pred :  [('logistic', 'model-selection', 'regression')]\n",
      "**********\n",
      "true :  [('estimation',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('references',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('data-transformation', 'pca')]\n",
      "pred :  [('data-transformation', 'pca')]\n",
      "**********\n",
      "true :  [('machine-learning',)]\n",
      "pred :  [('machine-learning',)]\n",
      "**********\n",
      "true :  [('normal-distribution',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('ordinal', 'regression')]\n",
      "pred :  [('regression',)]\n",
      "**********\n",
      "true :  [('hypothesis-testing', 'probability')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('data-transformation', 'logistic', 'normal-distribution')]\n",
      "pred :  [('logistic', 'regression')]\n",
      "**********\n",
      "true :  [('repeated-measures',)]\n",
      "pred :  [('anova', 'repeated-measures')]\n",
      "**********\n",
      "true :  [('machine-learning',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('neural-networks',)]\n",
      "pred :  [('neural-networks',)]\n",
      "**********\n",
      "true :  [('neural-networks', 'self-study')]\n",
      "pred :  [('neural-networks',)]\n",
      "**********\n",
      "true :  [('r', 'references')]\n",
      "pred :  [('machine-learning', 'references')]\n",
      "**********\n",
      "true :  [('anova', 'dataset', 'least-squares', 't-test')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('machine-learning',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('machine-learning', 'r', 'svm')]\n",
      "pred :  [('svm',)]\n",
      "**********\n",
      "true :  [('normal-distribution', 'probability', 'sampling')]\n",
      "pred :  [('distributions', 'probability')]\n",
      "**********\n",
      "true :  [('predictive-models',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('distributions',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('experiment-design', 'generalized-linear-model', 'r')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('neural-networks',)]\n",
      "pred :  [('machine-learning', 'neural-networks')]\n",
      "**********\n",
      "true :  [('econometrics', 'expected-value', 'mathematical-statistics')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('t-test',)]\n",
      "pred :  [('t-test',)]\n",
      "**********\n",
      "true :  [('probability',)]\n",
      "pred :  [('bayesian', 'probability')]\n",
      "**********\n",
      "true :  [('correlation', 'multiple-regression', 'regression')]\n",
      "pred :  [('correlation', 'regression')]\n",
      "**********\n",
      "true :  [('repeated-measures', 'spss')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('estimation', 'matlab', 'r')]\n",
      "pred :  [('matlab', 'outliers')]\n",
      "**********\n",
      "true :  [('cross-validation', 'machine-learning')]\n",
      "pred :  [('cross-validation', 'machine-learning', 'multiple-comparisons')]\n",
      "**********\n",
      "true :  [('classification', 'logistic', 'regression', 't-test')]\n",
      "pred :  [('classification', 'logistic')]\n",
      "**********\n",
      "true :  [('distributions', 'estimation', 'maximum-likelihood')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('generalized-linear-model', 'mixed-model', 'model', 'modeling')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('machine-learning',)]\n",
      "pred :  [('machine-learning',)]\n",
      "**********\n",
      "true :  [('correlation', 'survey')]\n",
      "pred :  [('categorical-data', 'survey')]\n",
      "**********\n",
      "true :  [('binomial',)]\n",
      "pred :  [('bayesian',)]\n",
      "**********\n",
      "true :  [('hypothesis-testing', 'probability', 'statistical-significance')]\n",
      "pred :  [('probability', 'statistical-significance')]\n",
      "**********\n",
      "true :  [('hypothesis-testing', 'optimization', 'references')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('least-squares', 'regression')]\n",
      "pred :  [('regression',)]\n",
      "**********\n",
      "true :  [('matlab',)]\n",
      "pred :  [('matlab',)]\n",
      "**********\n",
      "true :  [('binomial', 'distributions', 'probability')]\n",
      "pred :  [('binomial',)]\n",
      "**********\n",
      "true :  [('r', 'regression', 't-test')]\n",
      "pred :  [('regression',)]\n",
      "**********\n",
      "true :  [('covariance', 'mean', 'multivariate-analysis', 'normal-distribution')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('python',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('dataset', 'factor-analysis', 'sample-size')]\n",
      "pred :  [('factor-analysis',)]\n",
      "**********\n",
      "true :  [('machine-learning', 'probability')]\n",
      "pred :  [('machine-learning',)]\n",
      "**********\n",
      "true :  [('chi-squared', 'feature-selection', 'r')]\n",
      "pred :  [('chi-squared', 'feature-selection')]\n",
      "**********\n",
      "true :  [('probability', 'self-study')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('terminology',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('panel-data',)]\n",
      "pred :  [('mixed-model',)]\n",
      "**********\n",
      "true :  [('classification', 'machine-learning')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('mathematical-statistics', 'maximum-likelihood')]\n",
      "pred :  [('maximum-likelihood',)]\n",
      "**********\n",
      "true :  [('distributions',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('classification', 'machine-learning', 'neural-networks')]\n",
      "pred :  [('neural-networks',)]\n",
      "**********\n",
      "true :  [('correlation', 'r', 'simulation', 'time-series')]\n",
      "pred :  [('r', 'time-series')]\n",
      "**********\n",
      "true :  [('r', 'time-series')]\n",
      "pred :  [('r', 'time-series')]\n",
      "**********\n",
      "true :  [('normal-distribution',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('clustering',)]\n",
      "pred :  [('clustering',)]\n",
      "**********\n",
      "true :  [('optimization', 'r')]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('arima', 'forecasting', 'regression', 'time-series')]\n",
      "pred :  [('arima', 'time-series')]\n",
      "**********\n",
      "true :  [('econometrics',)]\n",
      "pred :  [()]\n",
      "**********\n",
      "true :  [('logistic', 'prediction')]\n",
      "pred :  [('bootstrap', 'logistic', 'r')]\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "#include \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "estimator = SGDClassifier(n_jobs= -1,loss = 'modified_huber',penalty = 'l1',l1_ratio=0.150)\n",
    "\n",
    "clf  = OneVsRestClassifier(estimator,n_jobs= -1)\n",
    "\n",
    "clf.fit(trainx,trainy)\n",
    "\n",
    "print(time.time() - t1)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score,hamming_loss,accuracy_score\n",
    "\n",
    "pred_train = clf.predict(trainx)\n",
    "pred_test = clf.predict(testx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('train loss:',hamming_loss(trainy,pred_train))\n",
    "print('test loss:',hamming_loss(testy,pred_test))\n",
    "\n",
    "print('train score :',1 - hamming_loss(trainy,pred_train))\n",
    "print('test score :',1 - hamming_loss(testy,pred_test))\n",
    "\n",
    "\n",
    "\n",
    "# print('train score:',accuracy_score(trainy,pred_train))\n",
    "# print('test score:',accuracy_score(testy,pred_test))\n",
    "\n",
    "print('train score:',f1_score(trainy,pred_train,average  = 'micro'))\n",
    "print('test score:',f1_score(testy,pred_test,average = 'micro'))\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    print('true : ',multilabel_binarizer.inverse_transform(np.array([testy[i]])))\n",
    "    print('pred : ',multilabel_binarizer.inverse_transform(np.array([pred_test[i]])))\n",
    "    \n",
    "    \n",
    "    print(\"**********\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>log</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hinged</td>\n",
       "      <td>0.4128</td>\n",
       "      <td>0.3729</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>modfied_huber</td>\n",
       "      <td>0.4637</td>\n",
       "      <td>0.4447</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  train_acc  val_acc  train_time\n",
       "0            log     0.4990   0.4767        10.0\n",
       "1         hinged     0.4128   0.3729         6.0\n",
       "2  modfied_huber     0.4637   0.4447        10.4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "'''\n",
    " 0.41289432759360767\n",
    "test score: 0.37297099521021826\n",
    "\n",
    "\n",
    "train score: 0.46370377062615753\n",
    "test score: 0.44471277205904547\n",
    "'''\n",
    "\n",
    "data = {\n",
    "        'loss':['log','hinged','modfied_huber'],\n",
    "        'train_acc':[0.499,0.4128,0.4637],\n",
    "        'val_acc':[0.4767,0.3729,0.4447],\n",
    "        'train_time':[10,6,10.40]\n",
    "        }\n",
    "\n",
    "\n",
    "pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020000000000000018"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 - 0.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using log loss has highest train and val acc and lowes accuracy of abt 2%  the training time is also \n",
    "#### moderate 10s is good for 85000 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "frSIsc109RqA"
   },
   "source": [
    "# Using Truncated_SVD\n",
    "\n",
    "(n_components = 400)\n",
    "1. with tfidf + svd => train - 47 and test - 46 traning time is 2min\n",
    "2. with tfidf + svd + standard_scalar => train -45 and test 43 overfitting \n",
    "3. with tfidf + standard_scalar + svd = > train -12 and test- 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfv = TfidfVectorizer(use_idf=1,smooth_idf=1,sublinear_tf=True,ngram_range = (1,1),\n",
    "# #                       min_df = 2,max_df = 0.95,\n",
    "#                       max_features = 50000\n",
    "                      \n",
    "#                      )\n",
    "# words = tfv.fit(df_questions['procesed_text'])\n",
    "# X = tfv.transform(df_questions['procesed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using processed reduces the accuracy and causes more overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHw9Pa4RXGoh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.9757559299469\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "tsvd = TruncatedSVD(n_components = 400)\n",
    "\n",
    "X_svd = tsvd.fit_transform(X)\n",
    "\n",
    "\n",
    "print(time.time() - t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((53455, 400), (53455, 100))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainx,testx,trainy,testy = train_test_split(X_svd,Y,test_size = 0.3)\n",
    "\n",
    "trainx.shape,trainy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VskrdA-X-brl"
   },
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "qtTDibZ0toMm",
    "outputId": "58d0f7fd-16f6-4aac-8884-6e0acb92cd0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.64832544326782\n"
     ]
    }
   ],
   "source": [
    "#include \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "estimator = SGDClassifier(alpha = 0.00002,\n",
    "                          n_jobs= -1,\n",
    "                          random_state = 909,\n",
    "                          loss = 'log',\n",
    "                          penalty = 'l1',\n",
    "#                           learning_rate = 'invscaling',\n",
    "#                           eta0 = 500,\n",
    "#                           power_t = 0.5,\n",
    "                          \n",
    "                          shuffle = True,\n",
    "                          early_stopping=True,\n",
    "                          \n",
    "                         )\n",
    "\n",
    "clf  = OneVsRestClassifier(estimator,n_jobs= -1)\n",
    "\n",
    "clf.fit(trainx,trainy)\n",
    "\n",
    "print(time.time() - t1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "75 + 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "MEz7o7-YtoJ4",
    "outputId": "97e9d85c-ef78-4952-c970-dc20e19c588a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.01582059676363296\n",
      "test loss: 0.016168485377564382\n",
      "train score: 0.480735341974543\n",
      "test score: 0.4652827900799723\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,hamming_loss,accuracy_score\n",
    "\n",
    "pred_train = clf.predict(trainx)\n",
    "pred_test = clf.predict(testx)\n",
    "\n",
    "print('train loss:',hamming_loss(trainy,pred_train))\n",
    "print('test loss:',hamming_loss(testy,pred_test))\n",
    "\n",
    "# print('train score:',accuracy_score(trainy,pred_train))\n",
    "# print('test score:',accuracy_score(testy,pred_test))\n",
    "\n",
    "print('train score:',f1_score(trainy,pred_train,average  = 'micro'))\n",
    "print('test score:',f1_score(testy,pred_test,average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training the model with svd takes the most time > 2m \n",
    "##### the train acc and val accuracy is reduced.\n",
    "##### overfitting is also reduced from 2% to 1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfidf +  std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.202172994613647\n",
      "train loss: 0.02214367224768497\n",
      "test loss: 0.028030990833697075\n",
      "train score: 0.4752658713798714\n",
      "test score: 0.3599796689223532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "X_std = StandardScaler(with_mean = False).fit_transform(X)\n",
    "\n",
    "\n",
    "# tsvd = TruncatedSVD(n_components = 400)\n",
    "\n",
    "# X_svd = tsvd.fit_transform(X_std)\n",
    "\n",
    "\n",
    "trainx,testx,trainy,testy = train_test_split(X_std,Y,test_size = 0.3)\n",
    "\n",
    "\n",
    "#include \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "estimator = SGDClassifier(alpha = 0.002,\n",
    "                          n_jobs= -1,\n",
    "                          random_state = 909,\n",
    "                          loss = 'log',\n",
    "                          penalty = 'l1',\n",
    "#                           learning_rate = 'invscaling',\n",
    "#                           eta0 = 500,\n",
    "#                           power_t = 0.5,\n",
    "                          \n",
    "                          shuffle = True,\n",
    "                          early_stopping=True,\n",
    "                          \n",
    "                         )\n",
    "\n",
    "clf  = OneVsRestClassifier(estimator,n_jobs= -1)\n",
    "\n",
    "clf.fit(trainx,trainy)\n",
    "\n",
    "\n",
    "print(time.time() - t1)\n",
    "\n",
    "pred_train = clf.predict(trainx)\n",
    "pred_test = clf.predict(testx)\n",
    "\n",
    "print('train loss:',hamming_loss(trainy,pred_train))\n",
    "print('test loss:',hamming_loss(testy,pred_test))\n",
    "\n",
    "# print('train score:',accuracy_score(trainy,pred_train))\n",
    "# print('test score:',accuracy_score(testy,pred_test))\n",
    "\n",
    "print('train score:',f1_score(trainy,pred_train,average  = 'micro'))\n",
    "print('test score:',f1_score(testy,pred_test,average = 'micro'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "75 + 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by std scaling the svd matrix we get more overfitting of about 12% \n",
    "##### the train and val acc is also low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfidf + std + svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138.86205554008484\n",
      "train loss: 0.03660462070900758\n",
      "test loss: 0.03835573985159319\n",
      "train score: 0.11647039699454538\n",
      "test score: 0.10972310872009969\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "X_std = StandardScaler(with_mean = False).fit_transform(X)\n",
    "\n",
    "\n",
    "tsvd = TruncatedSVD(n_components = 400)\n",
    "\n",
    "X_svd = tsvd.fit_transform(X_std)\n",
    "\n",
    "\n",
    "trainx,testx,trainy,testy = train_test_split(X_svd,Y,test_size = 0.3)\n",
    "\n",
    "\n",
    "#include \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "estimator = SGDClassifier(alpha = 0.002,\n",
    "                          n_jobs= -1,\n",
    "                          random_state = 909,\n",
    "                          loss = 'log',\n",
    "                          penalty = 'l1',\n",
    "#                           learning_rate = 'invscaling',\n",
    "#                           eta0 = 500,\n",
    "#                           power_t = 0.5,\n",
    "                          \n",
    "                          shuffle = True,\n",
    "                          early_stopping=True,\n",
    "                          \n",
    "                         )\n",
    "\n",
    "clf  = OneVsRestClassifier(estimator,n_jobs= -1)\n",
    "\n",
    "clf.fit(trainx,trainy)\n",
    "\n",
    "\n",
    "print(time.time() - t1)\n",
    "\n",
    "pred_train = clf.predict(trainx)\n",
    "pred_test = clf.predict(testx)\n",
    "\n",
    "print('train loss:',hamming_loss(trainy,pred_train))\n",
    "print('test loss:',hamming_loss(testy,pred_test))\n",
    "\n",
    "# print('train score:',accuracy_score(trainy,pred_train))\n",
    "# print('test score:',accuracy_score(testy,pred_test))\n",
    "\n",
    "print('train score:',f1_score(trainy,pred_train,average  = 'micro'))\n",
    "print('test score:',f1_score(testy,pred_test,average = 'micro'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with tfidf + svd + standard_scalar => train -52 and test 45 overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155.29935145378113\n",
      "train loss: 0.019768029183425313\n",
      "test loss: 0.020475338280226976\n",
      "train score: 0.4581192373567993\n",
      "test score: 0.43804731955675347\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "tsvd = TruncatedSVD(n_components = 400)\n",
    "\n",
    "X_svd = tsvd.fit_transform(X)\n",
    "\n",
    "\n",
    "X_std = StandardScaler(with_mean = False).fit_transform(X_svd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainx,testx,trainy,testy = train_test_split(X_std,Y,test_size = 0.3)\n",
    "\n",
    "\n",
    "#include \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "estimator = SGDClassifier(alpha = 0.002,\n",
    "                          n_jobs= -1,\n",
    "                          random_state = 909,\n",
    "                          loss = 'log',\n",
    "                          penalty = 'l1',\n",
    "#                           learning_rate = 'invscaling',\n",
    "#                           eta0 = 500,\n",
    "#                           power_t = 0.5,\n",
    "                          \n",
    "                          shuffle = True,\n",
    "                          early_stopping=True,\n",
    "                          \n",
    "                         )\n",
    "\n",
    "clf  = OneVsRestClassifier(estimator,n_jobs= -1)\n",
    "\n",
    "clf.fit(trainx,trainy)\n",
    "\n",
    "\n",
    "print(time.time() - t1)\n",
    "\n",
    "pred_train = clf.predict(trainx)\n",
    "pred_test = clf.predict(testx)\n",
    "\n",
    "print('train loss:',hamming_loss(trainy,pred_train))\n",
    "print('test loss:',hamming_loss(testy,pred_test))\n",
    "\n",
    "# print('train score:',accuracy_score(trainy,pred_train))\n",
    "# print('test score:',accuracy_score(testy,pred_test))\n",
    "\n",
    "print('train score:',f1_score(trainy,pred_train,average  = 'micro'))\n",
    "print('test score:',f1_score(testy,pred_test,average = 'micro'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cofig</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>time(s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tfidf+svd</td>\n",
       "      <td>47</td>\n",
       "      <td>46</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tfidf+std</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tfidf+std+svd</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidf+svd+std</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           cofig  train_acc  val_acc  time(s)\n",
       "0      tfidf+svd         47       46      150\n",
       "1      tfidf+std         47       35      110\n",
       "2  tfidf+std+svd         12       11      137\n",
       "3  tfidf+svd+std         45       43      149"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'cofig':['tfidf+svd','tfidf+std','tfidf+std+svd','tfidf+svd+std'],\n",
    "        'train_acc':[47,47,12,45],\n",
    "        'val_acc':[46,35,11,43],\n",
    "        'time(s)':[150,110,137,149]\n",
    "       }\n",
    "\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using svd the overfitting is reduced but the time is increases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.01591600411561126\n",
      "test loss: 0.01664687909209952\n",
      "train score: 0.4382852577857299\n",
      "test score: 0.40764786282306165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "estimator = LinearSVC(penalty = 'l1',dual = False)\n",
    "\n",
    "clf2 = OneVsRestClassifier(estimator,n_jobs = -1)\n",
    "\n",
    "clf2.fit(trainx,trainy)\n",
    "\n",
    "from sklearn.metrics import f1_score,hamming_loss,accuracy_score\n",
    "\n",
    "pred_train = clf2.predict(trainx)\n",
    "pred_test = clf2.predict(testx)\n",
    "\n",
    "print('train loss:',hamming_loss(trainy,pred_train))\n",
    "print('test loss:',hamming_loss(testy,pred_test))\n",
    "\n",
    "# print('train score:',accuracy_score(trainy,pred_train))\n",
    "# print('test score:',accuracy_score(testy,pred_test))\n",
    "\n",
    "print('train score:',f1_score(trainy,pred_train,average  = 'micro'))\n",
    "print('test score:',f1_score(testy,pred_test,average = 'micro'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model\n",
    "\n",
    "\n",
    "1. loss = 'log'\n",
    "2. alpha  = 0.0002\n",
    "3. penalty = 'l1'\n",
    "4. early_stopping = True\n",
    "5. shuffle = True\n",
    "6. Using only Tfidf with max_features = 50000 (if max. features is reduced it causes overfitting.)\n",
    "7. Using procesed_text and \n",
    "8. post_processed_test with wordlen = 2\n",
    "9. both 7 and 8 produce same results with 7 producing slightly higher results \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning Time Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zeX7GHojGjAw"
   },
   "source": [
    "### SGDClassifeir Tunning \n",
    "#### # of parameters\n",
    "1. loss\n",
    "2. penalty *\n",
    "3. alpha **\n",
    "4. l1_ratio\n",
    "5. epsilon\n",
    "6. class_weight\n",
    "\n",
    "7. learning rate = 'constant' , 'optimal','invscaling','adaptive'\n",
    "7.1 eta0\n",
    "7.2 power_t\n",
    "\n",
    "8. average\n",
    "\n",
    "9. early_stopping \n",
    "\n",
    "10. validation_fraction \n",
    "    The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.\n",
    "\n",
    "11. n_iter_no_changeint, default=5\n",
    "\n",
    "    Number of iterations with no improvement to wait before early stopping.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PgG6NSU2toEw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3GSFIYstoBn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGKrYZy3tn95"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "_r-VyySaBiLa",
    "Tny6hDJjA5By",
    "zqWK4zlFVOyt",
    "ePWvtSq7bnM9",
    "e-fW3aFA6zLg",
    "XP8kjSXqaM9z"
   ],
   "name": "Project_File_1(machine learning).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
