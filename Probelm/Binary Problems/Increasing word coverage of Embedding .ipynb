{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The aim of this notebooks is to increse the word coverage for embedding and see if this increases the accuracy.\n",
    "\n",
    "## Things Learned.\n",
    "\n",
    "### Its not good to apply the same preprocessing steps to every dataset \n",
    "\n",
    "### while using embeddins the preprocessing needs to try to get the dataset as close to the embedding words used to increase the words coverage.\n",
    "\n",
    "\n",
    "1. When the trainnig is set to true for embedding layer it reduces the number of epochs need by more than half.\n",
    "2. Blindly apply stemming , lemmatization can reduce the word coverage.\n",
    "3. Applying spacing for punctuations increases the coverafe the most.\n",
    "4. Word Correction was also another important step to increase coverage.\n",
    "5. I also tried contraction correction but this didnt help mush.\n",
    "6. Taking care of extra spaces is also an important step in preprocessing.\n",
    "7. Removing Html tags\n",
    "8. Removing punctuations except [' , . ,-].\n",
    "     \n",
    "     8.a .,- needs to be replaced by '' and not ' '.\n",
    "     \n",
    "     8.b (optional) If you want to keep punctuations.\n",
    "        One approach is to only keep the one's common in embeddings and dataset.\n",
    "\n",
    "9. Its better to set the training \n",
    "\n",
    "\n",
    "# Results:\n",
    "\n",
    "1. With initial preprocessing step the embedding coverage was 97% and test score was 83.\n",
    "2. With punct. preprocessing and Word Correction we increased the coverage to 99.% and test score of 86.\n",
    "\n",
    "So. word processing did help.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "T4kQ1cvSZyC2",
    "outputId": "b486d584-1818-4cfb-d9fc-038af35c4be5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'clr.py', 'Completed Hacktons', 'Detecting Outlinears.ipynb', 'df_questions', 'embedding', 'Feature Secection.ipynb', 'Glove Preprocessing .ipynb', 'Increasing word coverage of Embedding .ipynb', 'Learning Rate Schedularas.ipynb', 'MNIST PCA.ipynb', 'mnist.npz', 'PCA.ipynb', 'processed_train_data', 'Simalarity.ipynb', 'Spell Checker using Word2vec.ipynb', 'TextDataTemplate.ipynb', 'to do.txt', 'Unballanced Classes .ipynb', 'untitled.txt', 'Visualizing Word Embeddings.ipynb', 'word2vec-nlp-tutorial', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(os.listdir())\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vuwE8ADKv5Cn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#helper functions \n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "import operator\n",
    "####helper functions \n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    \n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words\n",
    "\n",
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "\n",
    "def get_unknown_words(text,word_matrix):\n",
    "    \n",
    "    vocab_raw = build_vocab(text)\n",
    "    unk_words_raw = check_coverage(vocab_raw,word_matrix)\n",
    "    \n",
    "    return unk_words_raw\n",
    "\n",
    "\n",
    "\n",
    "def  get_matrix(tokenizer,emb_dim,embeddings):\n",
    "\n",
    "  embedding_matrix = np.zeros((len(tokenizer.word_index) + 1,emb_dim))\n",
    "\n",
    "  count  = 0\n",
    "  for word,index in tokenizer.word_index.items():\n",
    "\n",
    "      vec = embeddings.get(word)\n",
    "      \n",
    "      if vec is not None:\n",
    "        embedding_matrix[index] = vec\n",
    "      else:\n",
    "          count += 1\n",
    "\n",
    "  print(f'{count/len(tokenizer.word_index)} empty')\n",
    "\n",
    "  return  embedding_matrix   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "f6w0006badkZ",
    "outputId": "bca755ae-dcc6-4cf8-d8cd-a048572be437"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading data \n",
    "\n",
    "train_dataset_path = 'word2vec-nlp-tutorial/labeledTrainData.tsv.zip'\n",
    "test_dataset_path = 'word2vec-nlp-tutorial/testData.tsv.zip'\n",
    "\n",
    "\n",
    "train_dataset = pd.read_csv(train_dataset_path,sep = '\\t')\n",
    "\n",
    "train_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "StR21LMihnkW",
    "outputId": "a68d478e-ddde-451b-8486-a5f063224486"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JYN82yXVc-Lw"
   },
   "source": [
    " Generating Test Sentiments \n",
    "Cheat Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0P7inrsav4f"
   },
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(test_dataset_path,sep = '\\t')\n",
    "\n",
    "def get_labels(x):\n",
    "\n",
    "  return int(int(x.split('_')[-1]) >=5)\n",
    "\n",
    "\n",
    "test_dataset['sentiment'] = test_dataset['id'].map(get_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "PZrKnNB2dnpt",
    "outputId": "2cdba993-1453-4a9a-9bd6-cc7c94631be0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>Naturally in a film who's main themes are of m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>This movie is a disaster within a disaster fil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>All in all, this is a movie for kids. We saw i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>Afraid of the Dark left me with the impression...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>A very accurate depiction of small time mob li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             review  sentiment\n",
       "0  12311_10  Naturally in a film who's main themes are of m...          1\n",
       "1    8348_2  This movie is a disaster within a disaster fil...          0\n",
       "2    5828_4  All in all, this is a movie for kids. We saw i...          0\n",
       "3    7186_2  Afraid of the Dark left me with the impression...          0\n",
       "4   12128_7  A very accurate depiction of small time mob li...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "7KX6KrCndyb0",
    "outputId": "765ab523-951c-42df-ddfd-988a019981dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    12500\n",
      "0    12500\n",
      "Name: sentiment, dtype: int64\n",
      "1    12500\n",
      "0    12500\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking class balance \n",
    "\n",
    "print(train_dataset['sentiment'].value_counts())\n",
    "\n",
    "print(test_dataset['sentiment'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGxEZ7xLeFWF"
   },
   "source": [
    "#### EDA \n",
    "\n",
    "1. check the number of words \n",
    "2. check the number of punctuations \n",
    "3. Digits \n",
    "4. other special chatarters.\n",
    "5. get the max , min , avg. length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EkTC43VDjHMe"
   },
   "outputs": [],
   "source": [
    "full_text = list(train_dataset['review']) + list(test_dataset['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        With all this stuff going down at the moment w...\n",
       "1        \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2        The film starts with a manager (Nicholas Bell)...\n",
       "3        It must be assumed that those who praised this...\n",
       "4        Superbly trashy and wondrously unpretentious 8...\n",
       "                               ...                        \n",
       "49995    Sony Pictures Classics, I'm looking at you! So...\n",
       "49996    I always felt that Ms. Merkerson had never got...\n",
       "49997    I was so disappointed in this movie. I am very...\n",
       "49998    From the opening sequence, filled with black a...\n",
       "49999    This is a great horror film for people who don...\n",
       "Length: 50000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "DcgK5jpOefZu",
    "outputId": "15c2d82b-0aa2-467f-d8cb-edc6f94e50f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "449675"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocab(pd.Series(full_text))\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "yoIWQQHJefWD",
    "outputId": "612b882c-2c62-450e-a034-880f124b0fbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "\n",
    "words = []\n",
    "numbers = []\n",
    "punctuations = []\n",
    "need_cleaning = []\n",
    "chars = []\n",
    "\n",
    "print('number of unique words')\n",
    "\n",
    "for word in vocab.keys():\n",
    "\n",
    "    if word.isdigit():\n",
    "      numbers.append(word)\n",
    "    elif len(word) == 1 and word in string.punctuation:\n",
    "      punctuations.append(word)\n",
    "    elif len(word) == 1:\n",
    "      chars.append(word)\n",
    "    else:\n",
    "       words.append(word)    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "GVKqCKQ9f3DV",
    "outputId": "8776a233-2c45-4856-9f00-156f25e9dc49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words 449008\n",
      "number of digits 563\n",
      "number of punct 31\n",
      "number of chars 73\n"
     ]
    }
   ],
   "source": [
    "print(f'number of words {len(words)}')\n",
    "print(f'number of digits {len(numbers)}')\n",
    "print(f'number of punct {len(punctuations)}')\n",
    "print(f'number of chars {len(chars)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "bBlRtxKBgLbe",
    "outputId": "be64d3b2-fe8b-43e5-ade2-3873407159ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". - , : ( & ? ) = / * ! % \\ ~ # \" ' > + $ @ } ` ; { [ _ < ] ^ "
     ]
    }
   ],
   "source": [
    "def print_data(x):\n",
    "\n",
    "  for i in x:\n",
    "    print(i, end = ' ')\n",
    "\n",
    "print_data(punctuations)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "colab_type": "code",
    "id": "uUCKgjk9gZFk",
    "outputId": "4eccf370-bff1-4027-de60-1564ece23597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i a I A R  L Q B S o n G O D v – u X V s P H ® y J K  T E U N Z x b r f C e F à W ½ d l Y M c h z m t w ¨ p   À ¾ » · é Ð ö g  j ý è k ­ á Ö "
     ]
    }
   ],
   "source": [
    "print_data(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chars(x):\n",
    "    \n",
    "    \n",
    "    if ord(x)>=65 and ord(x)<=90 or ord(x) >=97 and ord(x)<=122:\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\x96',\n",
       " '–',\n",
       " '®',\n",
       " '\\x97',\n",
       " 'à',\n",
       " '½',\n",
       " '¨',\n",
       " '\\uf0b7',\n",
       " '\\x95',\n",
       " 'À',\n",
       " '¾',\n",
       " '»',\n",
       " '·',\n",
       " 'é',\n",
       " 'Ð',\n",
       " 'ö',\n",
       " '\\x80',\n",
       " 'ý',\n",
       " 'è',\n",
       " '\\xad',\n",
       " 'á',\n",
       " 'Ö']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_chars_to_remove = []\n",
    "\n",
    "for i in chars:\n",
    "    \n",
    "    if not is_chars(i):\n",
    "        special_chars_to_remove.append(i)\n",
    "        \n",
    "        \n",
    "        \n",
    "special_chars_to_remove        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aax7hZDuhWhY"
   },
   "source": [
    " Ckecking out the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "QkUtaAnQgplm",
    "outputId": "1683b0df-faea-4dfa-87c7-426857e3e7c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 449008 words.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(words)} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "EAFpeQfbgpgd",
    "outputId": "3c091377-f730-4ec5-8f8e-d57b67141e68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final word count after lower casing 400932\n"
     ]
    }
   ],
   "source": [
    "#lower casing all words \n",
    "\n",
    "\n",
    "lower_cased_words = list(map(lambda x : x.lower(),words))\n",
    "final_words = set(lower_cased_words)\n",
    "\n",
    "print(f'final word count after lower casing {len(final_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9q3Nb_M-gpX5"
   },
   "outputs": [],
   "source": [
    "#find words with punctuations or numbers in them \n",
    "\n",
    "def checker(x):\n",
    "\n",
    "  for i in string.punctuation:\n",
    "\n",
    "    if i in x:\n",
    "      return True\n",
    "\n",
    "  return False\n",
    "\n",
    "\n",
    "improper_words = []\n",
    "\n",
    "for word in final_words:\n",
    "\n",
    "    if checker(word):improper_words.append(word)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "v_wlE6d8gpVQ",
    "outputId": "c5c699be-1a32-4c98-e2f3-1985935ecc6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317311"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(improper_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "4ohBlSEUgpS9",
    "outputId": "1ba43a09-4217-4af0-9b85-fdf94c0575ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267924"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using extended punct\n",
    "\n",
    "\n",
    "# puncts = [',', '\"', ':', ')', '(', '!', '?', '|', ';', '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    "#  '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    "#  '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    "#  '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    "#  '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "\n",
    "puncts = ['´',',', '\"','.', ':', ')', '(', '!', '?', '|', ';', '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '●', 'â', '►', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "\n",
    "\n",
    "punct_to_remove = []\n",
    "\n",
    "\n",
    "\n",
    "def checker(x):\n",
    "\n",
    "  for i in  puncts:\n",
    "\n",
    "    if i in x:\n",
    "      punct_to_remove.append(i)\n",
    "      return True\n",
    "\n",
    "  return False\n",
    "\n",
    "\n",
    "improper_words = []\n",
    "\n",
    "for word in final_words:\n",
    "\n",
    "    if checker(word):improper_words.append(word)\n",
    "\n",
    "\n",
    "\n",
    "len(improper_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qg5U1WMXnL5u"
   },
   "source": [
    "found 614 more words using entended punctuations list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\x96',\n",
       " '–',\n",
       " '®',\n",
       " '\\x97',\n",
       " 'à',\n",
       " '½',\n",
       " '¨',\n",
       " '\\uf0b7',\n",
       " '\\x95',\n",
       " 'À',\n",
       " '¾',\n",
       " '»',\n",
       " '·',\n",
       " 'é',\n",
       " 'Ð',\n",
       " 'ö',\n",
       " '\\x80',\n",
       " 'ý',\n",
       " 'è',\n",
       " '\\xad',\n",
       " 'á',\n",
       " 'Ö']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_chars_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "gk-HC54lgpQu",
    "outputId": "15bb9277-ae20-4d1d-f3f2-311e4259e7e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "´|\\,|\\\"|\\.|\\:|\\)|\\(|\\!|\\?|\\||\\;|\\$|\\&|\\/|\\[|\\]|\\>|\\%|\\=|\\#|\\*|\\+|\\\\|\\•|\\~|\\@|\\£|\\·|\\_|\\{|\\}|\\©|\\^|\\®|\\`|\\<|\\→|\\°|\\€|\\™|\\›|\\♥|\\←|\\×|\\§|\\″|\\′|\\Â|\\█|\\½|\\à|\\…|\\“|\\★|\\”|\\●|\\â|\\►|\\¢|\\²|\\¬|\\░|\\¶|\\↑|\\±|\\¿|\\▾|\\═|\\¦|\\║|\\―|\\¥|\\▓|\\—|\\‹|\\─|\\▒|\\：|\\¼|\\⊕|\\▼|\\▪|\\†|\\■|\\’|\\▀|\\¨|\\▄|\\♫|\\☆|\\é|\\¯|\\♦|\\¤|\\▲|\\è|\\¸|\\¾|\\Ã|\\⋅|\\‘|\\∞|\\∙|\\）|\\↓|\\、|\\│|\\（|\\»|\\，|\\♪|\\╩|\\╚|\\³|\\・|\\╦|\\╣|\\╔|\\╗|\\▬|\\❤|\\ï|\\Ø|\\¹|\\≤|\\‡|\\√|\\|\\–|\\®|\\|\\à|\\½|\\¨|\\|\\|\\À|\\¾|\\»|\\·|\\é|\\Ð|\\ö|\\|\\ý|\\è|\\­|\\á|\\Ö\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'father henry ø '"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_punct = '|\\\\'.join(puncts+special_chars_to_remove)\n",
    "\n",
    "print(extended_punct)\n",
    "\n",
    "re.sub(extended_punct,' ','father(henry?øØ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ss0IVC40gpLK"
   },
   "outputs": [],
   "source": [
    "\n",
    "#from the above preprocessing punct list i have removed ', and .\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    \n",
    "    regex = re.compile('<.*?>')\n",
    "    input =  re.sub(regex, ' ',x)\n",
    "    \n",
    "    #removing punctuations.\n",
    "    input = re.sub(extended_punct,' ',input)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return input.lower() \n",
    "\n",
    "\n",
    "processed_text = pd.Series(full_text).map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words\n",
      "number of words 156380\n",
      "number of digits 760\n",
      "number of punct 2\n",
      "number of chars 31\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "\n",
    "words = []\n",
    "numbers = []\n",
    "punctuations = []\n",
    "need_cleaning = []\n",
    "chars = []\n",
    "\n",
    "\n",
    "vocab = build_vocab(processed_text)\n",
    "\n",
    "\n",
    "print('number of unique words')\n",
    "\n",
    "for word in vocab.keys():\n",
    "\n",
    "    if word.isdigit():\n",
    "      numbers.append(word)\n",
    "    elif len(word) == 1 and word in string.punctuation:\n",
    "      punctuations.append(word)\n",
    "    elif len(word) == 1:\n",
    "      chars.append(word)\n",
    "    else:\n",
    "       words.append(word)    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "print(f'number of words {len(words)}')\n",
    "print(f'number of digits {len(numbers)}')\n",
    "print(f'number of punct {len(punctuations)}')\n",
    "print(f'number of chars {len(chars)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-', \"'\"]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Preporcessing to train and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['processed'] = train_dataset['review'].map(preprocess)\n",
    "test_dataset['processed'] = test_dataset['review'].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load embeddings \n",
    "\n",
    "glove_filepath = 'embedding/glove.6B/glove.6B.300d.txt'\n",
    "def load_embed(file):\n",
    "    \n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "  \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index\n",
    "\n",
    "word_matrix = load_embed(glove_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Coverage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 50.35% of vocab\n",
      "Found embeddings for  97.07% of all text\n",
      "no of unk words 78041\n"
     ]
    }
   ],
   "source": [
    "unk_words = check_coverage(vocab,word_matrix)\n",
    "\n",
    "print(f'no of unk words {len(unk_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unk_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So now we have 2 options\n",
    "1. Either use Contrctions \n",
    "2. split the words in 2 halfs eg [it  ,'s]\n",
    "3. For the . we need to replce by ''\n",
    "\n",
    "## But First lets check the perforamce of the model on\n",
    "1. the embedding layer with no. pre-trained embeddings\n",
    "2. using glove 300d embedding training == False\n",
    "3. using glove embeddings training == True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXudNfiyczCj"
   },
   "source": [
    " The first preprocessing function did not contain any preprocessning needed to increase word embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        with all this stuff going down at the moment w...\n",
       "1         the classic war of the worlds   by timothy hi...\n",
       "2        the film starts with a manager  nicholas bell ...\n",
       "3        it must be assumed that those who praised this...\n",
       "4        superbly trashy and wondrously unpretentious 8...\n",
       "                               ...                        \n",
       "49995    sony pictures classics  i'm looking at you  so...\n",
       "49996    i always felt that ms. merkerson had never got...\n",
       "49997    i was so disappointed in this movie. i am very...\n",
       "49998    from the opening sequence  filled with black a...\n",
       "49999    this is a great horror film for people who don...\n",
       "Length: 50000, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Input Data Ready\n",
    "1. train the tokenizer make user that the filers is set to none.\n",
    "2. Convert the input to sequence \n",
    "3. pad them (the padding length maxlen is a hyperparameter.)\n",
    "4. get the labels in proper form \n",
    "5. split the data. the splitting ratio 80-20 is also a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "pOOw4se7gHeI",
    "outputId": "da844352-c1d3-42e7-9f73-ea199a01680d"
   },
   "outputs": [],
   "source": [
    "#Word embeddinsg \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(lower = True,filters = '')\n",
    "\n",
    "tokenizer.fit_on_texts(processed_text)\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1;vocab_size\n",
    "\n",
    "\n",
    "print(f'vocab size {vocab_size}')\n",
    "\n",
    "print('tokenizing')\n",
    "\n",
    "X = tokenizer.texts_to_sequences(train_dataset['processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text.map(lambda x : len(x.split())).plot(kind = 'hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "pOOw4se7gHeI",
    "outputId": "da844352-c1d3-42e7-9f73-ea199a01680d"
   },
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(f'padding with max len of {maxlen}')\n",
    "\n",
    "X_pad = pad_sequences(X,maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_data = train_dataset['sentiment']\n",
    "\n",
    "Y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "Y_data = to_categorical(Y_data,num_classes=2)\n",
    "Y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "HQ5igU4pg6LO",
    "outputId": "72c4ac7d-c1e3-43f0-c7c9-0a222de7a0d8"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "trainx,testx,trainy,testy = train_test_split(X_pad,Y_data,test_size = 0.2,random_state = 101)\n",
    "\n",
    "trainx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JPnpDTNMhEJ-"
   },
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the embedding layer I tried using both with train = False and True.\n",
    "\n",
    "With False the training time reduces but it takes more number of epochs / steps about 10 to get 74. While setting \n",
    "it to True we get a 70 % above accuracy in the 1st epoch and then the model starts overfitting.\n",
    "\n",
    "\n",
    "I have also no\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aMZ-_DoEcyEH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0722 12:54:45.573032  5808 nn_ops.py:4372] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0722 12:54:45.626021  5808 nn_ops.py:4372] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          31837200  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              103424    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 2050      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 33,152,674\n",
      "Trainable params: 33,152,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTM netrow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "def get_model(vocab_size,weights = None,dim = 128,train = False,maxlen = 100):\n",
    "  \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,\n",
    "                      dim,\n",
    "                      weights= weights,\n",
    "                      input_length=maxlen,\n",
    "                      trainable=train))\n",
    "\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(LSTM(100))\n",
    "\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.8))\n",
    "\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.8))\n",
    "\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=Adam(learning_rate = 0.001),\n",
    "                  metrics = ['accuracy',f1])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model  = get_model(vocab_size,dim = 300,train = True)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx.shape,trainy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "colab_type": "code",
    "id": "Eb3a_PhvhbIV",
    "outputId": "edbe9b24-723a-4a5f-fe07-be7c21bf5a61"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping,LearningRateScheduler,ReduceLROnPlateau\n",
    "\n",
    "# es = EarlyStopping(monitor= 'val_accuracy',min_delta = 0.0001,patience=5,restore_best_weights=True,verbose = 1)\n",
    "# rop = ReduceLROnPlateau(monitor = 'loss')\n",
    "\n",
    "\n",
    "history = model.fit(trainx,trainy,\n",
    "                    epochs = 3,\n",
    "                    batch_size = 100,\n",
    "                    validation_data=(testx,testy),\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score is 0.857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_test_score():\n",
    "\n",
    "    test_data = tokenizer.texts_to_sequences(test_dataset['processed'])\n",
    "    test_pad = pad_sequences(test_data,maxlen=100)\n",
    "\n",
    "    preds = model.predict(test_pad)\n",
    "\n",
    "    \n",
    "\n",
    "    print(f\"test score is {accuracy_score(np.argmax(preds,axis = 1),test_dataset['sentiment'])}\")\n",
    "    \n",
    "    \n",
    "get_test_score()    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "dbwok5hdlDu_",
    "outputId": "016cc6c3-71c0-4444-a0f7-eb32ada175cf"
   },
   "outputs": [],
   "source": [
    "del model\n",
    "# del df_questions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WON-vKI_esuB"
   },
   "source": [
    "# Using Word Glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-MJN3vYrepji"
   },
   "outputs": [],
   "source": [
    "weights = get_matrix(tokenizer,emb_dim = 300,embeddings=word_matrix)\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del word_matrix\n",
    "# del df_questions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "colab_type": "code",
    "id": "-euTSLwuonoG",
    "outputId": "311f3e11-ecfd-433a-e262-a27c51b3d709"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = get_model(vocab_size,weights=[weights],train = False,dim=300)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(trainx,trainy,\n",
    "                    epochs = 3,\n",
    "                    batch_size = 100,\n",
    "                    validation_data=(testx,testy),\n",
    "                    \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_test_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = get_model(vocab_size,weights=[weights],train = True,dim=300)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(trainx,trainy,\n",
    "                    epochs = 3,\n",
    "                    batch_size = 100,\n",
    "                    validation_data=(testx,testy),\n",
    "                    \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_test_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'method':['embd','glove_F','glove_T'],\n",
    "        'epochs':[3,3,3],\n",
    "        'Train Score':[96,81,91],\n",
    "        'Val Score':[83,83.73,86],\n",
    "        'Test Score':[82.73,83.41,85],\n",
    "        'TTPEim':[3,1,3]\n",
    "       }\n",
    "\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQzuz7HErKS1"
   },
   "source": [
    "# Applying the second stage of preprocessing for Glove \n",
    "\n",
    "### Main Aim over here is to increase the coverage and see if this increses our accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking out of context words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_matrix = load_embed(glove_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 37.04% of vocab\n",
      "Found embeddings for  92.50% of all text\n"
     ]
    }
   ],
   "source": [
    "oov_words = check_coverage(vocab,word_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"it's\", 33014), (\"don't\", 16530), ('it.', 12995), ('movie.', 12339)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_words[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can try doing \n",
    "1. check if splitting  words like it's to it 's helps. \n",
    "2. check if converting it to it is helps.\n",
    "3. do spelling correction on out of vocab words only and see if they have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hXhfiMy3bSO2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing done...\n",
      "Found embeddings for 74.20% of vocab\n",
      "Found embeddings for  99.64% of all text\n"
     ]
    }
   ],
   "source": [
    "#preprocessing pipeline \n",
    "import re\n",
    "\n",
    "\n",
    "puncts = ['´',',', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "\n",
    "to_replace = '´'\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    \n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    \n",
    "    \n",
    "    #will convert ´ to '\n",
    "    if to_replace in x:\n",
    "        x = x.replace(to_replace,\"'\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    #willl convert e.g to eg\n",
    "    if '.' in x:\n",
    "        x = x.replace('.','')\n",
    "        \n",
    "  \n",
    "        \n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = processed_text.apply(clean_text)\n",
    "\n",
    "print('preprocessing done...')\n",
    "\n",
    "unk_words = get_unknown_words(cleaned_text,word_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of unk words remaining 26220\n"
     ]
    }
   ],
   "source": [
    "print(f'no. of unk words remaining {len(unk_words)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have reduced out of vocab words from 132853 to 39567"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "0wkpxscFsFJo",
    "outputId": "6b6b9854-eb23-4489-ed3c-dffba6678d47",
    "scrolled": true
   },
   "source": [
    "# Using option 2 taking care of contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Known Contractions -\n",
      "   Glove :\n",
      "[\"'cause\", \"ma'am\", \"o'clock\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def known_contractions(embed):\n",
    "    known = []\n",
    "    for contract in contraction_mapping:\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known\n",
    "\n",
    "\n",
    "\n",
    "print(\"- Known Contractions -\")\n",
    "print(\"   Glove :\")\n",
    "print(known_contractions(word_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of contractions found is [\"ain't\", \"aren't\", \"can't\", \"'cause\", \"could've\", \"couldn't\", \"didn't\", \"doesn't\", \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"he'd\", \"he'll\", \"he's\", \"how'd\", \"how's\", \"i'd\", \"i'd've\", \"i'll\", \"i'm\", \"i've\", \"isn't\", \"it'd\", \"it'll\", \"it's\", \"let's\", \"ma'am\", \"might've\", \"mightn't\", \"must've\", \"mustn't\", \"needn't\", \"o'clock\", \"shan't\", \"she'd\", \"she'll\", \"she's\", \"should've\", \"shouldn't\", \"so's\", \"this's\", \"that'd\", \"that's\", \"there'd\", \"there's\", \"here's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"to've\", \"wasn't\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"weren't\", \"what'll\", \"what're\", \"what's\", \"what've\", \"when's\", \"where'd\", \"where's\", \"who'll\", \"who's\", \"who've\", \"why's\", \"won't\", \"would've\", \"wouldn't\", \"y'all\", \"you'd\", \"you'll\", \"you're\", \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "vocab_raw = build_vocab(processed_text)\n",
    "contaction_list = known_contractions(vocab_raw)\n",
    "\n",
    "print(f'no. of contractions found is {contaction_list}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 37.07% of vocab\n",
      "Found embeddings for  93.97% of all text\n",
      "no. of unk words remaining 132677\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    \n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    \n",
    "    #does the replacing \n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    \n",
    "    return text\n",
    "\n",
    "text_no_contractions = processed_text.map(lambda x : clean_contractions(x,contraction_mapping))\n",
    "\n",
    "unk_words = get_unknown_words(text_no_contractions,word_matrix)\n",
    "\n",
    "print(f'no. of unk words remaining {len(unk_words)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SO the contractions did not work must just incresed the coverage by 1% from 97 to 98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f84Hl_rWvARF"
   },
   "source": [
    "We will go forward with approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing done...\n",
      "Found embeddings for 74.20% of vocab\n",
      "Found embeddings for  99.64% of all text\n"
     ]
    }
   ],
   "source": [
    "# Clean the text\n",
    "cleaned_text = processed_text.apply(clean_text)\n",
    "\n",
    "print('preprocessing done...')\n",
    "\n",
    "unk_words = get_unknown_words(cleaned_text,word_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hadn', 540), ('imho', 79), ('gypo', 60), ('feinstone', 49)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_words[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the last step we will be correcting spellings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'apple' in 'apple products have good hardware'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A faster approach is to createa a dict of {'mis-spelled word':'correct-spelled word'} and only replace those \n",
    "#words that change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# #takes 2 long.\n",
    "\n",
    "# # def correct_spelling(x,unk_owrds):\n",
    "    \n",
    "    \n",
    "# #     for word,count in unk_words:\n",
    "        \n",
    "# #         if word in x:\n",
    "# #             return str(TextBlob(x).correct())\n",
    "    \n",
    "    \n",
    "    \n",
    "# #     return x\n",
    "\n",
    "\n",
    "# correct_dict = {}\n",
    "\n",
    "\n",
    "# for word,count in unk_words:\n",
    "    \n",
    "#     print(word,TextBlob(word).correct())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # print(f'no. of unk_words. before correction. {len(unk_words)}')\n",
    "\n",
    "# # corrected_text = cleaned_text.map(lambda x : correct_spelling(x,unk_words))\n",
    "    \n",
    "# # unk_words = get_unknown_words(corrected_text,word_matrix)\n",
    "    \n",
    "# # print(f'no. of unk. owrds after correction {len(unk_words)}')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A better method is stem,lemmatize and correct the word when creating the embedding matrix but this method doesnt change the word in the vocab code from kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v4B9YFgmvCnL"
   },
   "outputs": [],
   "source": [
    "#imports \n",
    "from __future__ import absolute_import, division\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "### stemmers \n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "import sys\n",
    "from os.path import dirname\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "sjJOvLEKvD8l",
    "outputId": "1a5c67cd-d096-4fa0-e49f-676691d1fe70"
   },
   "outputs": [],
   "source": [
    "path = 'GoogleNews-vectors-negative300.bin'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('embedding/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin', \n",
    "                                                        binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "zwk3wmC1vD6e",
    "outputId": "db701d86-bee9-4445-bf59-ea6eb742df3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded...\n",
      "done..\n"
     ]
    }
   ],
   "source": [
    "words = model.index2word\n",
    "\n",
    "print('loaded...')\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "WORDS = w_rank\n",
    "\n",
    "print('done..')\n",
    "\n",
    "\n",
    "# Use fast text as vocabulary\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or [word])\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def singlify(word):\n",
    "    \n",
    "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the correct Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dict = {}\n",
    "\n",
    "\n",
    "for word,count in unk_words:\n",
    "    correct_word = correction(word)\n",
    "    \n",
    "    if correct_word != word:\n",
    "        correct_dict[word] = correct_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'gypo' in correct_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of unk_words. before correction. 26220\n",
      "Found embeddings for 81.90% of vocab\n",
      "Found embeddings for  99.75% of all text\n",
      "no. of unk. owrds after correction 16928\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def correct_mapping(text,mapping):\n",
    "    \n",
    "    #does the replacing \n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    \n",
    "    return text\n",
    "    \n",
    "    \n",
    "\n",
    "print(f'no. of unk_words. before correction. {len(unk_words)}')\n",
    "    \n",
    "    \n",
    "corrected_text = cleaned_text.map(lambda x : correct_mapping(x,correct_dict))\n",
    "\n",
    "    \n",
    "unk_words = get_unknown_words(corrected_text,word_matrix)\n",
    "    \n",
    "print(f'no. of unk. owrds after correction {len(unk_words)}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Approach 1 . and Correction to Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## method to call clean_text ,correct_mapping    \n",
    "\n",
    "train_dataset['processed'] = train_dataset['processed'].map(clean_text)\n",
    "train_dataset['processed'] = train_dataset['processed'].map(lambda x : correct_mapping(x,correct_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['processed'] = test_dataset['processed'].map(clean_text)\n",
    "test_dataset['processed'] = test_dataset['processed'].map(lambda x : correct_mapping(x,correct_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This has reduced the words from 26000 to 16000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " At last we are checking if stemming  and lemmatizing the text increases the coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run '"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "              'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "\n",
    "def lemmatize_sent(text): \n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    ls = list(wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in pos_tag(word_tokenize(text)))\n",
    "\n",
    "    str = ''\n",
    "\n",
    "    for i in  ls:\n",
    "          str += i.lower() + ' '\n",
    "\n",
    "    return str  \n",
    "\n",
    "\n",
    "\n",
    "lemmatize_sent('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mt57aFqxvD4W"
   },
   "outputs": [],
   "source": [
    "\n",
    "stem = []\n",
    "correct = []\n",
    "lemmatize = []\n",
    "no_embd = []\n",
    "\n",
    "\n",
    "def load_glove(tokenizer,embeddings_index,emb_dim = 100):\n",
    "\n",
    "    '''\n",
    "    word is check \n",
    "    1. the actual word \n",
    "    2. lower case and check\n",
    "    3. upper case and check\n",
    "    4. captalize and check\n",
    "    5. Stem and check\n",
    "    6. lemma and check\n",
    "    7. correct spelling and check.\n",
    "    '''\n",
    "    known_words = 0\n",
    "    unk_words = 0\n",
    "\n",
    "\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index)+ 1,emb_dim)) - 1.0\n",
    "\n",
    "    count  = 0\n",
    "\n",
    "    for key,index in tqdm(tokenizer.word_index.items()):\n",
    "\n",
    "        word = key\n",
    "\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "\n",
    "\n",
    "        #checking if word is present.\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            known_words += 1\n",
    "            continue\n",
    "\n",
    "        #check if lower case of the word is present.\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            known_words += 1\n",
    "            continue\n",
    "\n",
    "        #cheking if the upper case of the word is present.\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            known_words += 1\n",
    "            continue\n",
    "\n",
    "        #checking if capital word is present.\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            known_words += 1\n",
    "            continue\n",
    "\n",
    "        '''\n",
    "        Three steppers are  used \n",
    "\n",
    "        ps -> PorterStemmer \n",
    "        ls -> lancaster \n",
    "        sb->snowball stemmer \n",
    "\n",
    "        need to check on this as well\n",
    "\n",
    "        \n",
    "        '''    \n",
    "\n",
    "\n",
    "\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            stem.append((word,key))\n",
    "            known_words += 1\n",
    "            continue\n",
    "\n",
    "\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            stem.append((word,key))\n",
    "            known_words += 1\n",
    "            continue\n",
    "\n",
    "\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            stem.append((word,key))\n",
    "            known_words += 1\n",
    "            continue\n",
    "\n",
    "        '''\n",
    "        Using Lemmatization\n",
    "        \n",
    "        '''    \n",
    "\n",
    "\n",
    "        word = lemmatize_sent(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            lemmatize.append((word,key))\n",
    "            known_words += 1\n",
    "            continue\n",
    "\n",
    "        ''''\n",
    "        Checking for spellling mistakes \n",
    "        \n",
    "        '''    \n",
    "\n",
    "\n",
    "\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "                correct.append((word,key))\n",
    "                known_words += 1\n",
    "                continue\n",
    "\n",
    "                \n",
    "        #unknown words.    \n",
    "        # embedding_matrix[word_dict[key]] = unknown_vector        \n",
    "        unk_words += 1\n",
    "        no_embd.append(key)\n",
    "    \n",
    "    \n",
    "\n",
    "    print(known_words / len(tokenizer.word_index))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "Gr7GBjUfHv1d",
    "outputId": "31ab0f0e-d336-4468-d19d-8636bbaae393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 84338/106123 [00:40<00:19, 1127.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding with max len of 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000, 100)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word embeddinsg \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(filters = '')\n",
    "\n",
    "tokenizer.fit_on_texts(corrected_text)\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1;vocab_size\n",
    "\n",
    "print('tokenizing')\n",
    "\n",
    "X = tokenizer.texts_to_sequences(train_dataset['processed'])\n",
    "\n",
    "\n",
    "\n",
    "maxlen = 100\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(f'padding with max len of {maxlen}')\n",
    "\n",
    "X_pad = pad_sequences(X,maxlen)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "trainx,testx,trainy,testy = train_test_split(X_pad,Y_data,test_size = 0.2)\n",
    "\n",
    "trainx.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "AP3M0JyYLTao",
    "outputId": "acf69799-f10a-45b9-a1e3-43485516f1df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94468, 94469)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index),vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "UMjYxhQ4vDzd",
    "outputId": "0028233b-e789-4cc2-edfb-6b69f1e94327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done building vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/94468 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 16274/94468 [00:00<00:00, 162729.68it/s]\u001b[A\n",
      " 22%|██▏       | 21156/94468 [00:00<00:00, 89903.91it/s] \u001b[A\n",
      " 27%|██▋       | 25893/94468 [00:00<00:01, 53172.86it/s]\u001b[A\n",
      " 32%|███▏      | 30026/94468 [00:00<00:02, 31354.54it/s]\u001b[A\n",
      " 35%|███▌      | 33400/94468 [00:00<00:02, 26835.43it/s]\u001b[A\n",
      " 38%|███▊      | 36364/94468 [00:00<00:02, 23765.36it/s]\u001b[A\n",
      " 41%|████▏     | 39002/94468 [00:01<00:02, 19844.47it/s]\u001b[A\n",
      " 44%|████▎     | 41278/94468 [00:01<00:03, 14832.86it/s]\u001b[A\n",
      " 46%|████▌     | 43152/94468 [00:01<00:03, 13123.67it/s]\u001b[A\n",
      " 47%|████▋     | 44774/94468 [00:01<00:05, 9283.26it/s] \u001b[A\n",
      " 49%|████▉     | 46075/94468 [00:02<00:05, 9489.21it/s]\u001b[A\n",
      " 50%|█████     | 47286/94468 [00:02<00:04, 9458.19it/s]\u001b[A\n",
      " 51%|█████▏    | 48416/94468 [00:02<00:05, 8449.94it/s]\u001b[A\n",
      " 52%|█████▏    | 49410/94468 [00:02<00:05, 8091.03it/s]\u001b[A\n",
      " 53%|█████▎    | 50327/94468 [00:02<00:06, 6971.34it/s]\u001b[A\n",
      " 54%|█████▍    | 51127/94468 [00:02<00:07, 5616.65it/s]\u001b[A\n",
      " 55%|█████▍    | 51803/94468 [00:02<00:07, 5794.94it/s]\u001b[A\n",
      " 56%|█████▌    | 52464/94468 [00:03<00:07, 5269.12it/s]\u001b[A\n",
      " 56%|█████▌    | 53057/94468 [00:03<00:08, 4909.64it/s]\u001b[A\n",
      " 57%|█████▋    | 53600/94468 [00:03<00:08, 5030.10it/s]\u001b[A\n",
      " 57%|█████▋    | 54140/94468 [00:03<00:08, 4737.16it/s]\u001b[A\n",
      " 58%|█████▊    | 54643/94468 [00:03<00:08, 4429.37it/s]\u001b[A\n",
      " 58%|█████▊    | 55111/94468 [00:03<00:09, 4338.83it/s]\u001b[A\n",
      " 59%|█████▉    | 55563/94468 [00:03<00:09, 4027.57it/s]\u001b[A\n",
      " 59%|█████▉    | 55983/94468 [00:03<00:10, 3642.86it/s]\u001b[A\n",
      " 60%|█████▉    | 56366/94468 [00:04<00:11, 3428.86it/s]\u001b[A\n",
      " 60%|██████    | 56725/94468 [00:04<00:11, 3154.87it/s]\u001b[A\n",
      " 60%|██████    | 57071/94468 [00:04<00:11, 3240.42it/s]\u001b[A\n",
      " 61%|██████    | 57610/94468 [00:04<00:10, 3673.30it/s]\u001b[A\n",
      " 61%|██████▏   | 58017/94468 [00:04<00:09, 3783.69it/s]\u001b[A\n",
      " 62%|██████▏   | 58508/94468 [00:04<00:08, 4053.28it/s]\u001b[A\n",
      " 62%|██████▏   | 58933/94468 [00:04<00:09, 3906.29it/s]\u001b[A\n",
      " 63%|██████▎   | 59339/94468 [00:04<00:09, 3889.51it/s]\u001b[A\n",
      " 63%|██████▎   | 59746/94468 [00:04<00:08, 3941.87it/s]\u001b[A\n",
      " 64%|██████▍   | 60315/94468 [00:05<00:07, 4341.99it/s]\u001b[A\n",
      " 64%|██████▍   | 60767/94468 [00:05<00:08, 3844.41it/s]\u001b[A\n",
      " 65%|██████▍   | 61175/94468 [00:05<00:09, 3649.62it/s]\u001b[A\n",
      " 65%|██████▌   | 61558/94468 [00:05<00:09, 3577.36it/s]\u001b[A\n",
      " 66%|██████▌   | 61962/94468 [00:05<00:08, 3704.55it/s]\u001b[A\n",
      " 66%|██████▌   | 62343/94468 [00:05<00:09, 3386.79it/s]\u001b[A\n",
      " 66%|██████▋   | 62695/94468 [00:05<00:10, 3035.62it/s]\u001b[A\n",
      " 67%|██████▋   | 63015/94468 [00:05<00:11, 2721.23it/s]\u001b[A\n",
      " 67%|██████▋   | 63305/94468 [00:06<00:13, 2312.99it/s]\u001b[A\n",
      " 67%|██████▋   | 63559/94468 [00:06<00:14, 2080.31it/s]\u001b[A\n",
      " 68%|██████▊   | 63788/94468 [00:06<00:16, 1862.41it/s]\u001b[A\n",
      " 68%|██████▊   | 63993/94468 [00:06<00:16, 1847.62it/s]\u001b[A\n",
      " 68%|██████▊   | 64191/94468 [00:06<00:16, 1802.98it/s]\u001b[A\n",
      " 68%|██████▊   | 64381/94468 [00:06<00:18, 1662.73it/s]\u001b[A\n",
      " 68%|██████▊   | 64556/94468 [00:06<00:19, 1532.71it/s]\u001b[A\n",
      " 69%|██████▊   | 64758/94468 [00:07<00:17, 1652.25it/s]\u001b[A\n",
      " 69%|██████▉   | 64949/94468 [00:07<00:17, 1721.83it/s]\u001b[A\n",
      " 69%|██████▉   | 65128/94468 [00:07<00:17, 1664.07it/s]\u001b[A\n",
      " 69%|██████▉   | 65300/94468 [00:07<00:17, 1675.51it/s]\u001b[A\n",
      " 69%|██████▉   | 65543/94468 [00:07<00:15, 1847.58it/s]\u001b[A\n",
      " 70%|██████▉   | 65746/94468 [00:07<00:15, 1898.72it/s]\u001b[A\n",
      " 70%|██████▉   | 65994/94468 [00:07<00:13, 2042.30it/s]\u001b[A\n",
      " 70%|███████   | 66206/94468 [00:07<00:13, 2041.10it/s]\u001b[A\n",
      " 70%|███████   | 66416/94468 [00:07<00:15, 1836.89it/s]\u001b[A\n",
      " 71%|███████   | 66640/94468 [00:07<00:14, 1936.65it/s]\u001b[A\n",
      " 71%|███████   | 66841/94468 [00:08<00:15, 1739.44it/s]\u001b[A\n",
      " 71%|███████   | 67024/94468 [00:08<00:16, 1624.45it/s]\u001b[A\n",
      " 71%|███████   | 67194/94468 [00:08<00:17, 1590.98it/s]\u001b[A\n",
      " 71%|███████▏  | 67359/94468 [00:08<00:17, 1532.04it/s]\u001b[A\n",
      " 71%|███████▏  | 67517/94468 [00:08<00:17, 1541.56it/s]\u001b[A\n",
      " 72%|███████▏  | 67675/94468 [00:08<00:18, 1415.16it/s]\u001b[A\n",
      " 72%|███████▏  | 67862/94468 [00:08<00:17, 1526.52it/s]\u001b[A\n",
      " 72%|███████▏  | 68125/94468 [00:08<00:15, 1742.84it/s]\u001b[A\n",
      " 72%|███████▏  | 68314/94468 [00:09<00:14, 1759.56it/s]\u001b[A\n",
      " 73%|███████▎  | 68509/94468 [00:09<00:14, 1812.63it/s]\u001b[A\n",
      " 73%|███████▎  | 68734/94468 [00:09<00:13, 1924.84it/s]\u001b[A\n",
      " 73%|███████▎  | 68934/94468 [00:09<00:13, 1844.41it/s]\u001b[A\n",
      " 73%|███████▎  | 69134/94468 [00:09<00:13, 1888.44it/s]\u001b[A\n",
      " 73%|███████▎  | 69328/94468 [00:09<00:13, 1897.99it/s]\u001b[A\n",
      " 74%|███████▎  | 69524/94468 [00:09<00:13, 1910.53it/s]\u001b[A\n",
      " 74%|███████▍  | 69718/94468 [00:09<00:14, 1679.90it/s]\u001b[A\n",
      " 74%|███████▍  | 69893/94468 [00:09<00:15, 1567.71it/s]\u001b[A\n",
      " 74%|███████▍  | 70056/94468 [00:10<00:15, 1536.49it/s]\u001b[A\n",
      " 74%|███████▍  | 70215/94468 [00:10<00:16, 1474.48it/s]\u001b[A\n",
      " 74%|███████▍  | 70373/94468 [00:10<00:16, 1504.59it/s]\u001b[A\n",
      " 75%|███████▍  | 70565/94468 [00:10<00:14, 1608.69it/s]\u001b[A\n",
      " 75%|███████▍  | 70751/94468 [00:10<00:14, 1672.21it/s]\u001b[A\n",
      " 75%|███████▌  | 70927/94468 [00:10<00:13, 1692.92it/s]\u001b[A\n",
      " 75%|███████▌  | 71155/94468 [00:10<00:12, 1834.61it/s]\u001b[A\n",
      " 76%|███████▌  | 71344/94468 [00:10<00:13, 1777.72it/s]\u001b[A\n",
      " 76%|███████▌  | 71526/94468 [00:10<00:13, 1764.13it/s]\u001b[A\n",
      " 76%|███████▌  | 71767/94468 [00:10<00:11, 1918.33it/s]\u001b[A\n",
      " 76%|███████▌  | 71965/94468 [00:11<00:12, 1765.44it/s]\u001b[A\n",
      " 76%|███████▋  | 72149/94468 [00:11<00:13, 1647.78it/s]\u001b[A\n",
      " 77%|███████▋  | 72320/94468 [00:11<00:14, 1539.98it/s]\u001b[A\n",
      " 77%|███████▋  | 72536/94468 [00:11<00:13, 1685.06it/s]\u001b[A\n",
      " 77%|███████▋  | 72713/94468 [00:11<00:13, 1598.46it/s]\u001b[A\n",
      " 77%|███████▋  | 72880/94468 [00:11<00:14, 1489.26it/s]\u001b[A\n",
      " 77%|███████▋  | 73050/94468 [00:11<00:13, 1546.55it/s]\u001b[A\n",
      " 77%|███████▋  | 73210/94468 [00:11<00:14, 1517.82it/s]\u001b[A\n",
      " 78%|███████▊  | 73419/94468 [00:12<00:12, 1645.89it/s]\u001b[A\n",
      " 78%|███████▊  | 73590/94468 [00:12<00:13, 1594.72it/s]\u001b[A\n",
      " 78%|███████▊  | 73771/94468 [00:12<00:12, 1649.14it/s]\u001b[A\n",
      " 78%|███████▊  | 73964/94468 [00:12<00:11, 1724.41it/s]\u001b[A\n",
      " 79%|███████▊  | 74176/94468 [00:12<00:11, 1826.64it/s]\u001b[A\n",
      " 79%|███████▊  | 74363/94468 [00:12<00:11, 1812.64it/s]\u001b[A\n",
      " 79%|███████▉  | 74571/94468 [00:12<00:10, 1884.57it/s]\u001b[A\n",
      " 79%|███████▉  | 74808/94468 [00:12<00:09, 2007.92it/s]\u001b[A\n",
      " 79%|███████▉  | 75013/94468 [00:12<00:09, 1956.69it/s]\u001b[A\n",
      " 80%|███████▉  | 75212/94468 [00:12<00:10, 1856.44it/s]\u001b[A\n",
      " 80%|███████▉  | 75401/94468 [00:13<00:11, 1714.00it/s]\u001b[A\n",
      " 80%|████████  | 75621/94468 [00:13<00:10, 1835.63it/s]\u001b[A\n",
      " 80%|████████  | 75890/94468 [00:13<00:09, 2028.90it/s]\u001b[A\n",
      " 81%|████████  | 76104/94468 [00:13<00:09, 1894.15it/s]\u001b[A\n",
      " 81%|████████  | 76303/94468 [00:13<00:09, 1834.09it/s]\u001b[A\n",
      " 81%|████████  | 76497/94468 [00:13<00:09, 1864.59it/s]\u001b[A\n",
      " 81%|████████  | 76748/94468 [00:13<00:08, 2015.52it/s]\u001b[A\n",
      " 82%|████████▏ | 76993/94468 [00:13<00:08, 2128.76it/s]\u001b[A\n",
      " 82%|████████▏ | 77251/94468 [00:13<00:07, 2246.60it/s]\u001b[A\n",
      " 82%|████████▏ | 77515/94468 [00:14<00:07, 2351.65it/s]\u001b[A\n",
      " 82%|████████▏ | 77772/94468 [00:14<00:06, 2406.33it/s]\u001b[A\n",
      " 83%|████████▎ | 78017/94468 [00:14<00:07, 2180.24it/s]\u001b[A\n",
      " 83%|████████▎ | 78243/94468 [00:14<00:08, 2014.92it/s]\u001b[A\n",
      " 83%|████████▎ | 78452/94468 [00:14<00:08, 1862.53it/s]\u001b[A\n",
      " 83%|████████▎ | 78646/94468 [00:14<00:08, 1858.02it/s]\u001b[A\n",
      " 83%|████████▎ | 78838/94468 [00:14<00:08, 1807.37it/s]\u001b[A\n",
      " 84%|████████▎ | 79023/94468 [00:14<00:09, 1662.77it/s]\u001b[A\n",
      " 84%|████████▍ | 79238/94468 [00:15<00:08, 1784.02it/s]\u001b[A\n",
      " 84%|████████▍ | 79460/94468 [00:15<00:07, 1895.65it/s]\u001b[A\n",
      " 84%|████████▍ | 79665/94468 [00:15<00:07, 1939.46it/s]\u001b[A\n",
      " 85%|████████▍ | 79899/94468 [00:15<00:07, 2044.36it/s]\u001b[A\n",
      " 85%|████████▍ | 80139/94468 [00:15<00:06, 2139.46it/s]\u001b[A\n",
      " 85%|████████▌ | 80400/94468 [00:15<00:06, 2261.71it/s]\u001b[A\n",
      " 85%|████████▌ | 80632/94468 [00:15<00:06, 2265.49it/s]\u001b[A\n",
      " 86%|████████▌ | 80863/94468 [00:15<00:06, 2181.78it/s]\u001b[A\n",
      " 86%|████████▌ | 81087/94468 [00:15<00:06, 2192.43it/s]\u001b[A\n",
      " 86%|████████▌ | 81309/94468 [00:15<00:06, 1993.09it/s]\u001b[A\n",
      " 86%|████████▋ | 81514/94468 [00:16<00:06, 2004.00it/s]\u001b[A\n",
      " 87%|████████▋ | 81718/94468 [00:16<00:06, 1850.16it/s]\u001b[A\n",
      " 87%|████████▋ | 81951/94468 [00:16<00:06, 1971.88it/s]\u001b[A\n",
      " 87%|████████▋ | 82154/94468 [00:16<00:06, 1827.73it/s]\u001b[A\n",
      " 87%|████████▋ | 82367/94468 [00:16<00:06, 1908.96it/s]\u001b[A\n",
      " 87%|████████▋ | 82564/94468 [00:16<00:07, 1693.37it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 82742/94468 [00:16<00:07, 1615.49it/s]\u001b[A\n",
      " 88%|████████▊ | 82910/94468 [00:16<00:07, 1629.61it/s]\u001b[A\n",
      " 88%|████████▊ | 83147/94468 [00:17<00:06, 1798.10it/s]\u001b[A\n",
      " 88%|████████▊ | 83389/94468 [00:17<00:05, 1948.27it/s]\u001b[A\n",
      " 88%|████████▊ | 83594/94468 [00:17<00:05, 1922.03it/s]\u001b[A\n",
      " 89%|████████▊ | 83794/94468 [00:17<00:05, 1916.77it/s]\u001b[A\n",
      " 89%|████████▉ | 83991/94468 [00:17<00:05, 1790.05it/s]\u001b[A\n",
      " 89%|████████▉ | 84176/94468 [00:17<00:05, 1756.17it/s]\u001b[A\n",
      " 89%|████████▉ | 84356/94468 [00:17<00:06, 1603.59it/s]\u001b[A\n",
      " 89%|████████▉ | 84522/94468 [00:17<00:06, 1517.85it/s]\u001b[A\n",
      " 90%|████████▉ | 84679/94468 [00:17<00:06, 1515.34it/s]\u001b[A\n",
      " 90%|████████▉ | 84846/94468 [00:18<00:06, 1558.61it/s]\u001b[A\n",
      " 90%|████████▉ | 85005/94468 [00:18<00:06, 1480.27it/s]\u001b[A\n",
      " 90%|█████████ | 85156/94468 [00:18<00:06, 1453.34it/s]\u001b[A\n",
      " 90%|█████████ | 85325/94468 [00:18<00:06, 1514.19it/s]\u001b[A\n",
      " 91%|█████████ | 85542/94468 [00:18<00:05, 1665.14it/s]\u001b[A\n",
      " 91%|█████████ | 85799/94468 [00:18<00:04, 1859.24it/s]\u001b[A\n",
      " 91%|█████████ | 86053/94468 [00:18<00:04, 2021.08it/s]\u001b[A\n",
      " 91%|█████████▏| 86268/94468 [00:18<00:04, 1845.44it/s]\u001b[A\n",
      " 92%|█████████▏| 86464/94468 [00:18<00:04, 1801.88it/s]\u001b[A\n",
      " 92%|█████████▏| 86653/94468 [00:19<00:05, 1548.83it/s]\u001b[A\n",
      " 92%|█████████▏| 86824/94468 [00:19<00:04, 1593.86it/s]\u001b[A\n",
      " 92%|█████████▏| 86993/94468 [00:19<00:04, 1611.27it/s]\u001b[A\n",
      " 92%|█████████▏| 87261/94468 [00:19<00:03, 1827.22it/s]\u001b[A\n",
      " 93%|█████████▎| 87464/94468 [00:19<00:03, 1878.42it/s]\u001b[A\n",
      " 93%|█████████▎| 87663/94468 [00:19<00:03, 1763.23it/s]\u001b[A\n",
      " 93%|█████████▎| 87875/94468 [00:19<00:03, 1856.95it/s]\u001b[A\n",
      " 93%|█████████▎| 88144/94468 [00:19<00:03, 2042.42it/s]\u001b[A\n",
      " 94%|█████████▎| 88360/94468 [00:19<00:03, 1842.50it/s]\u001b[A\n",
      " 94%|█████████▎| 88556/94468 [00:20<00:03, 1808.67it/s]\u001b[A\n",
      " 94%|█████████▍| 88775/94468 [00:20<00:02, 1908.30it/s]\u001b[A\n",
      " 94%|█████████▍| 89016/94468 [00:20<00:02, 2035.38it/s]\u001b[A\n",
      " 95%|█████████▍| 89313/94468 [00:20<00:02, 2242.46it/s]\u001b[A\n",
      " 95%|█████████▍| 89549/94468 [00:20<00:02, 1855.97it/s]\u001b[A\n",
      " 95%|█████████▌| 89754/94468 [00:20<00:02, 1713.74it/s]\u001b[A\n",
      " 95%|█████████▌| 89941/94468 [00:20<00:02, 1659.42it/s]\u001b[A\n",
      " 95%|█████████▌| 90119/94468 [00:20<00:02, 1597.98it/s]\u001b[A\n",
      " 96%|█████████▌| 90288/94468 [00:21<00:02, 1615.26it/s]\u001b[A\n",
      " 96%|█████████▌| 90456/94468 [00:21<00:02, 1619.84it/s]\u001b[A\n",
      " 96%|█████████▌| 90623/94468 [00:21<00:02, 1552.56it/s]\u001b[A\n",
      " 96%|█████████▌| 90848/94468 [00:21<00:02, 1711.71it/s]\u001b[A\n",
      " 96%|█████████▋| 91027/94468 [00:21<00:02, 1639.12it/s]\u001b[A\n",
      " 97%|█████████▋| 91230/94468 [00:21<00:01, 1739.55it/s]\u001b[A\n",
      " 97%|█████████▋| 91410/94468 [00:21<00:01, 1669.25it/s]\u001b[A\n",
      " 97%|█████████▋| 91639/94468 [00:21<00:01, 1812.59it/s]\u001b[A\n",
      " 97%|█████████▋| 91828/94468 [00:21<00:01, 1609.97it/s]\u001b[A\n",
      " 97%|█████████▋| 91998/94468 [00:22<00:01, 1521.78it/s]\u001b[A\n",
      " 98%|█████████▊| 92192/94468 [00:22<00:01, 1622.88it/s]\u001b[A\n",
      " 98%|█████████▊| 92362/94468 [00:22<00:01, 1631.03it/s]\u001b[A\n",
      " 98%|█████████▊| 92530/94468 [00:22<00:01, 1580.37it/s]\u001b[A\n",
      " 98%|█████████▊| 92744/94468 [00:22<00:01, 1714.79it/s]\u001b[A\n",
      " 98%|█████████▊| 92954/94468 [00:22<00:00, 1805.36it/s]\u001b[A\n",
      " 99%|█████████▊| 93144/94468 [00:22<00:00, 1832.64it/s]\u001b[A\n",
      " 99%|█████████▉| 93332/94468 [00:22<00:00, 1743.83it/s]\u001b[A\n",
      " 99%|█████████▉| 93567/94468 [00:22<00:00, 1887.83it/s]\u001b[A\n",
      " 99%|█████████▉| 93762/94468 [00:23<00:00, 1662.27it/s]\u001b[A\n",
      " 99%|█████████▉| 93960/94468 [00:23<00:00, 1741.71it/s]\u001b[A\n",
      "100%|██████████| 94468/94468 [00:23<00:00, 4048.17it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8578142863191769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('done building vocab')\n",
    "weights = load_glove(tokenizer,word_matrix,emb_dim = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings found because of stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of words found becz. of stemming are 10565\n"
     ]
    }
   ],
   "source": [
    "print(f'no. of words found becz. of stemming are {len(stem)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings found because of Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "3BS3v17UE8Is",
    "outputId": "e146b380-f50d-486e-80f7-07cacbf86292"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of words found becz. of lemmatization are 0\n"
     ]
    }
   ],
   "source": [
    "print(f'no. of words found becz. of lemmatization are {len(lemmatize)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings found because of correction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of words found becz. of lemmatization are 1809\n"
     ]
    }
   ],
   "source": [
    "print(f'no. of words found becz. of lemmatization are {len(correct)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we find more words in correction ?\n",
    "Ealier we only took are of words found in the found in unk_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of these last correction are just spaces we should probably modify our preprocessing pipeline to include this spaces or create a regax that removes extra spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words with no. EMbeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24238\n"
     ]
    }
   ],
   "source": [
    "print(len(no_embd))\n",
    "# no_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "rKhxrxytL-Ue",
    "outputId": "bf112155-832d-46f2-ea5f-b639cee5f2f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import gc\n",
    "# del model,tokenizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Models with Weights having 99% word coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "colab_type": "code",
    "id": "Tx30b_NeGCS1",
    "outputId": "26df023c-16c2-4b05-d978-76a2173748d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 12:55:22.823021  5808 nn_ops.py:4372] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0722 12:55:22.875717  5808 nn_ops.py:4372] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          31837200  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              103424    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 2050      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 33,152,674\n",
      "Trainable params: 1,315,474\n",
      "Non-trainable params: 31,837,200\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 12:55:23.400757  5808 nn_ops.py:4372] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 51s 3ms/sample - loss: 0.5956 - accuracy: 0.6724 - f1: 0.6727 - val_loss: 0.4602 - val_accuracy: 0.7903 - val_f1: 0.7907\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 49s 2ms/sample - loss: 0.4869 - accuracy: 0.7749 - f1: 0.7749 - val_loss: 0.4191 - val_accuracy: 0.8066 - val_f1: 0.8063\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 53s 3ms/sample - loss: 0.4389 - accuracy: 0.8012 - f1: 0.8012 - val_loss: 0.4177 - val_accuracy: 0.8122 - val_f1: 0.8122\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 57s 3ms/sample - loss: 0.4049 - accuracy: 0.8202 - f1: 0.8203 - val_loss: 0.3895 - val_accuracy: 0.8261 - val_f1: 0.8262\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 55s 3ms/sample - loss: 0.3783 - accuracy: 0.8338 - f1: 0.8339 - val_loss: 0.3657 - val_accuracy: 0.8349 - val_f1: 0.8349\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 60s 3ms/sample - loss: 0.3613 - accuracy: 0.8452 - f1: 0.8452 - val_loss: 0.3921 - val_accuracy: 0.8240 - val_f1: 0.8240\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 57s 3ms/sample - loss: 0.3360 - accuracy: 0.8525 - f1: 0.8525 - val_loss: 0.3520 - val_accuracy: 0.8424 - val_f1: 0.8424\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 59s 3ms/sample - loss: 0.3197 - accuracy: 0.8623 - f1: 0.8624 - val_loss: 0.3567 - val_accuracy: 0.8427 - val_f1: 0.8426\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 57s 3ms/sample - loss: 0.3049 - accuracy: 0.8681 - f1: 0.8682 - val_loss: 0.3496 - val_accuracy: 0.8467 - val_f1: 0.8467\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 57s 3ms/sample - loss: 0.2860 - accuracy: 0.8786 - f1: 0.8786 - val_loss: 0.3451 - val_accuracy: 0.8514 - val_f1: 0.8514\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = get_model(vocab_size,[weights],300,train = False)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(trainx,trainy,\n",
    "                    epochs = 10,\n",
    "                    batch_size = 100,\n",
    "                    validation_data=(testx,testy),\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_test_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-84ed580b159a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_test_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'get_test_score' is not defined"
     ]
    }
   ],
   "source": [
    "get_test_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3% increase form 83 to 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = get_model(vocab_size,[weights],300,train = True)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(trainx,trainy,\n",
    "                    epochs = 3,\n",
    "                    batch_size = 100,\n",
    "                    validation_data=(testx,testy),\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_test_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Normalizing the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of weight (106124, 300)\n",
      "normalizing...\n",
      "shape of weight (106124, 300)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "norm = Normalizer()\n",
    "print(f'shape of weight {weights.shape}')\n",
    "\n",
    "print('normalizing...')\n",
    "\n",
    "weightsnorml = norm.fit_transform(weights)\n",
    "\n",
    "print(f'shape of weight {weightsnorml.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 56s 3ms/sample - loss: 0.3703 - accuracy: 0.8381 - f1: 0.8381 - val_loss: 0.3617 - val_accuracy: 0.8373 - val_f1: 0.8373\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 59s 3ms/sample - loss: 0.3588 - accuracy: 0.8420 - f1: 0.8420 - val_loss: 0.3546 - val_accuracy: 0.8440 - val_f1: 0.8440\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 55s 3ms/sample - loss: 0.3559 - accuracy: 0.8455 - f1: 0.8456 - val_loss: 0.3543 - val_accuracy: 0.8472 - val_f1: 0.8473\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 59s 3ms/sample - loss: 0.3429 - accuracy: 0.8502 - f1: 0.8502 - val_loss: 0.3564 - val_accuracy: 0.8474 - val_f1: 0.8475\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 57s 3ms/sample - loss: 0.3371 - accuracy: 0.8544 - f1: 0.8544 - val_loss: 0.3593 - val_accuracy: 0.8463 - val_f1: 0.8463\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 57s 3ms/sample - loss: 0.3319 - accuracy: 0.8579 - f1: 0.8579 - val_loss: 0.3444 - val_accuracy: 0.8509 - val_f1: 0.8509\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 61s 3ms/sample - loss: 0.3242 - accuracy: 0.8591 - f1: 0.8592 - val_loss: 0.3598 - val_accuracy: 0.8434 - val_f1: 0.8434\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 58s 3ms/sample - loss: 0.3172 - accuracy: 0.8648 - f1: 0.8648 - val_loss: 0.3457 - val_accuracy: 0.8560 - val_f1: 0.8560\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 57s 3ms/sample - loss: 0.3124 - accuracy: 0.8664 - f1: 0.8664 - val_loss: 0.3705 - val_accuracy: 0.8360 - val_f1: 0.8360\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 58s 3ms/sample - loss: 0.3054 - accuracy: 0.8712 - f1: 0.8712 - val_loss: 0.3496 - val_accuracy: 0.8488 - val_f1: 0.8488\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = get_model(vocab_size,[weightsnorml],300,train = False)\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "history = model.fit(trainx,trainy,\n",
    "                    epochs = 10,\n",
    "                    batch_size = 100,\n",
    "                    validation_data=(testx,testy),\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# observations:\n",
    "In preprocessing \n",
    "\n",
    "1. apply spacing to all punct. except . increase the coverage.\n",
    "2. correcting the spellings also helps.\n",
    "3. recoving the html tags\n",
    "4. also stemming increaes word coverage but this stem shoudnt be applied while preprocessing but while\n",
    "   finding the word embedding.\n",
    "   \n",
    "   explaination. Stemming might help find word embeddings of words found in oov. but apply it at the \n",
    "   early preprcess steps is a bad idea, as it might stem other words which were present in the embedding \n",
    "   matrix but their stemming is not.\n",
    "\n",
    "5. another important thing is taking care of spacing \n",
    "   in our approach this is taken care by the correction method.but we could also design an regix to care of this\n",
    "   in the future.\n",
    "\n",
    "\n",
    "Our current approach \n",
    "\n",
    "\n",
    "\n",
    "In Modeling \n",
    "1. setting training to true reduces the number of epochs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Modification and Lr Scheduling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM netrow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "def get_model2(vocab_size,weights = None,dim = 128,train = False,maxlen = 100):\n",
    "  \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,\n",
    "                      dim,\n",
    "                      weights= weights,\n",
    "                      input_length=maxlen,\n",
    "                      trainable=train))\n",
    "\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(LSTM(300,return_sequences = True))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "#     model.add(Dense(1024, activation='sigmoid'))\n",
    "#     model.add(Dropout(0.8))\n",
    "\n",
    "    model.add(Dense(1024, activation='sigmoid'))\n",
    "    model.add(Dropout(0.8))\n",
    "\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=Adam(learning_rate = 0.001),\n",
    "                  metrics = ['accuracy',f1])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = get_model2(vocab_size,weights = [weights], dim = 300,train = True)\n",
    "model.summary()\n",
    "history1 = model.fit(trainx,trainy,\n",
    "                    epochs = 5,\n",
    "                    batch_size = 100,\n",
    "                    validation_data=(testx,testy),\n",
    "                \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clr import CyclicLR\n",
    "\n",
    "\n",
    "step_size = len(train_dataset) // 100\n",
    "\n",
    "clr = CyclicLR(base_lr=0.001, max_lr=0.001,\n",
    "                                step_size= step_size, mode='triangular')\n",
    "\n",
    "\n",
    "step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = get_model2(vocab_size,weights = [weights], dim = 300,train = True)\n",
    "model.summary()\n",
    "history1 = model.fit(trainx,trainy,\n",
    "                    epochs = 5,\n",
    "                    batch_size = 100,\n",
    "                    validation_data=(testx,testy),\n",
    "                    callbacks = [clr]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>with all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "      <td>the classic war of the worlds   by timothy hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>the film starts with a manager  nicholas bell ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>it must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review  \\\n",
       "0  5814_8          1  With all this stuff going down at the moment w...   \n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...   \n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3  3630_4          0  It must be assumed that those who praised this...   \n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "\n",
       "                                           processed  \n",
       "0  with all this stuff going down at the moment w...  \n",
       "1   the classic war of the worlds   by timothy hi...  \n",
       "2  the film starts with a manager  nicholas bell ...  \n",
       "3  it must be assumed that those who praised this...  \n",
       "4  superbly trashy and wondrously unpretentious 8...  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>Naturally in a film who's main themes are of m...</td>\n",
       "      <td>1</td>\n",
       "      <td>naturally in a film who ' s main themes are of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>This movie is a disaster within a disaster fil...</td>\n",
       "      <td>0</td>\n",
       "      <td>this movie is a disaster within a disaster fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>All in all, this is a movie for kids. We saw i...</td>\n",
       "      <td>0</td>\n",
       "      <td>all in all  this is a movie for kids we saw it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>Afraid of the Dark left me with the impression...</td>\n",
       "      <td>0</td>\n",
       "      <td>afraid of the dark left me with the impression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>A very accurate depiction of small time mob li...</td>\n",
       "      <td>1</td>\n",
       "      <td>a very accurate depiction of small time mob li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             review  sentiment  \\\n",
       "0  12311_10  Naturally in a film who's main themes are of m...          1   \n",
       "1    8348_2  This movie is a disaster within a disaster fil...          0   \n",
       "2    5828_4  All in all, this is a movie for kids. We saw i...          0   \n",
       "3    7186_2  Afraid of the Dark left me with the impression...          0   \n",
       "4   12128_7  A very accurate depiction of small time mob li...          1   \n",
       "\n",
       "                                           processed  \n",
       "0  naturally in a film who ' s main themes are of...  \n",
       "1  this movie is a disaster within a disaster fil...  \n",
       "2  all in all  this is a movie for kids we saw it...  \n",
       "3  afraid of the dark left me with the impression...  \n",
       "4  a very accurate depiction of small time mob li...  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "\n",
    "\n",
    "pk.dump(train_dataset,open('train_dataset','wb'))\n",
    "\n",
    "pk.dump(test_dataset,open('test_dataset','wb'))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "WON-vKI_esuB",
    "eQzuz7HErKS1",
    "f84Hl_rWvARF"
   ],
   "name": "Untitled3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
