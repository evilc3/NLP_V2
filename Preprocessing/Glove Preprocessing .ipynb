{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim: Preprocessing Steps for Pre-Trained Embeddings\n",
    "\n",
    "The standard preprocessing steps cannot be applied to vector pre-trained embeddings. Our main objectitive should be to get the dataset as close to the pre-trained embeddings vectors being used. \n",
    "\n",
    "\n",
    "1. Blindly applying stemming or lemmatization might change the words which might reduce the word coverage. \n",
    "2. Simply removing embeddings doesnt help. Take the following example\n",
    "   The glove embedding has seperate  vector embedding for it and 's so removing ' would create a new word its \n",
    "   which doesnt have embeddings \n",
    "3. For digits above 9 can be replaced by #\n",
    "\n",
    "\n",
    "Things covered in this notebook\n",
    "1. correcting misspellings two methods \n",
    "2. adding spaces to punctuations \n",
    "3. mapping punctuations and special characters \n",
    "4. mapping numbers larger than 9 to #'s\n",
    "5. contraction correction \n",
    "\n",
    "I have Used the Stackoverflow dataset here.Thi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import string \n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "glove_filepath = '../../../kaggle/embedding/glove.6B/glove.6B.100d.txt'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the embeddings from glove 100d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(file):\n",
    "    \n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "  \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "word_matrix = load_embed(glove_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "print(len(word_matrix.keys()))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have vector embeddings for 400000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Have a close look at the different words , digits and other special characters if any in the glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:00<00:00, 564933.56it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "we need to know \n",
    "\n",
    "1. numbers present \n",
    "2. punctuation present \n",
    "3. emojies\n",
    "4. words \n",
    "5. characters \n",
    "\n",
    "\n",
    "'''\n",
    "numbers = []\n",
    "words = []\n",
    "chars = []\n",
    "punctuations = []\n",
    "not_ascii = []\n",
    "words_with_apostrophe = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def is_char(x):\n",
    "    \n",
    "    x = ord(x)\n",
    "    \n",
    "    if x >=65 and x<=90 or x>= 97 and x <= 122:\n",
    "        return True\n",
    "        #meaning its a character \n",
    "    else:\n",
    "        return False\n",
    "        #not a character \n",
    "        \n",
    "        \n",
    "\n",
    "for word in tqdm(word_matrix.keys()):\n",
    "    \n",
    "     if word.isdigit():\n",
    "            numbers.append(word)\n",
    "     \n",
    "     elif  len(word) == 1 and not is_char(word):\n",
    "            punctuations.append(word)\n",
    "            \n",
    "     elif len(word) == 1 and is_char(word):\n",
    "            chars.append(word)\n",
    "     \n",
    "     elif not word.isascii():\n",
    "            not_ascii.append(word)\n",
    "            \n",
    "     elif word[0] == \"'\":\n",
    "          words_with_apostrophe.append(word)\n",
    "     else:\n",
    "            words.append(word)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of digits 3124\n",
      "number of characters 26\n",
      "number of not_ascii 9966\n",
      "number of punctuations 32\n",
      "number of words 386743\n",
      "number of words with apostophy 109\n"
     ]
    }
   ],
   "source": [
    "print(f'number of digits {len(numbers)}')\n",
    "print(f'number of characters {len(chars)}')\n",
    "print(f'number of not_ascii {len(not_ascii)}')\n",
    "print(f'number of punctuations {len(punctuations)}')\n",
    "print(f'number of words {len(words)}')\n",
    "print(f'number of words with apostophy {len(words_with_apostrophe)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chars\n",
    "\n",
    "## glove has all 26 characts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a i e s b x c r d m t v g p q y n f k o l w h u j z "
     ]
    }
   ],
   "source": [
    "for i in chars:\n",
    "    print(i,end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words or letters with Apostrochy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s '' 're 've 'm 'll 'd 'em '70s '60s '80s '90s 'n' '50s 'n '96 '40s '98 '30s '97 '95 'cause '99 '94 'til '92 '20s '93 '08 '06 '04 '88 '91 '86 '07 '05 '89 '68 '87 '84 '90 '09 '03 '67 '72 '85 '69 '76 '02 '78 'twas '82 '74 '64 '79 '57 '01 '83 '66 '60 '75 '10 '65 'till '81 '73 '77 '70 '80 '62 '59 '71 '63 '58 '00 '56 '50 '61 '55 '48 '49 '54 '51 '30 '27 '11 '12 '53 '13 '47 '32 '52 '46 '20 '36 '45 '29 '34 '40 '39 '28 '38 '42 '41 '15 '37 '25 '14 '44 "
     ]
    }
   ],
   "source": [
    "for i in words_with_apostrophe:\n",
    "    print(i,end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to Check Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    \n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Check If un processed  Improves our word Coverage for GLove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unknown_words(text):\n",
    "    \n",
    "    vocab_raw = build_vocab(text)\n",
    "    unk_words_raw = check_coverage(vocab_raw,word_matrix)\n",
    "    \n",
    "    return unk_words_raw\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stackoverflow Dataset\n",
    "### The  processed_text column is results of apply  preprcessing steps which include removing stopwrds,html tags, stemming. I have included them for comparision with the new pre-precessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "\n",
    "dataset = pk.load(open('df_questions','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tags</th>\n",
       "      <th>procesed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010-07-19T19:14:44Z</td>\n",
       "      <td>272</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>Last year, I read a blog post from Brendan O'C...</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>[machine-learning]</td>\n",
       "      <td>the two cultur statist machin learn last year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2010-07-19T19:24:36Z</td>\n",
       "      <td>4</td>\n",
       "      <td>Forecasting demographic census</td>\n",
       "      <td>What are some of the ways to forecast demograp...</td>\n",
       "      <td>Forecasting demographic census What are some o...</td>\n",
       "      <td>[forecasting]</td>\n",
       "      <td>forecast demograph censu what way forecast de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2010-07-19T19:25:39Z</td>\n",
       "      <td>208</td>\n",
       "      <td>Bayesian and frequentist reasoning in plain En...</td>\n",
       "      <td>How would you describe in plain English the ch...</td>\n",
       "      <td>Bayesian and frequentist reasoning in plain En...</td>\n",
       "      <td>[bayesian]</td>\n",
       "      <td>bayesian frequentist reason plain english how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2010-07-19T19:28:44Z</td>\n",
       "      <td>138</td>\n",
       "      <td>What is the meaning of p values and t values i...</td>\n",
       "      <td>After taking a statistics course and then tryi...</td>\n",
       "      <td>What is the meaning of p values and t values i...</td>\n",
       "      <td>[hypothesis-testing, t-test, p-value, interpre...</td>\n",
       "      <td>what mean valu valu statist test after take s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2010-07-19T19:31:47Z</td>\n",
       "      <td>58</td>\n",
       "      <td>Examples for teaching: Correlation does not me...</td>\n",
       "      <td>There is an old saying: \"Correlation does not ...</td>\n",
       "      <td>Examples for teaching: Correlation does not me...</td>\n",
       "      <td>[correlation]</td>\n",
       "      <td>exampl teach correl causat there correl causa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  OwnerUserId          CreationDate  Score  \\\n",
       "0   6          5.0  2010-07-19T19:14:44Z    272   \n",
       "1  21         59.0  2010-07-19T19:24:36Z      4   \n",
       "2  22         66.0  2010-07-19T19:25:39Z    208   \n",
       "3  31         13.0  2010-07-19T19:28:44Z    138   \n",
       "4  36          8.0  2010-07-19T19:31:47Z     58   \n",
       "\n",
       "                                               Title  \\\n",
       "0  The Two Cultures: statistics vs. machine learn...   \n",
       "1                     Forecasting demographic census   \n",
       "2  Bayesian and frequentist reasoning in plain En...   \n",
       "3  What is the meaning of p values and t values i...   \n",
       "4  Examples for teaching: Correlation does not me...   \n",
       "\n",
       "                                                Body  \\\n",
       "0  Last year, I read a blog post from Brendan O'C...   \n",
       "1  What are some of the ways to forecast demograp...   \n",
       "2  How would you describe in plain English the ch...   \n",
       "3  After taking a statistics course and then tryi...   \n",
       "4  There is an old saying: \"Correlation does not ...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  The Two Cultures: statistics vs. machine learn...   \n",
       "1  Forecasting demographic census What are some o...   \n",
       "2  Bayesian and frequentist reasoning in plain En...   \n",
       "3  What is the meaning of p values and t values i...   \n",
       "4  Examples for teaching: Correlation does not me...   \n",
       "\n",
       "                                                Tags  \\\n",
       "0                                 [machine-learning]   \n",
       "1                                      [forecasting]   \n",
       "2                                         [bayesian]   \n",
       "3  [hypothesis-testing, t-test, p-value, interpre...   \n",
       "4                                      [correlation]   \n",
       "\n",
       "                                       procesed_text  \n",
       "0   the two cultur statist machin learn last year...  \n",
       "1   forecast demograph censu what way forecast de...  \n",
       "2   bayesian frequentist reason plain english how...  \n",
       "3   what mean valu valu statist test after take s...  \n",
       "4   exampl teach correl causat there correl causa...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Check the Coverage on the processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 10.78% of vocab\n",
      "Found embeddings for  74.24% of all text\n"
     ]
    }
   ],
   "source": [
    "unk_words_raw = get_unknown_words(dataset['procesed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Lest Check the Coverage on UnProcessedText \n",
    "\n",
    "## The unprocessed text will consist of title + body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_text = dataset['Title'] + dataset['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 4.66% of vocab\n",
      "Found embeddings for  70.01% of all text\n"
     ]
    }
   ],
   "source": [
    "unk_words_raw = get_unknown_words(unprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok by the old processing style we were able to increase the text coverage by 4% and vocab coverage by about 6%\n",
    "\n",
    "## Lets take a look at the unknown words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 280153), ('The', 40999), ('&lt;-', 28677), (\"I'm\", 23018)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_words_raw[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowering all words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 5.75% of vocab\n",
      "Found embeddings for  78.42% of all text\n"
     ]
    }
   ],
   "source": [
    "unprocessed_text_lower = unprocessed_text.map(lambda x : x.lower())\n",
    "unk_words_raw = get_unknown_words(unprocessed_text_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok by just lowering the words we increased our coverage by 8%. We could have used a glove embedding which contains a larger vocab but that would also increase the memory requirment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('&lt;-', 28677) (\"i'm\", 23332) ('0,', 18219) ('$$', 12435) ('however,', 11696) (\"don't\", 10849) ('1l,', 10619) ('1,', 10428) ('2l,', 8909) ('&gt;', 8486) (\"i've\", 8459) ('data.', 6631) ('example,', 6061) (\"it's\", 5969) ('data,', 5330) ('so,', 5258) ('model.', 4935) ('&lt;', 4852) ('$x$', 4734) ('2,', 4423) ('is,', 4208) (\"can't\", 4150) ('is:', 4109) ('\\\\\\\\', 4046) ('0l,', 4017) (\"doesn't\", 3995) (\"i'd\", 3970) ('this?', 3957) ('variables.', 3926) ('model,', 3925) ('3l,', 3719) ('(or', 3597) ('1)', 3512) ('(i', 3399) ('$n$', 3328) ('it.', 3282) ('na,', 3263) ('distribution.', 3259) ('$y$', 3221) ('now,', 3002) ('this:', 2964) ('\\\\sim', 2875) ('3,', 2777) (\"let's\", 2773) ('variable.', 2770) ('variables,', 2766) ('(the', 2749) ('(e.g.', 2731) ('(i.e.', 2712) ('(and', 2690) "
     ]
    }
   ],
   "source": [
    "for i in unk_words_raw[0:50]:\n",
    "    print(i,end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding space to  punctuations \n",
    "\n",
    "Will add spaces to all punctuations except -,., and ' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "puncts = ['´',',', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "\n",
    "# no_spacing = [\"'\",'.','-']\n",
    "\n",
    "to_replace = '´'\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    \n",
    "    x = str(x)\n",
    "    \n",
    "    #will convert ´ to '\n",
    "    \n",
    "    if to_replace in x:\n",
    "        x = x.replace(to_replace,\"'\")\n",
    "    \n",
    "    \n",
    "    for punct in puncts:\n",
    "        if punct  == \"'\":\n",
    "            x = x.replace(punct, f' {punct}')\n",
    "        else:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    \n",
    " \n",
    "    #will convert e.g to eg\n",
    "    if '.' in x:\n",
    "        x = x.replace('.','')\n",
    "        \n",
    "       \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 14.20% of vocab\n",
      "Found embeddings for  95.67% of all text\n"
     ]
    }
   ],
   "source": [
    "cleaned_lower_text = unprocessed_text_lower.map(clean_text)\n",
    "unk_words_raw = get_unknown_words(cleaned_lower_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok Wow !!! We have 95.67% text coverage. But we cover only 14.20% of unique words lets take a look at the words with no embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('â', 27435),\n",
       " ('mathbf', 7528),\n",
       " ('\\x80\\x99', 5908),\n",
       " ('\\x80\\x98', 5323),\n",
       " ('sqrt', 5090),\n",
       " ('0l', 4190),\n",
       " ('mathbb', 3668),\n",
       " ('dataframe', 2783),\n",
       " ('infty', 2765),\n",
       " ('mathcal', 2670)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_words_raw[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### '\\x80\\x98' are spaces and will need to take care of them as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another thing we could do is try replacing contractions with words \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \n",
    "                       \"aren't\": \"are not\",\n",
    "                       \"can't\": \"cannot\", \n",
    "                       \"'cause\": \"because\",\n",
    "                       \"could've\": \"could have\", \n",
    "                       \"couldn't\": \"could not\", \n",
    "                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def known_contractions(embed):\n",
    "    known = []\n",
    "    for contract in contraction_mapping:\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Known Contractions -\n",
      "   Glove :\n",
      "[\"'cause\", \"ma'am\", \"o'clock\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"- Known Contractions -\")\n",
    "print(\"   Glove :\")\n",
    "print(known_contractions(word_matrix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have embeddings for only 3 contractions from our list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contractions in raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_raw = build_vocab(unprocessed_text)\n",
    "contaction_list = known_contractions(vocab_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contaction_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contaction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"ain't\", \"aren't\", \"can't\", \"'cause\", \"could've\", \"couldn't\"]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contaction_list[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'cause\n"
     ]
    }
   ],
   "source": [
    "for i in contaction_list:\n",
    "    if i in word_matrix:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only one Word was found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clearning contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "text_no_contractions = unprocessed_text.map(lambda x : clean_contractions(x,contraction_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 4.66% of vocab\n",
      "Found embeddings for  70.46% of all text\n"
     ]
    }
   ],
   "source": [
    "unk_words_raw = get_unknown_words(text_no_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No improvemet may using embeddings with more contractions can help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PUNCTUATION MAPPING\n",
    "\n",
    "The below step must not be confused with the earlier step there we just seperated the punctuations by space here \n",
    "we try to map them. Its like finding word embedding of a synonmy of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "\n",
    "punct = punctuations\n",
    "\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \n",
    "                 \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \n",
    "                 \"√\": \" sqrt \", \"×\": \"x\", \"²\": \n",
    "                 \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \n",
    "                 \"'\", \"_\": \"-\", \n",
    "                 \"`\": \"'\", '“': \n",
    "                 '\"', '”': '\"', \n",
    "                 '“': '\"', \"£\": \n",
    "                 \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def punct_mapper(text, mapping):\n",
    "    \n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_punt_map = unprocessed_text_lower.map(lambda x : punct_mapper(x,punct_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 14.41% of vocab\n",
      "Found embeddings for  95.93% of all text\n"
     ]
    }
   ],
   "source": [
    "unk_words_raw = get_unknown_words(text_punt_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('â', 27435) ('mathbf', 7528) ('\\x80\\x99', 5908) ('\\x80\\x98', 5323) ('sqrt', 5090) ('0l', 4190) ('mathbb', 3668) ('dataframe', 2783) ('infty', 2765) ('mathcal', 2670) ('nbsp', 2627) ('rnorm', 2620) ('boldsymbol', 2373) ('î', 2272) ('mathrm', 2145) ('leq', 2136) ('ldots', 2108) ('±', 2019) ('lmer', 2017) ('coef', 1576) ('datai', 1515) ('00000', 1492) ('covariate', 1374) ('mydata', 1319) ('8l', 1314) ('\\x80\\x9d', 1285) ('asfactor', 1266) ('varepsilon', 1264) ('cbind', 1196) ('signif', 1190) ('regressioni', 1164) ('nrow', 1150) ('\\x95\\x90', 1148) ('bmatrix', 1120) ('setseed', 1095) ('lme4', 1086) ('rightarrow', 1076) ('ï', 1067) ('\\x80\\x99s', 999) ('modeli', 977) "
     ]
    }
   ],
   "source": [
    "for i in unk_words_raw[0:40]:\n",
    "    print(i,end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct Mispelled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mispell_dict = {'colour': 'color', \n",
    "                'centre': 'center', \n",
    "                'favourite': 'favorite', \n",
    "                'travelling': 'traveling',\n",
    "                'counselling': 'counseling',\n",
    "                'theatre': 'theater', 'cancelled':\n",
    "                'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def correct_spelling(x, dic):\n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "        \n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mispelled = unprocessed_text_lower.map(lambda x : correct_spelling(x,mispell_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 14.40% of vocab\n",
      "Found embeddings for  95.92% of all text\n"
     ]
    }
   ],
   "source": [
    "oov_glove = get_unknown_words(text_mispelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correcting Spelling doesnt help much there is another method used later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking care of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(x):\n",
    "    \n",
    "    \n",
    "#     x = re.sub('[0-9]{5}','#####',x)\n",
    "#     x = re.sub('[0-9]{4}','###'',x)\n",
    "    x = re.sub('[0-9]{3}',' ### ',x)\n",
    "    x = re.sub('[0-9]{2}',' ## ',x)\n",
    "    \n",
    "    return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 31.52% of vocab\n",
      "Found embeddings for  97.37% of all text\n"
     ]
    }
   ],
   "source": [
    "text_clean_numbers = cleaned_lower_text.map(clean_numbers)\n",
    "ov_glove = get_unknown_words(text_clean_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('â', 27435),\n",
       " ('mathbf', 7528),\n",
       " ('\\x80\\x99', 5908),\n",
       " ('\\x80\\x98', 5327),\n",
       " ('sqrt', 5092),\n",
       " ('0l', 4758),\n",
       " ('mathbb', 3668),\n",
       " ('dataframe', 2783),\n",
       " ('infty', 2765),\n",
       " ('mathcal', 2670),\n",
       " ('nbsp', 2627),\n",
       " ('rnorm', 2620),\n",
       " ('boldsymbol', 2373),\n",
       " ('î', 2272),\n",
       " ('leq', 2154),\n",
       " ('mathrm', 2145),\n",
       " ('ldots', 2111),\n",
       " ('lmer', 2043),\n",
       " ('±', 2019),\n",
       " ('8l', 1612),\n",
       " ('coef', 1576),\n",
       " ('datai', 1515),\n",
       " ('covariate', 1374),\n",
       " ('mydata', 1319),\n",
       " ('\\x80\\x9d', 1285),\n",
       " ('asfactor', 1266),\n",
       " ('varepsilon', 1264),\n",
       " ('²', 1202),\n",
       " ('cbind', 1196),\n",
       " ('signif', 1190),\n",
       " ('regressioni', 1164),\n",
       " ('nrow', 1150),\n",
       " ('\\x95\\x90', 1148),\n",
       " ('bmatrix', 1120),\n",
       " ('setseed', 1095),\n",
       " ('lme4', 1086),\n",
       " ('rightarrow', 1078),\n",
       " ('ï', 1067),\n",
       " ('\\x80\\x99s', 999),\n",
       " ('\\x80\\x93', 986),\n",
       " ('modeli', 977),\n",
       " ('rmse', 902),\n",
       " ('glmer', 886),\n",
       " ('cdots', 870),\n",
       " ('runif', 857),\n",
       " ('reml', 811),\n",
       " ('ncol', 803),\n",
       " ('garch', 798),\n",
       " ('dnorm', 790),\n",
       " ('\\x80', 761),\n",
       " ('dfrac', 752),\n",
       " ('aov', 741),\n",
       " ('randomforest', 740),\n",
       " ('textbf', 737),\n",
       " ('loglik', 736),\n",
       " ('variablesi', 706),\n",
       " ('pmatrix', 692),\n",
       " ('glmnet', 692),\n",
       " ('newdata', 684),\n",
       " ('neq', 679),\n",
       " ('autoarima', 667),\n",
       " ('\\x80\\x99t', 666),\n",
       " ('multicollinearity', 650),\n",
       " ('surv', 646),\n",
       " ('distributioni', 638),\n",
       " ('var1', 598),\n",
       " ('pvalue', 588),\n",
       " ('¦', 570),\n",
       " ('nlme', 569),\n",
       " ('scikit', 565),\n",
       " ('propto', 563),\n",
       " ('\\x80\\x99m', 560),\n",
       " ('ancova', 555),\n",
       " ('stddev', 553),\n",
       " ('asnumeric', 535),\n",
       " ('rownames', 534),\n",
       " ('operatorname', 531),\n",
       " ('\\x95\\x91', 512),\n",
       " ('glmm', 507),\n",
       " ('chisq', 505),\n",
       " ('model1', 503),\n",
       " ('manova', 498),\n",
       " ('mbox', 494),\n",
       " ('widehat', 493),\n",
       " ('var2', 487),\n",
       " ('coxph', 481),\n",
       " ('¼', 480),\n",
       " ('iv2', 477),\n",
       " ('ylab', 475),\n",
       " ('pacf', 470),\n",
       " ('iv1', 468),\n",
       " ('knn', 466),\n",
       " ('resid', 465),\n",
       " ('analysisi', 457),\n",
       " ('xreg', 452),\n",
       " ('xlab', 448),\n",
       " ('regressor', 440),\n",
       " ('vdots', 439),\n",
       " ('cointegration', 439),\n",
       " ('¸', 438),\n",
       " ('variablei', 438),\n",
       " ('forall', 437),\n",
       " ('textrm', 418),\n",
       " ('ttest', 411),\n",
       " ('ifelse', 410),\n",
       " ('rpart', 398),\n",
       " ('colnames', 396),\n",
       " ('abline', 396),\n",
       " ('fit1', 393),\n",
       " ('kmeans', 390),\n",
       " ('eqnarray', 386),\n",
       " ('ggplot', 383),\n",
       " ('collinearity', 382),\n",
       " ('seriesi', 380),\n",
       " ('enwikipediaorg', 379),\n",
       " ('\\x83', 373),\n",
       " ('asmatrix', 369),\n",
       " ('hline', 363),\n",
       " ('rht', 362),\n",
       " ('params', 355),\n",
       " ('displaystyle', 353),\n",
       " ('lty', 353),\n",
       " ('modelsi', 353),\n",
       " ('ylim', 352),\n",
       " ('rbind', 346),\n",
       " ('mtcars', 346),\n",
       " ('heteroscedasticity', 340),\n",
       " ('fit2', 340),\n",
       " ('model2', 340),\n",
       " ('timeseries', 339),\n",
       " ('heteroskedasticity', 337),\n",
       " ('ggplot2', 334),\n",
       " ('lsmeans', 332),\n",
       " ('chi2', 327),\n",
       " ('binom', 323),\n",
       " ('datapoints', 322),\n",
       " ('\\x88\\x92', 321),\n",
       " ('©', 319),\n",
       " ('optim', 319),\n",
       " ('lstm', 316),\n",
       " ('readcsv', 315),\n",
       " ('beta1', 312),\n",
       " ('\\x89', 309),\n",
       " ('qquad', 306),\n",
       " ('data1', 304),\n",
       " ('sklearn', 295),\n",
       " ('rbinom', 294),\n",
       " ('svms', 291),\n",
       " ('hyperparameters', 290),\n",
       " ('asdataframe', 287),\n",
       " ('softmax', 284),\n",
       " ('ar1', 282),\n",
       " ('rangle', 278),\n",
       " ('langle', 276),\n",
       " ('scatterplot', 274),\n",
       " ('boxplot', 272),\n",
       " ('data2', 270),\n",
       " ('libsvm', 269),\n",
       " ('mod1', 269),\n",
       " ('coeff', 264),\n",
       " ('group1', 262),\n",
       " ('naaction', 262),\n",
       " ('timepoint', 260),\n",
       " ('pymc', 260),\n",
       " ('valuesi', 260),\n",
       " ('attr', 259),\n",
       " ('£', 258),\n",
       " ('mlogit', 258),\n",
       " ('var3', 252),\n",
       " ('sapply', 251),\n",
       " ('mathscr', 250),\n",
       " ('anovas', 249),\n",
       " ('autocorrelated', 249),\n",
       " ('chisqtest', 244),\n",
       " ('¬', 240),\n",
       " ('»', 237),\n",
       " ('overdispersion', 237),\n",
       " ('df2', 235),\n",
       " ('confint', 230),\n",
       " ('nparray', 226),\n",
       " ('xlim', 225),\n",
       " ('lavaan', 224),\n",
       " ('varphi', 223),\n",
       " ('readtable', 223),\n",
       " ('mcnemar', 223),\n",
       " ('intr', 220),\n",
       " ('disp', 219),\n",
       " ('hspace', 218),\n",
       " ('adaboost', 217),\n",
       " ('nnet', 215),\n",
       " ('glms', 214),\n",
       " ('scipy', 213),\n",
       " ('resample', 212),\n",
       " ('underset', 211),\n",
       " ('factor1', 211),\n",
       " ('modelmatrix', 209),\n",
       " ('·', 207),\n",
       " ('glmnb', 206),\n",
       " ('pnorm', 206),\n",
       " ('subj', 206),\n",
       " ('distributionsi', 206),\n",
       " ('setosa', 205),\n",
       " ('vecm', 203),\n",
       " ('functioni', 203),\n",
       " ('stdev', 203),\n",
       " ('naomit', 202),\n",
       " ('sefit', 201),\n",
       " ('\\x80\\x99ve', 200),\n",
       " ('mgcv', 198),\n",
       " ('testdata', 196),\n",
       " ('whuber', 196),\n",
       " ('group2', 194),\n",
       " ('glht', 192),\n",
       " ('mod2', 192),\n",
       " ('mu1', 189),\n",
       " ('dunif', 189),\n",
       " ('nabla', 188),\n",
       " ('minitab', 188),\n",
       " ('cvglmnet', 187),\n",
       " ('factor2', 187),\n",
       " ('probs', 185),\n",
       " ('mtry', 185),\n",
       " ('homoscedasticity', 184),\n",
       " ('¥', 183),\n",
       " ('bfgs', 182),\n",
       " ('equiv', 181),\n",
       " ('°', 180),\n",
       " ('mfrow', 180),\n",
       " ('missingness', 180),\n",
       " ('ecdf', 178),\n",
       " ('autoencoder', 177),\n",
       " ('stackoverflow', 177),\n",
       " ('dgamma', 177),\n",
       " ('yhat', 175),\n",
       " ('kstest', 175),\n",
       " ('winbugs', 173),\n",
       " ('endogeneity', 172),\n",
       " ('loglikelihood', 171),\n",
       " ('quasipoisson', 171),\n",
       " ('\\x80\\x8e', 170),\n",
       " ('xgboost', 170),\n",
       " ('kpss', 170),\n",
       " ('lapply', 169),\n",
       " ('overfit', 168),\n",
       " ('0i', 166),\n",
       " ('vcov', 165),\n",
       " ('iqr', 164),\n",
       " ('2sls', 164),\n",
       " ('mnist', 162),\n",
       " ('resultsi', 162),\n",
       " ('tbats', 162),\n",
       " ('classificationi', 162),\n",
       " ('\\x80\\x94', 161),\n",
       " ('1j', 160),\n",
       " ('overset', 160),\n",
       " ('varimax', 158),\n",
       " ('ezanova', 158),\n",
       " ('relu', 158),\n",
       " ('arimax', 158),\n",
       " ('traindata', 156),\n",
       " ('breusch', 156),\n",
       " ('eviews', 156),\n",
       " ('2y', 154),\n",
       " ('dput', 154),\n",
       " ('problemi', 154),\n",
       " ('groupsi', 154),\n",
       " ('maxit', 152),\n",
       " ('ntree', 151),\n",
       " ('¤', 151),\n",
       " ('anovai', 150),\n",
       " ('checkconv', 149),\n",
       " ('fm1', 149),\n",
       " ('lm1', 149),\n",
       " ('trcontrol', 148),\n",
       " ('asvector', 148),\n",
       " ('timepoints', 147),\n",
       " ('beta0', 147),\n",
       " ('matrixi', 147),\n",
       " ('variogram', 146),\n",
       " ('spssi', 146),\n",
       " ('expandgrid', 146),\n",
       " ('metafor', 146),\n",
       " ('pval', 145),\n",
       " ('\\x88\\x921', 145),\n",
       " ('\\x95', 143),\n",
       " ('cointegrated', 143),\n",
       " ('wilcoxtest', 143),\n",
       " ('beta2', 143),\n",
       " ('stderror', 143),\n",
       " ('dendrogram', 139),\n",
       " ('nsim', 139),\n",
       " ('heteroscedastic', 139),\n",
       " ('ma1', 138),\n",
       " ('sigma2', 137),\n",
       " ('time2', 137),\n",
       " ('qnorm', 136),\n",
       " ('sarima', 136),\n",
       " ('0j', 135),\n",
       " ('cumsum', 135),\n",
       " ('tensorflow', 135),\n",
       " ('valuei', 135),\n",
       " ('dimnames', 135),\n",
       " ('rpois', 135),\n",
       " ('mu2', 133),\n",
       " ('traincontrol', 133),\n",
       " ('fitdistr', 133),\n",
       " ('loocv', 133),\n",
       " ('bewert', 132),\n",
       " ('crossvalidated', 132),\n",
       " ('hyperparameter', 131),\n",
       " ('datatable', 130),\n",
       " ('statsmodels', 130),\n",
       " ('mcmcglmm', 130),\n",
       " ('stackexchange', 130),\n",
       " ('rescale', 129),\n",
       " ('samplesi', 129),\n",
       " ('µ', 129),\n",
       " ('alpha1', 128),\n",
       " ('prcomp', 128),\n",
       " ('\\x88', 128),\n",
       " ('subsamples', 128),\n",
       " ('stackoverflowcom', 127),\n",
       " ('time1', 127),\n",
       " ('pchisq', 126),\n",
       " ('timei', 126),\n",
       " ('sepallength', 125),\n",
       " ('randn', 125),\n",
       " ('dataseti', 125),\n",
       " ('hclust', 125),\n",
       " ('qqplot', 124),\n",
       " ('quasibinomial', 123),\n",
       " ('autoregression', 123),\n",
       " ('\\x9c\\x97', 122),\n",
       " ('y0', 122),\n",
       " ('clusteringi', 121),\n",
       " ('îµ', 120),\n",
       " ('asdate', 120),\n",
       " ('proptest', 120),\n",
       " ('estimationi', 119),\n",
       " ('¢', 119),\n",
       " ('test1', 118),\n",
       " ('pltplot', 118),\n",
       " ('stderr', 118),\n",
       " ('lemeshow', 118),\n",
       " ('glmmadmb', 117),\n",
       " ('unlist', 117),\n",
       " ('¯', 117),\n",
       " ('glmfit', 117),\n",
       " ('preds', 116),\n",
       " ('³', 116),\n",
       " ('questioni', 116),\n",
       " ('µí', 115),\n",
       " ('byrow', 115),\n",
       " ('variancei', 115),\n",
       " ('newdat', 114),\n",
       " ('validationi', 114),\n",
       " ('relevel', 114),\n",
       " ('sizei', 114),\n",
       " ('survreg', 113),\n",
       " ('pvalues', 113),\n",
       " ('model3', 113),\n",
       " ('twosided', 113),\n",
       " ('fixef', 113),\n",
       " ('qqnorm', 112),\n",
       " ('packagei', 111),\n",
       " ('lowertail', 111),\n",
       " ('multinom', 111),\n",
       " ('statai', 111),\n",
       " ('autocorrelations', 111),\n",
       " ('sparsearray', 110),\n",
       " ('vectorinstance', 110),\n",
       " ('condition1', 110),\n",
       " ('dpois', 110),\n",
       " ('boxplots', 109),\n",
       " ('covar', 109),\n",
       " ('dbscan', 108),\n",
       " ('mymodel', 107),\n",
       " ('mclust', 107),\n",
       " ('stackrel', 107),\n",
       " ('projectorg', 107),\n",
       " ('modelfit', 107),\n",
       " ('medoids', 106),\n",
       " ('ctree', 106),\n",
       " ('newcommand', 106),\n",
       " ('dat2', 105),\n",
       " ('d1a', 105),\n",
       " ('biplot', 105),\n",
       " ('var4', 105),\n",
       " ('errori', 104),\n",
       " ('coefficientsi', 104),\n",
       " ('howi', 103),\n",
       " ('hdbr', 103),\n",
       " ('polr', 103),\n",
       " ('otimes', 103),\n",
       " ('correlationi', 103),\n",
       " ('í', 103),\n",
       " ('theta1', 103),\n",
       " ('istate', 102),\n",
       " ('sleepstudy', 102),\n",
       " ('learningi', 102),\n",
       " ('overdispersed', 102),\n",
       " ('processi', 102),\n",
       " ('exp1', 101),\n",
       " ('mydf', 101),\n",
       " ('argmin', 101),\n",
       " ('intraclass', 101),\n",
       " ('sample1', 101),\n",
       " ('inits', 101),\n",
       " ('mvrnorm', 101),\n",
       " ('varcorr', 101),\n",
       " ('networki', 101),\n",
       " ('negbin', 100),\n",
       " ('studentized', 100),\n",
       " ('modeladd', 100),\n",
       " ('tfidf', 100),\n",
       " ('class1', 99),\n",
       " ('tukeyhsd', 99),\n",
       " ('arimasim', 99),\n",
       " ('linfct', 99),\n",
       " ('lmertest', 99),\n",
       " ('mplus', 98),\n",
       " ('umvue', 98),\n",
       " ('confounders', 98),\n",
       " ('matlabi', 98),\n",
       " ('rjags', 97),\n",
       " ('lmtest', 97),\n",
       " ('boxcox', 97),\n",
       " ('kaggle', 97),\n",
       " ('algorithmi', 97),\n",
       " ('nonstationary', 97),\n",
       " ('edit2', 97),\n",
       " ('gamlss', 96),\n",
       " ('rexp', 96),\n",
       " ('b1a', 95),\n",
       " ('sepalwidth', 95),\n",
       " ('parametersi', 95),\n",
       " ('fitdist', 95),\n",
       " ('effectsi', 95),\n",
       " ('predictioni', 95),\n",
       " ('day1', 94),\n",
       " ('tibshirani', 94),\n",
       " ('cointegrating', 94),\n",
       " ('rsquared', 94),\n",
       " ('bobyqa', 94),\n",
       " ('ymax', 93),\n",
       " ('petalwidth', 93),\n",
       " ('cdfs', 93),\n",
       " ('petallength', 93),\n",
       " ('shapirotest', 93),\n",
       " ('dtype', 93),\n",
       " ('ã\\x97', 93),\n",
       " ('xtreg', 93),\n",
       " ('samplesize', 92),\n",
       " ('designi', 92),\n",
       " ('interpretationi', 92),\n",
       " ('nchains', 92),\n",
       " ('asformula', 92),\n",
       " ('survfit', 92),\n",
       " ('testsi', 92),\n",
       " ('b1b', 91),\n",
       " ('testset', 91),\n",
       " ('test2', 91),\n",
       " ('autocovariance', 91),\n",
       " ('xtest', 91),\n",
       " ('npzeros', 90),\n",
       " ('multcomp', 90),\n",
       " ('theta2', 90),\n",
       " ('lambdamin', 90),\n",
       " ('logrank', 89),\n",
       " ('samplei', 89),\n",
       " ('colmeans', 88),\n",
       " ('y4', 88),\n",
       " ('outputi', 88),\n",
       " ('xrightarrow', 88),\n",
       " ('statisticsi', 88),\n",
       " ('exp2', 87),\n",
       " ('benjamini', 87),\n",
       " ('cortest', 87),\n",
       " ('ranef', 86),\n",
       " ('word2vec', 86),\n",
       " ('0x', 86),\n",
       " ('selectioni', 86),\n",
       " ('recode', 86),\n",
       " ('yvar', 86),\n",
       " ('dfresid', 86),\n",
       " ('¶', 86),\n",
       " ('wwwatsuclaedu', 86),\n",
       " ('sample2', 86),\n",
       " ('longrightarrow', 86),\n",
       " ('princomp', 86),\n",
       " ('changepoint', 86),\n",
       " ('glmermod', 85),\n",
       " ('dv1', 85),\n",
       " ('glimmix', 85),\n",
       " ('probabilityi', 85),\n",
       " ('meani', 85),\n",
       " ('lag1', 84),\n",
       " ('zeroinfl', 84),\n",
       " ('dplyr', 84),\n",
       " ('rsquare', 84),\n",
       " ('crossvalidation', 84),\n",
       " ('heteroskedastic', 83),\n",
       " ('siglevel', 83),\n",
       " ('corar1', 83),\n",
       " ('xbar', 83),\n",
       " ('pred1', 83),\n",
       " ('glmercontrol', 83),\n",
       " ('comp1', 82),\n",
       " ('posttest', 82),\n",
       " ('foreach', 82),\n",
       " ('ar2', 82),\n",
       " ('fishertest', 82),\n",
       " ('varimp', 82),\n",
       " ('methodi', 82),\n",
       " ('fit3', 81),\n",
       " ('subseteq', 81),\n",
       " ('minibatch', 81),\n",
       " ('argmax', 81),\n",
       " ('d1b', 80),\n",
       " ('resamples', 80),\n",
       " ('cranr', 80),\n",
       " ('clogit', 80),\n",
       " ('observationsi', 80),\n",
       " ('condition2', 79),\n",
       " ('lmfit', 79),\n",
       " ('derivs', 79),\n",
       " ('age2', 79),\n",
       " ('nplog', 79),\n",
       " ('statsstackexchangecom', 79),\n",
       " ('\\x81', 79),\n",
       " ('group3', 79),\n",
       " ('tseries', 79),\n",
       " ('\\x80\\x99d', 79),\n",
       " ('ndata', 78),\n",
       " ('naã', 78),\n",
       " ('naexclude', 78),\n",
       " ('groupb', 78),\n",
       " ('1y', 78),\n",
       " ('contrsum', 78),\n",
       " ('cramã', 78),\n",
       " ('predictorsi', 78),\n",
       " ('regressionin', 78),\n",
       " ('coefs', 78),\n",
       " ('kernlab', 77),\n",
       " ('chisquare', 77),\n",
       " ('leftrightarrow', 77),\n",
       " ('testingi', 77),\n",
       " ('rowsums', 77),\n",
       " ('jagsmodel', 77),\n",
       " ('bootci', 77),\n",
       " ('df0', 76),\n",
       " ('\\x8b', 76),\n",
       " ('indepvar', 76),\n",
       " ('dotsc', 76),\n",
       " ('type1', 76),\n",
       " ('logits', 76),\n",
       " ('naivebayes', 76),\n",
       " ('lengthout', 76),\n",
       " ('scipystats', 76),\n",
       " ('adftest', 76),\n",
       " ('measuresi', 76),\n",
       " ('lm2', 75),\n",
       " ('autoencoders', 75),\n",
       " ('rugarch', 75),\n",
       " ('matplotlibpyplot', 75),\n",
       " ('newx', 75),\n",
       " ('intervalsi', 75),\n",
       " ('mathsf', 74),\n",
       " ('resampled', 74),\n",
       " ('hmisc', 74),\n",
       " ('vglm', 74),\n",
       " ('pred2', 74),\n",
       " ('errorsi', 74),\n",
       " ('xvar', 73),\n",
       " ('importances', 73),\n",
       " ('ploti', 73),\n",
       " ('wwwdropboxcom', 73),\n",
       " ('untransformed', 73),\n",
       " ('tapply', 73),\n",
       " ('rbrace', 73),\n",
       " ('lbrace', 73),\n",
       " ('finalmodel', 73),\n",
       " ('tunegrid', 73),\n",
       " ('¿', 73),\n",
       " ('sig2', 72),\n",
       " ('polychoric', 72),\n",
       " ('unstandardized', 72),\n",
       " ('npdot', 72),\n",
       " ('rgamma', 72),\n",
       " ('mcar', 72),\n",
       " ('arxivorg', 72),\n",
       " ('glmi', 72),\n",
       " ('ascharacter', 72),\n",
       " ('score1', 71),\n",
       " ('underbrace', 71),\n",
       " ('rmamv', 71),\n",
       " ('lambda2', 71),\n",
       " ('includemean', 71),\n",
       " ('preprocess', 71),\n",
       " ('npmean', 71),\n",
       " ('featuresi', 71),\n",
       " ('class2', 70),\n",
       " ('holtwinters', 70),\n",
       " ('neuralnet', 70),\n",
       " ('genmod', 70),\n",
       " ('sigma1', 70),\n",
       " ('nahead', 70),\n",
       " ('heatmap', 70),\n",
       " ('pointsi', 70),\n",
       " ('site2', 70),\n",
       " ('dataset1', 70),\n",
       " ('idata', 69),\n",
       " ('repmat', 69),\n",
       " ('githubcom', 69),\n",
       " ('pltshow', 69),\n",
       " ('variableslet', 69),\n",
       " ('listwise', 69),\n",
       " ('periodogram', 68),\n",
       " ('groupa', 68),\n",
       " ('unif', 68),\n",
       " ('lvert', 68),\n",
       " ('glmms', 68),\n",
       " ('hisei', 67),\n",
       " ('cforest', 67),\n",
       " ('pvals', 67),\n",
       " ('detrending', 67),\n",
       " ('mdash', 67),\n",
       " ('bayesians', 67),\n",
       " ('homoscedastic', 67),\n",
       " ('cat1', 67),\n",
       " ('fittedvalues', 67),\n",
       " ('lambdas', 67),\n",
       " ('discretize', 67),\n",
       " ('bigl', 67),\n",
       " ('sizesi', 67),\n",
       " ('correlogram', 67),\n",
       " ('setsi', 67),\n",
       " ('yeardecimal', 66),\n",
       " ('installpackages', 66),\n",
       " ('probabilitiesi', 66),\n",
       " ('rmvnorm', 66),\n",
       " ('methodsi', 66),\n",
       " ('factorsi', 66),\n",
       " ('mathstackexchangecom', 66),\n",
       " ('rocr', 66),\n",
       " ('num2str', 66),\n",
       " ('datetime', 66),\n",
       " ('maxiter', 66),\n",
       " ('tfrac', 65),\n",
       " ('type2', 65),\n",
       " ('paste0', 65),\n",
       " ('ã', 65),\n",
       " ('intervali', 65),\n",
       " ('qsec', 65),\n",
       " ('\\x94\\x80', 64),\n",
       " ('¾', 64),\n",
       " ('zinb', 64),\n",
       " ('varident', 64),\n",
       " ('nparange', 64),\n",
       " ('treatment2', 64),\n",
       " ('docall', 64),\n",
       " ('mvn', 64),\n",
       " ('mlogloss', 63),\n",
       " ('obdobinehn', 63),\n",
       " ('loc1', 63),\n",
       " ('score2', 63),\n",
       " ('stepaic', 63),\n",
       " ('populationi', 63),\n",
       " ('gpml', 63),\n",
       " ('ntrees', 63),\n",
       " ('rmsea', 63),\n",
       " ('conflevel', 63),\n",
       " ('level2', 63),\n",
       " ('crossprod', 63),\n",
       " ('meansi', 63),\n",
       " ('invlogit', 63),\n",
       " ('resdf', 63),\n",
       " ('virustype', 62),\n",
       " ('binomtest', 62),\n",
       " ('identifiability', 62),\n",
       " ('factor3', 62),\n",
       " ('plyr', 62),\n",
       " ('betareg', 62),\n",
       " ('dxy', 62),\n",
       " ('dbinom', 62),\n",
       " ('dbn', 62),\n",
       " ('distributionsuppose', 62),\n",
       " ('loc2', 61),\n",
       " ('gridsearchcv', 61),\n",
       " ('sexm', 61),\n",
       " ('drop1', 61),\n",
       " ('mancova', 61),\n",
       " ('dv2', 61),\n",
       " ('nsubj', 61),\n",
       " ('distributionlet', 61),\n",
       " ('xtmixed', 61),\n",
       " ('qqline', 61),\n",
       " ('5pct', 61),\n",
       " ('permute', 61),\n",
       " ('clffit', 61),\n",
       " ('vectorid', 60),\n",
       " ('supportvector', 60),\n",
       " ('predictor1', 60),\n",
       " ('sexmale', 60),\n",
       " ('logp', 60),\n",
       " ('datasetsi', 60),\n",
       " ('2j', 60),\n",
       " ('xmat', 60),\n",
       " ('lambda1', 60),\n",
       " ('numdf', 60),\n",
       " ('pc3', 60),\n",
       " ('homoskedasticity', 60),\n",
       " ('biserial', 60),\n",
       " ('cat2', 60),\n",
       " ('npsum', 60),\n",
       " ('denom', 60),\n",
       " ('chr1', 59),\n",
       " ('confounder', 59),\n",
       " ('exper', 59),\n",
       " ('ddots', 59),\n",
       " ('widetilde', 59),\n",
       " ('ymin', 59),\n",
       " ('predictor2', 59),\n",
       " ('subjectid', 59),\n",
       " ('colsums', 59),\n",
       " ('agegroup', 59),\n",
       " ('kruschke', 59),\n",
       " ('repeatedcv', 59),\n",
       " ('npexp', 59),\n",
       " ('networksi', 59),\n",
       " ('datacsv', 59),\n",
       " ('arcsin', 59),\n",
       " ('liblinear', 59),\n",
       " ('lowess', 59),\n",
       " ('nonumber', 58),\n",
       " ('samplingi', 58),\n",
       " ('undersampling', 58),\n",
       " ('ddply', 58),\n",
       " ('arcsine', 58),\n",
       " ('set1', 58),\n",
       " ('forecastingi', 58),\n",
       " ('effecti', 58),\n",
       " ('tetab', 57),\n",
       " ('comp2', 57),\n",
       " ('unbiasedness', 57),\n",
       " ('groupi', 57),\n",
       " ('treatment1', 57),\n",
       " ('frac1', 57),\n",
       " ('deviationi', 57),\n",
       " ('svmi', 57),\n",
       " ('solvedefault', 57),\n",
       " ('actuals', 57),\n",
       " ('set2', 57),\n",
       " ('proportionsi', 57),\n",
       " ('clmm', 57),\n",
       " ('col1', 57),\n",
       " ('iv3', 57),\n",
       " ('ptri', 56),\n",
       " ('subjid', 56),\n",
       " ('coeftest', 56),\n",
       " ('descriptives', 56),\n",
       " ('rapidminer', 56),\n",
       " ('exponentiated', 56),\n",
       " ('lmermod', 56),\n",
       " ('rbeta', 56),\n",
       " ('npsqrt', 56),\n",
       " ('lagmax', 56),\n",
       " ('rbms', 56),\n",
       " ('ardl', 56),\n",
       " ('loc3', 55),\n",
       " ('drug1', 55),\n",
       " ('xmax', 55),\n",
       " ('mydat', 55),\n",
       " ('notin', 55),\n",
       " ('detrend', 55),\n",
       " ('mu0', 55),\n",
       " ('zuur', 55),\n",
       " ('lrtest', 55),\n",
       " ('mean1', 55),\n",
       " ('rsq', 55),\n",
       " ('1pct', 55),\n",
       " ('2z', 55),\n",
       " ('xtrain', 55),\n",
       " ('variablessuppose', 55),\n",
       " ('kpca', 54),\n",
       " ('res2', 54),\n",
       " ('mfx', 54),\n",
       " ('site1', 54),\n",
       " ('vgam', 54),\n",
       " ('cifar', 54),\n",
       " ('zscore', 54),\n",
       " ('eventsi', 54),\n",
       " ('tfvariable', 54),\n",
       " ('\\x86', 54),\n",
       " ('lsmean', 54),\n",
       " ('outliersi', 54),\n",
       " ('acf1', 54),\n",
       " ('pastebincom', 54),\n",
       " ('matplotlib', 54),\n",
       " ('glmmpql', 54),\n",
       " ('igraph', 54),\n",
       " ('ihid', 53),\n",
       " ('mcmcchain', 53),\n",
       " ('lgamma', 53),\n",
       " ('xx2', 53),\n",
       " ('fiti', 53),\n",
       " ('thetas', 53),\n",
       " ('vect', 53),\n",
       " ('armax', 53),\n",
       " ('asinteger', 53),\n",
       " ('ma2', 53),\n",
       " ('classifieri', 53),\n",
       " ('rvert', 53),\n",
       " ('dtfail', 53),\n",
       " ('interpretability', 53),\n",
       " ('cond1', 52),\n",
       " ('out1', 52),\n",
       " ('fitcontrol', 52),\n",
       " ('arimafit', 52),\n",
       " ('limma', 52),\n",
       " ('distributionthe', 52),\n",
       " ('confusionmatrix', 52),\n",
       " ('\\x80\\x8b', 52),\n",
       " ('fitdistrplus', 52),\n",
       " ('logl', 52),\n",
       " ('vmatrix', 52),\n",
       " ('mean2', 52),\n",
       " ('xmin', 52),\n",
       " ('dandok', 51),\n",
       " ('datacol', 51),\n",
       " ('leftarrow', 51),\n",
       " ('randomisation', 51),\n",
       " ('sigmas', 51),\n",
       " ('pc4', 51),\n",
       " ('dendf', 51),\n",
       " ('parms', 51),\n",
       " ('plim', 51),\n",
       " ('dfresidual', 51),\n",
       " ('pythoni', 51),\n",
       " ('quantreg', 51),\n",
       " ('likelihoodi', 51),\n",
       " ('bigr', 51),\n",
       " ('exogeneity', 51),\n",
       " ('fland', 50),\n",
       " ('ð', 50),\n",
       " ('season1', 50),\n",
       " ('pmnormal', 50),\n",
       " ('pscl', 50),\n",
       " ('frequentists', 50),\n",
       " ('causalimpact', 50),\n",
       " ('\\x80\\x99ll', 50),\n",
       " ('time3', 50),\n",
       " ('auroc', 50),\n",
       " ('rawdata', 50),\n",
       " ('\\x80\\x99re', 50),\n",
       " ('asym', 50),\n",
       " ('backprop', 50),\n",
       " ('significanti', 50),\n",
       " ('clust', 50),\n",
       " ('var5', 50),\n",
       " ('ytest', 50),\n",
       " ('residualsi', 50),\n",
       " ('coefficienti', 50),\n",
       " ('col2', 50),\n",
       " ('dnn', 50),\n",
       " ('lme1', 49),\n",
       " ('product2', 49),\n",
       " ('catpca', 49),\n",
       " ('graphpad', 49),\n",
       " ('barplot', 49),\n",
       " ('simdata', 49),\n",
       " ('openbugs', 49),\n",
       " ('binomially', 49),\n",
       " ('times1', 49),\n",
       " ('distr', 49),\n",
       " ('vifs', 49),\n",
       " ('qplot', 49),\n",
       " ('stopwords', 49),\n",
       " ('parameteri', 49),\n",
       " ('optctrl', 49),\n",
       " ('modelin', 49),\n",
       " ('\\x9c\\x93', 48),\n",
       " ('pymc3', 48),\n",
       " ('tau2', 48),\n",
       " ('afaik', 48),\n",
       " ('betahat', 48),\n",
       " ('estimatori', 48),\n",
       " ('pmax', 48),\n",
       " ('randomforestclassifier', 48),\n",
       " ('myfit', 48),\n",
       " ('elasticnet', 48),\n",
       " ('beta3', 48),\n",
       " ('datamatrix', 48),\n",
       " ('levelsi', 48),\n",
       " ('noninformative', 48),\n",
       " ('rstudio', 48),\n",
       " ('xdata', 48),\n",
       " ('reshape2', 48),\n",
       " ('variable1', 48),\n",
       " ('nplinspace', 48),\n",
       " ('wwwncbinlmnihgov', 48),\n",
       " ('remldev', 48),\n",
       " ('regularizer', 48),\n",
       " ('iout', 47),\n",
       " ('modela', 47),\n",
       " ('nagelkerke', 47),\n",
       " ('logloss', 47),\n",
       " ('multilabel', 47),\n",
       " ('meanlog', 47),\n",
       " ('datain', 47),\n",
       " ('regressionsuppose', 47),\n",
       " ('mapsto', 47),\n",
       " ('nltk', 47),\n",
       " ('fullmodel', 47),\n",
       " ('0b', 47),\n",
       " ('pcai', 47),\n",
       " ('cajo', 47),\n",
       " ('experimenti', 47),\n",
       " ('½', 47),\n",
       " ('yeardummy', 46),\n",
       " ('cl1', 46),\n",
       " ('tau1', 46),\n",
       " ('whichmin', 46),\n",
       " ('thedata', 46),\n",
       " ('medoid', 46),\n",
       " ('emmeans', 46),\n",
       " ('factanal', 46),\n",
       " ('bayesglm', 46),\n",
       " ('concatenate', 46),\n",
       " ('plogis', 46),\n",
       " ('bigram', 46),\n",
       " ('week1', 46),\n",
       " ('resids', 46),\n",
       " ('nprandomnormal', 46),\n",
       " ('mylogit', 46),\n",
       " ('classprobs', 46),\n",
       " ('sqrt2', 46),\n",
       " ('algorithmsi', 46),\n",
       " ('wtcd', 45),\n",
       " ('nsims', 45),\n",
       " ('cutpoints', 45),\n",
       " ('usutf', 45),\n",
       " ('task1', 45),\n",
       " ('covtype', 45),\n",
       " ('fwer', 45),\n",
       " ('expr', 45),\n",
       " ('regsubsets', 45),\n",
       " ('pseudoreplication', 45),\n",
       " ('sdlog', 45),\n",
       " ('numel', 45),\n",
       " ('rfloor', 45),\n",
       " ('logisticregression', 45),\n",
       " ('ksvm', 45),\n",
       " ('covariatesi', 45),\n",
       " ('scatterplots', 45),\n",
       " ('studyi', 45),\n",
       " ('xshape', 45),\n",
       " ('vectorized', 45),\n",
       " ('nstart', 45),\n",
       " ('mod3', 45),\n",
       " ('maxfun', 45),\n",
       " ('accuracyi', 45),\n",
       " ('dataset2', 45),\n",
       " ('rawpeak', 44),\n",
       " ('feature1', 44),\n",
       " ('tmpdata', 44),\n",
       " ('comp3', 44),\n",
       " ('rarrb', 44),\n",
       " ('calculationi', 44),\n",
       " ('lag2', 44),\n",
       " ('subsetting', 44),\n",
       " ('fitlm', 44),\n",
       " ('polytomous', 44),\n",
       " ('emptyset', 44),\n",
       " ('geeglm', 44),\n",
       " ('overfitted', 44),\n",
       " ('vcv', 44),\n",
       " ('3x2', 44),\n",
       " ('bigcap', 44),\n",
       " ('trainingdata', 44),\n",
       " ('cnns', 44),\n",
       " ('xrange', 44),\n",
       " ('coeffs', 44),\n",
       " ('nagq', 44),\n",
       " ('statistici', 44),\n",
       " ('rlnorm', 44),\n",
       " ('pop0', 43),\n",
       " ('normalmix', 43),\n",
       " ('logn', 43),\n",
       " ('textsf', 43),\n",
       " ('matchit', 43),\n",
       " ('egarch', 43),\n",
       " ('cov1', 43),\n",
       " ('jth', 43),\n",
       " ('euclidian', 43),\n",
       " ('ld1', 43),\n",
       " ('pbinom', 43),\n",
       " ('\\x86\\x92', 43),\n",
       " ('kpsstest', 43),\n",
       " ('y5', 43),\n",
       " ...]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ov_glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing by Andrew Lukyanenko\n",
    "\n",
    "The cell is code is taken from a notebook by Andrew Lukyanenko "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 36.39% of vocab\n",
      "Found embeddings for  96.45% of all text\n"
     ]
    }
   ],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    \n",
    "#     x = re.sub('[0-9]{5,}', '#####', x)\n",
    "#     x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = unprocessed_text.apply(lambda x: clean_text(x.lower()))\n",
    "\n",
    "# Clean numbers\n",
    "clean_text_number = cleaned_text.apply(lambda x: clean_numbers(x))\n",
    "\n",
    "# Clean spellings\n",
    "clean_text_number_misspelling = clean_text_number.apply(lambda x: replace_typical_misspell(x))\n",
    "\n",
    "ov_glove = get_unknown_words(clean_text_number_misspelling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text-embedding is lowered by 1% but the vocab has increaes by 4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of out of vocab words 69607\n",
      "entire  vocab words 109825\n"
     ]
    }
   ],
   "source": [
    "print(f'number of out of vocab words {len(ov_glove)}')\n",
    "full_voacb = build_vocab(clean_text_number_misspelling)\n",
    "print(f'entire  vocab words {len(full_voacb)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My old method without stemming or removing stopwords \n",
    "## This method includes more punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaa   run running ban banned dancing dance                                  plokiu    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    \n",
    "    # remove html tags\n",
    "    regex = re.compile('<.*?>')\n",
    "    input =  re.sub(regex, '', x)\n",
    "\n",
    "    #remove punctuations, numbers.\n",
    "    input = re.sub('[!@#$%^&*()\\n_:><?\\-.{}|+-,;\"\"``~`—]|[0-9]|/|=|\\[\\]|\\[\\[\\]\\|\\\\|//]',' ',input)\n",
    "    input = re.sub('[“’\\']','',input)   \n",
    "    input = input.replace('\\\\','')\n",
    "        \n",
    "\n",
    "\n",
    "    return input.lower()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tmp_sent  = \"AAAAAA <html> <h1> run <i>running</i> ban banned dancing dance 1 2 3  4   5 5  5 !@#$%^&*(){{:><<< MMM<>?PLOKIU}} </h1> </html>\"\n",
    "\n",
    "\n",
    "print(clean_text(tmp_sent))\n",
    "\n",
    "\n",
    "processed_text_1 = unprocessed_text.map(clean_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 34.95% of vocab\n",
      "Found embeddings for  97.06% of all text\n"
     ]
    }
   ],
   "source": [
    "ov_glove = get_unknown_words(processed_text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mathbf', 7039),\n",
       " ('sqrt', 4979),\n",
       " ('mathbb', 3273),\n",
       " ('nbsp', 2627),\n",
       " ('rnorm', 2622),\n",
       " ('infty', 2515),\n",
       " ('mathcal', 2411),\n",
       " ('lmer', 2230),\n",
       " ('ldots', 2070),\n",
       " ('boldsymbol', 2057),\n",
       " ('mathrm', 2054),\n",
       " ('leq', 1793),\n",
       " ('coef', 1643),\n",
       " ('covariate', 1420),\n",
       " ('right]', 1277),\n",
       " ('mydata', 1254),\n",
       " ('datai', 1230),\n",
       " ('cbind', 1201),\n",
       " ('signif', 1191),\n",
       " ('varepsilon', 1163),\n",
       " ('nrow', 1145),\n",
       " ('bmatrix', 1120),\n",
       " ('glmnet', 963),\n",
       " ('aov', 961),\n",
       " ('glmer', 959),\n",
       " ('regressioni', 939),\n",
       " ('rmse', 926),\n",
       " ('â±', 890),\n",
       " (']]', 884),\n",
       " ('rightarrow', 857),\n",
       " ('runif', 857),\n",
       " ('garch', 833),\n",
       " ('reml', 813),\n",
       " ('cdots', 812),\n",
       " ('ncol', 801),\n",
       " ('dnorm', 791),\n",
       " ('modeli', 788),\n",
       " ('chisq', 767),\n",
       " ('randomforest', 754),\n",
       " ('x[', 745),\n",
       " ('shouldnt', 739),\n",
       " ('ggplot', 727),\n",
       " ('loglik', 724),\n",
       " ('dfrac', 721),\n",
       " ('surv', 717),\n",
       " ('textbf', 709),\n",
       " ('î²', 697),\n",
       " ('pmatrix', 692),\n",
       " ('multicollinearity', 650),\n",
       " ('newdata', 648),\n",
       " ('i]', 644),\n",
       " ('variablesi', 585),\n",
       " ('resid', 583),\n",
       " ('nlme', 580),\n",
       " ('nanâ±nan', 578),\n",
       " ('glmm', 575),\n",
       " ('â', 566),\n",
       " ('sklearn', 566),\n",
       " ('scikit', 559),\n",
       " ('ancova', 557),\n",
       " ('sumlimits', 555),\n",
       " ('propto', 528),\n",
       " ('left[', 518),\n",
       " ('distributioni', 517),\n",
       " ('â\\x95\\x91', 512),\n",
       " ('neq', 510),\n",
       " ('operatorname', 510),\n",
       " ('manova', 507),\n",
       " ('coxph', 503),\n",
       " ('knn', 501),\n",
       " ('iâ\\x80\\x99m', 487),\n",
       " ('xreg', 483),\n",
       " ('ylab', 477),\n",
       " ('scipy', 472),\n",
       " ('pacf', 470),\n",
       " ('[x', 469),\n",
       " ('mbox', 461),\n",
       " ('[i]', 451),\n",
       " ('xlab', 450),\n",
       " ('rpart', 448),\n",
       " ('xbeta', 444),\n",
       " ('regressor', 442),\n",
       " ('cointegration', 442),\n",
       " ('binom', 441),\n",
       " ('pymc', 436),\n",
       " ('widehat', 435),\n",
       " ('vdots', 425),\n",
       " ('[[', 420),\n",
       " ('data[', 415),\n",
       " ('dataframe', 413),\n",
       " ('ifelse', 408),\n",
       " ('abline', 402),\n",
       " ('kmeans', 402),\n",
       " ('forall', 402),\n",
       " ('e[', 392),\n",
       " ('eqnarray', 386),\n",
       " ('textrm', 386),\n",
       " ('colnames', 383),\n",
       " ('collinearity', 383),\n",
       " ('variablei', 378),\n",
       " ('analysisi', 375),\n",
       " ('ylim', 372),\n",
       " ('rht', 370),\n",
       " ('xright', 363),\n",
       " ('optim', 361),\n",
       " ('lty', 360),\n",
       " ('stackexchange', 357),\n",
       " ('x]', 355),\n",
       " ('hline', 354),\n",
       " ('params', 354),\n",
       " ('rbind', 352),\n",
       " ('e[x', 345),\n",
       " ('mtcars', 342),\n",
       " ('spearmans', 341),\n",
       " ('n]', 340),\n",
       " ('timeseries', 340),\n",
       " ('heteroscedasticity', 339),\n",
       " ('ï\\x83', 337),\n",
       " ('heteroskedasticity', 337),\n",
       " ('î¸', 335),\n",
       " ('lsmeans', 332),\n",
       " ('t]', 329),\n",
       " ('seriesi', 326),\n",
       " ('mlogit', 325),\n",
       " ('â\\x88\\x92', 324),\n",
       " ('datapoints', 324),\n",
       " ('svms', 323),\n",
       " ('hatbeta', 321),\n",
       " ('lstm', 315),\n",
       " ('î±', 315),\n",
       " ('j]', 313),\n",
       " ('[a', 308),\n",
       " ('stackoverflow', 302),\n",
       " ('pleft', 301),\n",
       " ('softmax', 297),\n",
       " ('e[y', 293),\n",
       " ('donâ\\x80\\x99t', 291),\n",
       " ('rbinom', 291),\n",
       " ('hyperparameters', 290),\n",
       " ('scatterplot', 285),\n",
       " ('modelsi', 285),\n",
       " ('y[', 284),\n",
       " ('anovas', 283),\n",
       " ('î¼', 283),\n",
       " ('expleft', 282),\n",
       " ('boxplot', 274),\n",
       " ('attr', 272),\n",
       " ('libsvm', 269),\n",
       " ('pvalue', 263),\n",
       " ('timepoint', 251),\n",
       " ('autocorrelated', 248),\n",
       " ('qquad', 246),\n",
       " ('sapply', 242),\n",
       " ('mathscr', 242),\n",
       " ('[y', 241),\n",
       " ('b[', 240),\n",
       " ('adaboost', 239),\n",
       " ('xlim', 238),\n",
       " ('overdispersion', 238),\n",
       " ('coeff', 237),\n",
       " ('glms', 235),\n",
       " ('subj', 233),\n",
       " ('nnet', 232),\n",
       " ('lavaan', 226),\n",
       " ('disp', 226),\n",
       " ('probs', 225),\n",
       " ('b]', 223),\n",
       " ('stddev', 222),\n",
       " ('intr', 221),\n",
       " ('langle', 219),\n",
       " ('kpss', 218),\n",
       " ('valuesi', 217),\n",
       " ('vecm', 216),\n",
       " ('resample', 216),\n",
       " ('k]', 215),\n",
       " ('dataim', 211),\n",
       " ('pnorm', 209),\n",
       " ('p[', 207),\n",
       " ('hattheta', 206),\n",
       " ('y]', 206),\n",
       " ('stdev', 206),\n",
       " ('hspace', 204),\n",
       " ('setosa', 204),\n",
       " ('varphi', 203),\n",
       " ('[i', 202),\n",
       " ('glht', 200),\n",
       " ('confint', 200),\n",
       " ('contr', 198),\n",
       " ('rangle', 196),\n",
       " ('mgcv', 195),\n",
       " ('ntimes', 194),\n",
       " ('underset', 192),\n",
       " ('winbugs', 192),\n",
       " ('dtheta', 191),\n",
       " ('e[x]', 191),\n",
       " ('ecdf', 191),\n",
       " ('y[i]', 191),\n",
       " ('dunif', 189),\n",
       " ('dgamma', 186),\n",
       " ('mtry', 186),\n",
       " ('minitab', 185),\n",
       " ('homoscedasticity', 184),\n",
       " ('x[i]', 183),\n",
       " ('bfgs', 182),\n",
       " ('statsmodels', 182),\n",
       " ('vcov', 182),\n",
       " ('autoencoder', 181),\n",
       " ('mfrow', 180),\n",
       " ('missingness', 180),\n",
       " ('xsim', 178),\n",
       " ('theta[', 174),\n",
       " ('distributionsi', 174),\n",
       " ('iâ\\x80\\x99ve', 173),\n",
       " ('relu', 173),\n",
       " ('regressionim', 173),\n",
       " ('xgboost', 172),\n",
       " ('endogeneity', 172),\n",
       " ('quasipoisson', 171),\n",
       " ('tbats', 171),\n",
       " ('mnist', 169),\n",
       " ('lapply', 169),\n",
       " ('varimax', 168),\n",
       " ('overfit', 168),\n",
       " ('iqr', 166),\n",
       " ('loglikelihood', 165),\n",
       " ('a[', 165),\n",
       " ('eviews', 165),\n",
       " ('nabla', 164),\n",
       " ('arimax', 164),\n",
       " ('yhat', 163),\n",
       " ('coefficients[', 161),\n",
       " ('xvar', 160),\n",
       " ('ezanova', 160),\n",
       " ('rownames', 160),\n",
       " ('fleft', 159),\n",
       " ('randn', 159),\n",
       " ('functioni', 158),\n",
       " ('yright', 157),\n",
       " ('mu[i]', 155),\n",
       " ('cronbachs', 155),\n",
       " ('itâ\\x80\\x99s', 154),\n",
       " ('dput', 154),\n",
       " ('metafor', 154),\n",
       " ('displaystyle', 153),\n",
       " ('maxit', 152),\n",
       " ('dendrogram', 151),\n",
       " ('variogram', 150),\n",
       " ('ntree', 150),\n",
       " ('checkconv', 149),\n",
       " ('iright', 149),\n",
       " ('timepoints', 149),\n",
       " ('df[', 148),\n",
       " ('phileft', 148),\n",
       " ('levenes', 147),\n",
       " ('nleft', 147),\n",
       " ('î»', 146),\n",
       " ('breusch', 146),\n",
       " ('interval]', 146),\n",
       " ('trcontrol', 146),\n",
       " ('overset', 146),\n",
       " ('mcnemars', 145),\n",
       " ('testdata', 144),\n",
       " ('left[frac', 144),\n",
       " ('cointegrated', 143),\n",
       " ('modelim', 142),\n",
       " ('argmin', 141),\n",
       " ('nsim', 141),\n",
       " ('cumsum', 141),\n",
       " ('tukeys', 140),\n",
       " ('sarima', 140),\n",
       " ('equiv', 140),\n",
       " ('][', 139),\n",
       " ('heteroscedastic', 139),\n",
       " ('rescale', 138),\n",
       " ('matplotlib', 138),\n",
       " ('qnorm', 136),\n",
       " ('rpois', 136),\n",
       " ('hclust', 136),\n",
       " ('coef[', 135),\n",
       " ('preds', 135),\n",
       " ('dimnames', 135),\n",
       " ('tensorflow', 134),\n",
       " ('traincontrol', 133),\n",
       " ('loocv', 133),\n",
       " ('bewert', 132),\n",
       " ('crossvalidated', 132),\n",
       " ('fitdistr', 132),\n",
       " ('hyperparameter', 131),\n",
       " ('mcmcglmm', 129),\n",
       " ('prcomp', 128),\n",
       " ('â£', 128),\n",
       " ('anovai', 128),\n",
       " ('x[i', 128),\n",
       " ('subsamples', 128),\n",
       " ('eleft', 127),\n",
       " ('pval', 126),\n",
       " ('râ²', 126),\n",
       " ('pchisq', 126),\n",
       " ('qqplot', 125),\n",
       " ('[p', 125),\n",
       " ('classificationi', 125),\n",
       " ('w[', 124),\n",
       " ('spssi', 124),\n",
       " ('covar', 124),\n",
       " ('whuber', 124),\n",
       " ('quasibinomial', 123),\n",
       " ('[x]', 123),\n",
       " ('resultsi', 123),\n",
       " ('autoregression', 123),\n",
       " ('â\\x9c\\x97', 122),\n",
       " ('boxcox', 120),\n",
       " ('problemi', 120),\n",
       " ('u[', 120),\n",
       " ('argmax', 120),\n",
       " ('groupsi', 120),\n",
       " ('par[', 119),\n",
       " ('multinom', 119),\n",
       " ('îµ', 118),\n",
       " ('survreg', 118),\n",
       " ('valuei', 118),\n",
       " ('fixef', 116),\n",
       " ('glmmadmb', 115),\n",
       " ('byrow', 115),\n",
       " ('unlist', 115),\n",
       " ('inits', 115),\n",
       " ('[t', 114),\n",
       " ('relevel', 114),\n",
       " ('matrixi', 114),\n",
       " ('qqnorm', 113),\n",
       " ('lemeshow', 113),\n",
       " ('boxplots', 112),\n",
       " ('âµ', 111),\n",
       " ('sparsearray', 110),\n",
       " ('vectorinstance', 110),\n",
       " ('gamlss', 110),\n",
       " ('traindata', 110),\n",
       " ('ã\\x97', 110),\n",
       " ('autocorrelations', 110),\n",
       " ('dpois', 110),\n",
       " ('samplesi', 110),\n",
       " ('â\\x80\\x8e', 109),\n",
       " ('mclust', 108),\n",
       " ('kaggle', 108),\n",
       " ('ctree', 107),\n",
       " ('dbscan', 107),\n",
       " ('newdat', 106),\n",
       " ('hatsigma', 106),\n",
       " ('mplus', 106),\n",
       " ('matlabs', 106),\n",
       " ('timei', 106),\n",
       " ('medoids', 105),\n",
       " ('polr', 105),\n",
       " ('biplot', 105),\n",
       " ('bigg]', 104),\n",
       " ('ttest', 104),\n",
       " ('howi', 103),\n",
       " ('hdbr', 103),\n",
       " ('cdfs', 103),\n",
       " ('negbin', 103),\n",
       " ('newcommand', 103),\n",
       " ('shape[', 103),\n",
       " ('overdispersed', 102),\n",
       " ('ysim', 102),\n",
       " ('dataseti', 102),\n",
       " ('[f', 102),\n",
       " ('mydf', 101),\n",
       " ('tukeyhsd', 101),\n",
       " ('xmid', 101),\n",
       " ('intraclass', 101),\n",
       " ('pvals', 101),\n",
       " ('estimationi', 101),\n",
       " ('mvrnorm', 101),\n",
       " ('varcorr', 101),\n",
       " ('studentized', 100),\n",
       " ('tfidf', 100),\n",
       " ('fland', 99),\n",
       " ('indepvar', 99),\n",
       " (']][', 99),\n",
       " ('umvue', 99),\n",
       " ('linfct', 99),\n",
       " ('lmertest', 99),\n",
       " ('survfit', 99),\n",
       " ('istate', 98),\n",
       " ('sleepstudy', 98),\n",
       " ('confounders', 98),\n",
       " ('clusteringi', 98),\n",
       " ('rjags', 97),\n",
       " ('nonstationary', 97),\n",
       " ('letâ\\x80\\x99s', 96),\n",
       " ('dtype', 96),\n",
       " ('beta[', 95),\n",
       " ('cointegrating', 94),\n",
       " ('bobyqa', 94),\n",
       " ('distributionim', 94),\n",
       " ('left[begin', 93),\n",
       " ('ranef', 93),\n",
       " ('mcnemar', 93),\n",
       " ('xtreg', 93),\n",
       " ('rexp', 92),\n",
       " ('variancei', 92),\n",
       " ('class[', 91),\n",
       " ('welchs', 91),\n",
       " ('autocovariance', 91),\n",
       " ('stackrel', 91),\n",
       " ('e[y]', 91),\n",
       " ('pyplot', 91),\n",
       " ('fitdist', 91),\n",
       " ('a]', 91),\n",
       " ('c]', 90),\n",
       " ('tibshirani', 90),\n",
       " ('yvar', 90),\n",
       " ('lmtest', 90),\n",
       " ('doesnâ\\x80\\x99t', 90),\n",
       " ('ntoinfty', 90),\n",
       " ('sizei', 90),\n",
       " ('pisigma', 89),\n",
       " ('recode', 89),\n",
       " ('questioni', 89),\n",
       " ('variablesim', 89),\n",
       " ('logrank', 89),\n",
       " ('letters[', 89),\n",
       " ('multcomp', 89),\n",
       " ('[n', 89),\n",
       " ('mymodel', 88),\n",
       " ('colmeans', 88),\n",
       " ('packagei', 88),\n",
       " ('[e', 88),\n",
       " ('coefficientsi', 88),\n",
       " ('xbar', 88),\n",
       " ('princomp', 88),\n",
       " ('statai', 87),\n",
       " ('xrightarrow', 87),\n",
       " ('m]', 87),\n",
       " ('zeroinfl', 86),\n",
       " ('validationi', 86),\n",
       " ('p[x', 86),\n",
       " ('glimmix', 85),\n",
       " ('youve', 85),\n",
       " ('[c', 85),\n",
       " ('processi', 85),\n",
       " ('crossvalidation', 85),\n",
       " ('posttest', 84),\n",
       " ('foreach', 84),\n",
       " ('minibatch', 84),\n",
       " ('correlationi', 84),\n",
       " ('corar', 84),\n",
       " ('dplyr', 84),\n",
       " ('mcar', 84),\n",
       " ('changepoint', 84),\n",
       " ('heteroskedastic', 83),\n",
       " ('d[', 83),\n",
       " ('longrightarrow', 83),\n",
       " ('xgb', 83),\n",
       " ('benjamini', 83),\n",
       " ('derivs', 83),\n",
       " ('vglm', 83),\n",
       " ('glmercontrol', 83),\n",
       " ('glmms', 83),\n",
       " ('groupb', 82),\n",
       " ('matlabi', 82),\n",
       " ('gammaleft', 81),\n",
       " ('nsum', 81),\n",
       " ('heatmap', 81),\n",
       " ('tmathbf', 80),\n",
       " ('[j]', 80),\n",
       " ('chisquare', 80),\n",
       " ('linspace', 80),\n",
       " ('ibeta', 80),\n",
       " ('varimp', 80),\n",
       " ('clogit', 80),\n",
       " ('logleft', 80),\n",
       " ('tâ\\x88\\x92', 79),\n",
       " ('errori', 79),\n",
       " ('naivebayes', 79),\n",
       " ('meani', 79),\n",
       " ('tseries', 79),\n",
       " ('hleft', 78),\n",
       " ('zinb', 78),\n",
       " ('holtwinters', 78),\n",
       " ('dat[', 78),\n",
       " ('resamples', 78),\n",
       " ('canâ\\x80\\x99t', 78),\n",
       " ('learningi', 78),\n",
       " ('interpretationi', 78),\n",
       " ('regressionin', 78),\n",
       " ('intlimits', 77),\n",
       " ('logits', 77),\n",
       " ('â\\x89¥', 77),\n",
       " ('â\\x80¢', 77),\n",
       " ('testsi', 77),\n",
       " ('í', 76),\n",
       " ('expleft[', 76),\n",
       " ('e[e', 76),\n",
       " ('estimate[', 76),\n",
       " ('nright', 76),\n",
       " ('displaystylesum', 76),\n",
       " ('preprocess', 76),\n",
       " ('rocr', 76),\n",
       " ('algorithmi', 76),\n",
       " ('dotsc', 75),\n",
       " ('autoencoders', 75),\n",
       " ('pvalues', 75),\n",
       " ('rugarch', 75),\n",
       " ('rowsums', 75),\n",
       " ('designi', 75),\n",
       " ('effectsi', 75),\n",
       " ('datetime', 75),\n",
       " ('coefs', 75),\n",
       " ('kernlab', 74),\n",
       " ('vect', 74),\n",
       " ('big]', 74),\n",
       " ('resampled', 74),\n",
       " ('hmisc', 74),\n",
       " ('rmsea', 74),\n",
       " ('statas', 74),\n",
       " ('[b', 74),\n",
       " ('predictioni', 74),\n",
       " ('networki', 74),\n",
       " ('mat[', 73),\n",
       " ('s]', 73),\n",
       " ('importances', 73),\n",
       " ('iris[', 73),\n",
       " ('rgamma', 73),\n",
       " ('tsigma', 73),\n",
       " ('untransformed', 73),\n",
       " ('tapply', 73),\n",
       " ('samplei', 73),\n",
       " ('werent', 73),\n",
       " ('finalmodel', 73),\n",
       " ('methodi', 73),\n",
       " ('samplesize', 72),\n",
       " ('genmod', 72),\n",
       " ('polychoric', 72),\n",
       " ('leftrightarrow', 72),\n",
       " ('unstandardized', 72),\n",
       " ('arange', 72),\n",
       " ('imgur', 72),\n",
       " ('probabilityi', 72),\n",
       " ('tfrac', 71),\n",
       " ('laymans', 71),\n",
       " ('whubers', 71),\n",
       " ('bayesians', 71),\n",
       " ('rsquare', 71),\n",
       " ('d]', 71),\n",
       " ('idata', 70),\n",
       " ('mathsf', 70),\n",
       " ('selectioni', 70),\n",
       " ('neuralnet', 70),\n",
       " ('p[y', 70),\n",
       " ('utf', 70),\n",
       " ('analysisim', 70),\n",
       " ('p]', 70),\n",
       " ('[glmermod]', 70),\n",
       " ('groupa', 70),\n",
       " ('ineq', 70),\n",
       " ('lambdas', 70),\n",
       " ('tunegrid', 70),\n",
       " ('â°c', 69),\n",
       " ('underbrace', 69),\n",
       " ('dbeta', 69),\n",
       " ('parametersi', 69),\n",
       " ('ï\\x80', 69),\n",
       " ('listwise', 69),\n",
       " ('ndata', 68),\n",
       " ('naã¯ve', 68),\n",
       " ('betareg', 68),\n",
       " ('[m', 68),\n",
       " ('periodogram', 68),\n",
       " ('posthoc', 68),\n",
       " ('repmat', 68),\n",
       " ('out[', 68),\n",
       " ('simmathcal', 68),\n",
       " ('statisticsi', 68),\n",
       " ('hisei', 67),\n",
       " ('cforest', 67),\n",
       " ('testset', 67),\n",
       " ('agegroup', 67),\n",
       " ('detrending', 67),\n",
       " ('rmvnorm', 67),\n",
       " ('mdash', 67),\n",
       " ('theyve', 67),\n",
       " ('homoscedastic', 67),\n",
       " ('pastebin', 67),\n",
       " ('concatenate', 67),\n",
       " ('discretize', 67),\n",
       " ('correlogram', 67),\n",
       " ('tetab', 66),\n",
       " ('yeardecimal', 66),\n",
       " ('exper', 66),\n",
       " ('cramers', 66),\n",
       " ('clmm', 66),\n",
       " ('lleft', 65),\n",
       " ('stepaic', 65),\n",
       " ('[l', 65),\n",
       " ('iâ´m', 65),\n",
       " ('rsq', 65),\n",
       " ('mvn', 65),\n",
       " ('gridsearchcv', 65),\n",
       " ('predictorsi', 65),\n",
       " ('xtest', 65),\n",
       " ('yeardummy', 64),\n",
       " ('nlambda', 64),\n",
       " ('varident', 64),\n",
       " ('nfrac', 64),\n",
       " ('ploti', 64),\n",
       " ('biserial', 64),\n",
       " ('permute', 64),\n",
       " ('â·', 64),\n",
       " ('lowess', 64),\n",
       " ('mlogloss', 63),\n",
       " ('obdobinehn', 63),\n",
       " ('dbn', 63),\n",
       " ('ï\\x81', 63),\n",
       " ('rdata', 63),\n",
       " ('exp[', 63),\n",
       " ('subseteq', 63),\n",
       " ('ymax', 63),\n",
       " ('gpml', 63),\n",
       " ('[h', 63),\n",
       " ('xlabel', 63),\n",
       " ('crossprod', 63),\n",
       " ('observationsi', 63),\n",
       " ('y[i', 62),\n",
       " ('vectorizer', 62),\n",
       " ('identifiability', 62),\n",
       " ('plyr', 62),\n",
       " ('distr', 62),\n",
       " ('rbms', 62),\n",
       " ('iâ\\x80\\x99d', 62),\n",
       " ('r[', 61),\n",
       " ('confounder', 61),\n",
       " ('sexm', 61),\n",
       " ('mancova', 61),\n",
       " ('dxy', 61),\n",
       " ('nsubj', 61),\n",
       " ('xtmixed', 61),\n",
       " ('outputi', 61),\n",
       " ('qqline', 61),\n",
       " ('e[f', 61),\n",
       " ('â\\x89¤', 61),\n",
       " ('qsec', 61),\n",
       " ('distributionsuppose', 61),\n",
       " ('vectorid', 60),\n",
       " ('supportvector', 60),\n",
       " ('boldsymbolbeta', 60),\n",
       " ('test[', 60),\n",
       " ('c[', 60),\n",
       " ('nltk', 60),\n",
       " ('homoskedasticity', 60),\n",
       " ('rbeta', 60),\n",
       " ('ylabel', 60),\n",
       " ('clust', 60),\n",
       " ('regr', 60),\n",
       " ('measuresi', 60),\n",
       " ('logisticregression', 60),\n",
       " ('asdh', 59),\n",
       " ('sexmale', 59),\n",
       " ('phi[', 59),\n",
       " ('xleq', 59),\n",
       " ('ddots', 59),\n",
       " ('â\\x86\\x92', 59),\n",
       " ('colsums', 59),\n",
       " ('repeatedcv', 59),\n",
       " ('numdf', 59),\n",
       " ('arcsine', 59),\n",
       " ('ntheta', 59),\n",
       " ('false]', 59),\n",
       " ('devx', 58),\n",
       " ('frequentists', 58),\n",
       " ('z[', 58),\n",
       " ('î³', 58),\n",
       " ('e[epsilon', 58),\n",
       " ('mfx', 58),\n",
       " ('undersampling', 58),\n",
       " ('prodlimits', 58),\n",
       " ('expr', 58),\n",
       " ('ddply', 58),\n",
       " ('î·', 58),\n",
       " ('quantreg', 58),\n",
       " ('arcsin', 58),\n",
       " ('liblinear', 58),\n",
       " ('intervalsi', 58),\n",
       " ('sizesi', 58),\n",
       " ('acf[', 57),\n",
       " ('unbiasedness', 57),\n",
       " ('xhat', 57),\n",
       " ('ymin', 57),\n",
       " ('nrightarrow', 57),\n",
       " ('openbugs', 57),\n",
       " ('[y]', 57),\n",
       " ('eleft[', 57),\n",
       " ('newx', 57),\n",
       " ('actuals', 57),\n",
       " ('mtimes', 57),\n",
       " ('dbinom', 57),\n",
       " ('ptri', 56),\n",
       " ('wordtype', 56),\n",
       " ('subjid', 56),\n",
       " ('p[i', 56),\n",
       " ('coeftest', 56),\n",
       " ('descriptives', 56),\n",
       " ('rapidminer', 56),\n",
       " ('logl', 56),\n",
       " ('detrend', 56),\n",
       " ('exponentiated', 56),\n",
       " ('intervali', 56),\n",
       " ('factorsi', 56),\n",
       " ('p[i]', 56),\n",
       " ('[hat', 56),\n",
       " ('denom', 56),\n",
       " ('e[z', 56),\n",
       " ('ardl', 56),\n",
       " ('kpca', 55),\n",
       " ('probabilitiesi', 55),\n",
       " ('vgam', 55),\n",
       " ('plim', 55),\n",
       " ('unif', 55),\n",
       " ('lrtest', 55),\n",
       " ('m[', 55),\n",
       " ('asym', 55),\n",
       " ('featuresi', 55),\n",
       " ('z]', 55),\n",
       " ('lsmean', 55),\n",
       " ('ptimes', 55),\n",
       " ('variablessuppose', 55),\n",
       " ('subjectsubject', 54),\n",
       " ('gamma[', 54),\n",
       " ('ftype', 54),\n",
       " ('modelfit', 54),\n",
       " ('lbrace', 54),\n",
       " ('confusionmatrix', 54),\n",
       " ('vifs', 54),\n",
       " ('ï\\x86', 54),\n",
       " ('linalg', 54),\n",
       " ('[v', 54),\n",
       " ('errorsi', 54),\n",
       " ('glmmpql', 54),\n",
       " ('[frac', 54),\n",
       " ('lgamma', 53),\n",
       " ('tleft', 53),\n",
       " ('displaystylesumlimits', 53),\n",
       " ('thetaright', 53),\n",
       " ('tright', 53),\n",
       " ('partialtheta', 53),\n",
       " ('fhat', 53),\n",
       " ('rbrace', 53),\n",
       " ('widetilde', 53),\n",
       " ('modela', 53),\n",
       " ('subjectid', 53),\n",
       " ('xsigma', 53),\n",
       " ('armax', 53),\n",
       " ('pt]', 53),\n",
       " ('nbinom', 53),\n",
       " ('testim', 53),\n",
       " ('zuur', 53),\n",
       " ('[lmermod]', 53),\n",
       " ('testingi', 53),\n",
       " ('stopwords', 53),\n",
       " ('interpretability', 53),\n",
       " ('kfold', 53),\n",
       " ('dnn', 53),\n",
       " ('habtype', 52),\n",
       " ('mfit', 52),\n",
       " ('logn', 52),\n",
       " ('lambda[', 52),\n",
       " ('lnleft', 52),\n",
       " ('rsquared', 52),\n",
       " ('z[i]', 52),\n",
       " ('limma', 52),\n",
       " ('graphpad', 52),\n",
       " ('causalimpact', 52),\n",
       " ('distributionthe', 52),\n",
       " ('barplot', 52),\n",
       " ('nbar', 52),\n",
       " ('fitdistrplus', 52),\n",
       " ('pdata', 52),\n",
       " ('vmatrix', 52),\n",
       " ('[mathbf', 52),\n",
       " ('methodsi', 52),\n",
       " ('bartletts', 52),\n",
       " ('rstudio', 52),\n",
       " ('pointsi', 52),\n",
       " ('igraph', 52),\n",
       " ('dandok', 51),\n",
       " ('heightcm', 51),\n",
       " ('datasetsi', 51),\n",
       " ('randomisation', 51),\n",
       " ('backprop', 51),\n",
       " ('ksvm', 51),\n",
       " ('modelsim', 51),\n",
       " ('exogeneity', 51),\n",
       " ('glmi', 51),\n",
       " ('maxiter', 51),\n",
       " ('kright', 50),\n",
       " ('tsls', 50),\n",
       " ('leftarrow', 50),\n",
       " ('pscl', 50),\n",
       " ('populationi', 50),\n",
       " ('bigg[', 50),\n",
       " ('train[', 50),\n",
       " ('auroc', 50),\n",
       " ('xdata', 50),\n",
       " ('survdiff', 50),\n",
       " ('a[i]', 50),\n",
       " ('mvad', 50),\n",
       " ('[edit]', 50),\n",
       " ('meansi', 50),\n",
       " ('invlogit', 50),\n",
       " ('catpca', 49),\n",
       " ('nonumber', 49),\n",
       " ('lvert', 49),\n",
       " ('variableslet', 49),\n",
       " ('randomforestclassifier', 49),\n",
       " ('sigmas', 49),\n",
       " ('fwer', 49),\n",
       " ('thetas', 49),\n",
       " ('otimes', 49),\n",
       " ('binomially', 49),\n",
       " ('aics', 49),\n",
       " ('cnns', 49),\n",
       " ('qplot', 49),\n",
       " ('î£', 49),\n",
       " ('optctrl', 49),\n",
       " ('rlnorm', 49),\n",
       " ('â\\x9c\\x93', 48),\n",
       " ('driscat', 48),\n",
       " ('afaik', 48),\n",
       " ('tbeta', 48),\n",
       " ('jth', 48),\n",
       " ('bigl', 48),\n",
       " ('oneway', 48),\n",
       " ('dendf', 48),\n",
       " ('resids', 48),\n",
       " ('ybar', 48),\n",
       " ('hatmu', 48),\n",
       " ('deviationi', 48),\n",
       " ('noninformative', 48),\n",
       " ('cens', 48),\n",
       " ('svmi', 48),\n",
       " ('modelin', 48),\n",
       " ('remldev', 48),\n",
       " ('[n]', 48),\n",
       " ('regularizer', 48),\n",
       " ('â\\x8b', 47),\n",
       " ('egarch', 47),\n",
       " ('bayesglm', 47),\n",
       " ('[z', 47),\n",
       " ('e[u', 47),\n",
       " ('posixct', 47),\n",
       " ('multilabel', 47),\n",
       " ('nsigma', 47),\n",
       " ('datain', 47),\n",
       " ('notin', 47),\n",
       " ('nlog', 47),\n",
       " ('itll', 47),\n",
       " ('mylogit', 47),\n",
       " ('muright', 47),\n",
       " ('variableim', 47),\n",
       " ('nagq', 47),\n",
       " ('xmin', 47),\n",
       " ('setsi', 47),\n",
       " ('alpha[', 46),\n",
       " ('mcmcchain[', 46),\n",
       " ('matchit', 46),\n",
       " ('medoid', 46),\n",
       " ('emmeans', 46),\n",
       " ('factanal', 46),\n",
       " ('betahat', 46),\n",
       " ('groupi', 46),\n",
       " ('donâ´t', 46),\n",
       " ('plogis', 46),\n",
       " ('logloss', 46),\n",
       " ('bigram', 46),\n",
       " ('regressionsuppose', 46),\n",
       " ('distributionlet', 46),\n",
       " ('coord', 46),\n",
       " ('outliersi', 46),\n",
       " ('fcast', 46),\n",
       " ('forecastingi', 46),\n",
       " ('genea', 45),\n",
       " ('loglike', 45),\n",
       " ('var[x]', 45),\n",
       " ('inmathbb', 45),\n",
       " ('[the', 45),\n",
       " ('lmfit', 45),\n",
       " ('xmax', 45),\n",
       " ('pmax', 45),\n",
       " ('covtype', 45),\n",
       " ('regsubsets', 45),\n",
       " ('zph', 45),\n",
       " ('pseudoreplication', 45),\n",
       " ('meanlog', 45),\n",
       " ('numel', 45),\n",
       " ('ivreg', 45),\n",
       " ('hyndmans', 45),\n",
       " ('scatterplots', 45),\n",
       " ('t[', 45),\n",
       " ('ydata', 45),\n",
       " ('vectorized', 45),\n",
       " ('maxfun', 45),\n",
       " ('dists', 45),\n",
       " ('rarrb', 44),\n",
       " ('ymid', 44),\n",
       " ('subsetting', 44),\n",
       " ('fiti', 44),\n",
       " ('h]', 44),\n",
       " ('polytomous', 44),\n",
       " ('geeglm', 44),\n",
       " ('overfitted', 44),\n",
       " ('kstest', 44),\n",
       " ('elasticnet', 44),\n",
       " ('mydat', 44),\n",
       " ('rnns', 44),\n",
       " ('classprobs', 44),\n",
       " ('dt[', 44),\n",
       " ('regiao', 43),\n",
       " ('normalmix', 43),\n",
       " ('fyear', 43),\n",
       " ('n[i]', 43),\n",
       " ('println', 43),\n",
       " ('logp', 43),\n",
       " ('ï\\x87', 43),\n",
       " ('estim', 43),\n",
       " ('ktimes', 43),\n",
       " ('euclidian', 43),\n",
       " ('pbinom', 43),\n",
       " ('samplingi', 43),\n",
       " ('detrended', 43),\n",
       " ('sdlog', 43),\n",
       " ('cifar', 43),\n",
       " ('deeplearning', 43),\n",
       " ('nmds', 43),\n",
       " ('isnâ\\x80\\x99t', 43),\n",
       " ('sigma[', 43),\n",
       " ('bigcap', 43),\n",
       " ('networksi', 43),\n",
       " ('significanti', 43),\n",
       " ('discriminants', 43),\n",
       " ('pythoni', 43),\n",
       " ('stype', 43),\n",
       " ('ethz', 43),\n",
       " ('glmfit', 43),\n",
       " ('proportionsi', 43),\n",
       " ('aucs', 43),\n",
       " ('akjbkqkdk', 42),\n",
       " ('textsf', 42),\n",
       " ('clusterings', 42),\n",
       " ('left[left', 42),\n",
       " ('psmatch', 42),\n",
       " ('dbin', 42),\n",
       " ('epanechnikov', 42),\n",
       " ('predictionvalue', 42),\n",
       " ('emptyset', 42),\n",
       " ('bptest', 42),\n",
       " ('sgdclassifier', 42),\n",
       " ('nfolds', 42),\n",
       " ('kruschke', 42),\n",
       " ('medv', 42),\n",
       " ('parms', 42),\n",
       " ('stringsasfactors', 42),\n",
       " ('seriesim', 42),\n",
       " ('ci[', 42),\n",
       " ('asymp', 42),\n",
       " ('linearregression', 42),\n",
       " ('svydesign', 42),\n",
       " ('x[t', 42),\n",
       " ('topleft', 42),\n",
       " ('coefficienti', 42),\n",
       " ('mathoverflow', 42),\n",
       " ('ugarchspec', 41),\n",
       " ('multicolinearity', 41),\n",
       " ('linetype', 41),\n",
       " ('dydx', 41),\n",
       " ('sqr', 41),\n",
       " ('envir', 41),\n",
       " ('vcv', 41),\n",
       " ('lnorm', 41),\n",
       " ('big[', 41),\n",
       " ('nlminb', 41),\n",
       " ('eventsi', 41),\n",
       " ('e]', 41),\n",
       " ('xrange', 41),\n",
       " ('fprintf', 41),\n",
       " ('datasuppose', 41),\n",
       " ('nrightarrowinfty', 41),\n",
       " ('likelihoodi', 41),\n",
       " ('ageyrs', 40),\n",
       " ('summ', 40),\n",
       " ('v[', 40),\n",
       " ('sdev', 40),\n",
       " ('cdotfrac', 40),\n",
       " ('plsr', 40),\n",
       " ('datadist', 40),\n",
       " ('harrells', 40),\n",
       " ('[which', 40),\n",
       " ('zscore', 40),\n",
       " ('fullmodel', 40),\n",
       " ('[log', 40),\n",
       " ('depvar', 40),\n",
       " ('[w', 40),\n",
       " ...]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ov_glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looks like blindly stemming or lemmatization hurts the coverage the most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glove 800 with lower got final coverage of 43% and text coverage of 98.51%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####helper functions \n",
    "\n",
    "\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    \n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words\n",
    "\n",
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "\n",
    "def get_unknown_words(text):\n",
    "    \n",
    "    vocab_raw = build_vocab(text)\n",
    "    unk_words_raw = check_coverage(vocab_raw,word_matrix)\n",
    "    \n",
    "    return unk_words_raw\n",
    "\n",
    "\n",
    "numbers = []\n",
    "words = []\n",
    "chars = []\n",
    "punctuations = []\n",
    "not_ascii = []\n",
    "words_with_apostrophe = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def is_char(x):\n",
    "    \n",
    "    x = ord(x)\n",
    "    \n",
    "    if x >=65 and x<=90 or x>= 97 and x <= 122:\n",
    "        return True\n",
    "        #meaning its a character \n",
    "    else:\n",
    "        return False\n",
    "        #not a character \n",
    "        \n",
    "        \n",
    "\n",
    "for word in tqdm(word_matrix.keys()):\n",
    "    \n",
    "     if word.isdigit():\n",
    "            numbers.append(word)\n",
    "     \n",
    "     elif  len(word) == 1 and not is_char(word):\n",
    "            punctuations.append(word)\n",
    "            \n",
    "     elif len(word) == 1 and is_char(word):\n",
    "            chars.append(word)\n",
    "     \n",
    "     elif not word.isascii():\n",
    "            not_ascii.append(word)\n",
    "            \n",
    "     elif word[0] == \"'\":\n",
    "          words_with_apostrophe.append(word)\n",
    "     else:\n",
    "            words.append(word)   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 31.54% of vocab\n",
      "Found embeddings for  97.37% of all text\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# adding space to punctuations \n",
    "import re\n",
    "\n",
    "\n",
    "puncts = ['´',',', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "\n",
    "# no_spacing = [\"'\",'.','-']\n",
    "\n",
    "to_replace = '´'\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    \n",
    "    x = str(x)\n",
    "    \n",
    "    #will convert ´ to '\n",
    "    \n",
    "    if to_replace in x:\n",
    "        x = x.replace(to_replace,\"'\")\n",
    "    \n",
    "    \n",
    "    for punct in puncts:\n",
    "        if punct  == \"'\":\n",
    "            x = x.replace(punct, f' {punct}')\n",
    "        else:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    \n",
    " \n",
    "    #will convert e.g to eg\n",
    "    if '.' in x:\n",
    "        x = x.replace('.','')\n",
    "        \n",
    "    \n",
    "    # have added lower \n",
    "    return x.lower()\n",
    "\n",
    "\n",
    "cleaned_text = unprocessed_text.map(clean_text)\n",
    "\n",
    "# mapping punctuations \n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \n",
    "                 \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \n",
    "                 \"√\": \" sqrt \", \"×\": \"x\", \"²\": \n",
    "                 \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \n",
    "                 \"'\", \"_\": \"-\", \n",
    "                 \"`\": \"'\", '“': \n",
    "                 '\"', '”': '\"', \n",
    "                 '“': '\"', \"£\": \n",
    "                 \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n",
    "\n",
    "\n",
    "\n",
    "def punct_mapper(text, mapping):\n",
    "    \n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])    \n",
    "    return text\n",
    "\n",
    "\n",
    "# cleaned_text = cleaned_text.map(lambda x: punct_mapper(x,punct_mapping))\n",
    "\n",
    "# mapping digits to #'s\n",
    "def clean_numbers(x):\n",
    "    \n",
    "    \n",
    "#     x = re.sub('[0-9]{5}','#####',x)\n",
    "#     x = re.sub('[0-9]{4}','###'',x)\n",
    "    x = re.sub('[0-9]{3}',' ### ',x)\n",
    "    x = re.sub('[0-9]{2}',' ## ',x)\n",
    "    \n",
    "    return x\n",
    "    \n",
    "\n",
    "cleaned_text = cleaned_text.map(clean_numbers)\n",
    "ov_glove = get_unknown_words(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Correction Method 2 \n",
    "\n",
    "In this method we require to load word2vec model . So it needs a lot of memory. \n",
    "The way we can use stemming , lemmatization and spelling correction if we dont find the vector embedding for a \n",
    "given word.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "from __future__ import absolute_import, division\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "### stemmers \n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "import sys\n",
    "from os.path import dirname\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runing '"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../kaggle/embedding/GoogleNews-vectors-negative300/'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(path + 'GoogleNews-vectors-negative300.bin', \n",
    "                                                        binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded...\n",
      "done..\n"
     ]
    }
   ],
   "source": [
    "words = model.index2word\n",
    "\n",
    "print('loaded...')\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "WORDS = w_rank\n",
    "\n",
    "print('done..')\n",
    "\n",
    "\n",
    "# Use fast text as vocabulary\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or [word])\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def singlify(word):\n",
    "    \n",
    "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correcting words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction('rbun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifying coverage to include correction\n",
    "\n",
    "\n",
    "def check_coverage(vocab, embeddings_index,with_correction):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    \n",
    "    for word in tqdm(vocab.keys()):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            if with_correction:\n",
    "                if len(word)>1:\n",
    "                    known_words[word] = embeddings_index[correction(word)]\n",
    "                else:\n",
    "                    known_words[word] = embeddings_index[word]\n",
    "            else:\n",
    "                known_words[word] = embeddings_index[word]\n",
    "            \n",
    "            nb_known_words += vocab[word]\n",
    "       \n",
    "    \n",
    "    \n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126838/126838 [00:53<00:00, 2349.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 53.53% of vocab\n",
      "Found embeddings for  98.45% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(cleaned_text)\n",
    "\n",
    "unk_words1 = check_coverage(vocab,word_matrix,with_correction = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok this is fantastic !!! our vocab coverage has increased over 50% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below function is used while loading creating the weight_matrix \n",
    "\n",
    "#### The method is quite simple Search for the stem , corrected , lemmatized version of the word only when the current version of the word is not present in the embeddings. \n",
    "\n",
    "#### eg. say we are looking for the embedding for word running which we can't find,if we apply stemming we get run whose vector is present in the embedding matrix this approach is better than blindly applying stem,lemmatiozation operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stem = []\n",
    "correct = []\n",
    "lemma = []\n",
    "\n",
    "def load_glove(vocab,embeddings_index):\n",
    "\n",
    "    '''\n",
    "    word is check \n",
    "    1. the actual word \n",
    "    2. lower case and check\n",
    "    3. upper case and check\n",
    "    4. captalize and check\n",
    "    5. Stem and check\n",
    "    6. lemma and check\n",
    "    7. correct spelling and check.\n",
    "    '''\n",
    "    known_words = 0\n",
    "    unk_words = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for key in tqdm(vocab.keys()):\n",
    "\n",
    "        word = key\n",
    "\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        #checking if word is present.\n",
    "        if embedding_vector is not None:\n",
    "#             embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            known_words += 1\n",
    "            continue\n",
    "\n",
    "        #check if lower case of the word is present.\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "#             embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            known_words += 1\n",
    "            continue\n",
    "\n",
    "        #cheking if the upper case of the word is present.\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "#             embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            known_words += 1\n",
    "            continue\n",
    "\n",
    "        #checking if capital word is present.\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "#             embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            known_words += 1\n",
    "            continue\n",
    "\n",
    "        '''\n",
    "        Three steppers are  used \n",
    "\n",
    "        ps -> PorterStemmer \n",
    "        ls -> lancaster \n",
    "        sb->snowball stemmer \n",
    "\n",
    "        need to check on this as well\n",
    "\n",
    "        \n",
    "        '''    \n",
    "\n",
    "\n",
    "\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "#             embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            stem.append(word)\n",
    "            known_words += 1\n",
    "            continue\n",
    "\n",
    "\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "#             embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            stem.append(word)\n",
    "            known_words += 1\n",
    "            continue\n",
    "\n",
    "\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "#             embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            stem.append(word)\n",
    "            known_words += 1\n",
    "            \n",
    "            continue\n",
    "\n",
    "        '''\n",
    "        Using Lemmatization\n",
    "        \n",
    "        '''    \n",
    "\n",
    "\n",
    "        word = wnl.lemmatize(key,'v')\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "#             embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            lemma.append(key)\n",
    "            known_words += 1\n",
    "            continue\n",
    "\n",
    "        ''''\n",
    "        Checking for spellling mistakes \n",
    "        \n",
    "        '''    \n",
    "\n",
    "\n",
    "\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "#                 embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                correct.append(word)\n",
    "                known_words += 1\n",
    "                continue\n",
    "\n",
    "                \n",
    "        #unknown words.    \n",
    "#         embedding_matrix[word_dict[key]] = unknown_vector        \n",
    "        unk_words += 1\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    return unk_words,known_words \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126838/126838 [00:59<00:00, 2145.97it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(cleaned_text)\n",
    "\n",
    "unk_words,known_words = load_glove(vocab,word_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126838"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words find by stemmers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11542"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['connor',\n",
       " 'machin',\n",
       " 'number',\n",
       " 'method',\n",
       " 'text',\n",
       " 'components',\n",
       " 'tests',\n",
       " 'sets',\n",
       " 'method',\n",
       " 'sizes',\n",
       " 'observations',\n",
       " 'time',\n",
       " 'crossing',\n",
       " 'survey',\n",
       " 'improv',\n",
       " 'variance',\n",
       " 'clustering',\n",
       " 'regular',\n",
       " 'score',\n",
       " 'train',\n",
       " 'respect',\n",
       " 'background',\n",
       " 'network',\n",
       " 'ranks',\n",
       " 'effect',\n",
       " 'sign',\n",
       " 'videos',\n",
       " 'testing',\n",
       " 'trend',\n",
       " 'purus',\n",
       " 'int',\n",
       " 'analysis',\n",
       " 'inverter',\n",
       " 'regression',\n",
       " 'pcl',\n",
       " 'revise',\n",
       " 'dredg',\n",
       " 'data',\n",
       " 'vector',\n",
       " 'series',\n",
       " 'period',\n",
       " 'loss',\n",
       " 'log',\n",
       " 'correlated',\n",
       " 'paradox',\n",
       " 'subsample',\n",
       " 'across',\n",
       " 'tmt',\n",
       " 'needed',\n",
       " 'expectancy',\n",
       " 'help',\n",
       " 'best',\n",
       " 'simulations',\n",
       " 'stl',\n",
       " 'distributions',\n",
       " 'ind',\n",
       " 'run',\n",
       " 'pairs',\n",
       " 'imperfect',\n",
       " 'toler',\n",
       " 'cov',\n",
       " 'target',\n",
       " 'l2',\n",
       " 'lsa',\n",
       " 'rpi',\n",
       " 'interaction',\n",
       " 'datapoint',\n",
       " 'variables',\n",
       " 'inflation',\n",
       " 'matrix',\n",
       " 'term',\n",
       " 'distribution',\n",
       " 'spec',\n",
       " 'rade',\n",
       " 'at',\n",
       " 'sc',\n",
       " 'exp',\n",
       " 'cont',\n",
       " 'population',\n",
       " 'container',\n",
       " 'rpo',\n",
       " 'ric',\n",
       " 'function',\n",
       " 'auc',\n",
       " 'sigma',\n",
       " 'classification',\n",
       " 'variable',\n",
       " 'forecasting',\n",
       " 'missing',\n",
       " 'realy',\n",
       " 'dist',\n",
       " 'mode',\n",
       " 'tide',\n",
       " 'rvi',\n",
       " 'perceptron',\n",
       " 'educ',\n",
       " 'data',\n",
       " 'peak',\n",
       " 'vectors',\n",
       " 'robust',\n",
       " 'means',\n",
       " 'analysis',\n",
       " 'size',\n",
       " 'samples',\n",
       " 'does',\n",
       " 'strat',\n",
       " 'transformations',\n",
       " 'neurons',\n",
       " 'matlab',\n",
       " 'fan',\n",
       " 'statistic',\n",
       " 'linear',\n",
       " 'numbers',\n",
       " 'contini',\n",
       " 'categorical',\n",
       " 'correlations',\n",
       " 'available',\n",
       " 'yh',\n",
       " 'regress',\n",
       " 'account',\n",
       " 'mot',\n",
       " 'sim',\n",
       " 'rbm',\n",
       " 'answer',\n",
       " 'mode',\n",
       " 'correction',\n",
       " 'paid',\n",
       " 'prediction',\n",
       " 'framework',\n",
       " 'collinear',\n",
       " 'other',\n",
       " 'classes',\n",
       " 'data',\n",
       " 'coefficient',\n",
       " 'accuracy',\n",
       " 'bot',\n",
       " 'ext',\n",
       " 'vec',\n",
       " 'class',\n",
       " 'bin',\n",
       " 'bin',\n",
       " 'tran',\n",
       " 'residuals',\n",
       " 'dpo',\n",
       " 'time',\n",
       " 'fail',\n",
       " 'lam',\n",
       " 'theta',\n",
       " 'init',\n",
       " 'algorithm',\n",
       " 'centroid',\n",
       " 'si',\n",
       " 'itl',\n",
       " 'irl',\n",
       " 'pain',\n",
       " 'suffer',\n",
       " 'decies',\n",
       " 'folder',\n",
       " 'minit',\n",
       " 'moven',\n",
       " 'dme',\n",
       " 'ability',\n",
       " 'obl',\n",
       " 'intervals',\n",
       " 'beggin',\n",
       " 'sign',\n",
       " 'dataset',\n",
       " 'random',\n",
       " 'the',\n",
       " 'somehow',\n",
       " 'inertia',\n",
       " 'cut',\n",
       " 'answers',\n",
       " 'errors',\n",
       " 'statistics',\n",
       " 'side',\n",
       " 'indirect',\n",
       " 'norma',\n",
       " 'algorithms',\n",
       " 'models',\n",
       " 'soult',\n",
       " 'rate',\n",
       " 'transformation',\n",
       " 'toy',\n",
       " 'predictors',\n",
       " 'enough',\n",
       " 'absolut',\n",
       " 'diversity',\n",
       " 'init',\n",
       " 'likelihood',\n",
       " 'proportions',\n",
       " 'confound',\n",
       " 'sim',\n",
       " 'sim',\n",
       " 'ques',\n",
       " 'find',\n",
       " 'ip',\n",
       " 'cont',\n",
       " 'trat',\n",
       " 'estimators',\n",
       " 'regress',\n",
       " 'formula',\n",
       " 'interpretation',\n",
       " 'biases',\n",
       " 'prob',\n",
       " 'scatters',\n",
       " 'process',\n",
       " 'sery',\n",
       " 'clusters',\n",
       " 'anova',\n",
       " 'cv',\n",
       " 'ext',\n",
       " 'mse',\n",
       " 'counts',\n",
       " 'jags',\n",
       " 'yt',\n",
       " 'yt',\n",
       " 'partit',\n",
       " 'kernel',\n",
       " 'rbf',\n",
       " 'ensembl',\n",
       " 'sample',\n",
       " 'it',\n",
       " 'interpret',\n",
       " 'parameter',\n",
       " 'th',\n",
       " 'techniques',\n",
       " 'groups',\n",
       " 'mard',\n",
       " 'participants',\n",
       " 'sever',\n",
       " 'pattern',\n",
       " 'problem',\n",
       " 'glm',\n",
       " 'foral',\n",
       " 'domains',\n",
       " 'value',\n",
       " 'biomass',\n",
       " 'seed',\n",
       " 'growth',\n",
       " 'unknown',\n",
       " 'grouping',\n",
       " 'mean',\n",
       " 'litteratur',\n",
       " 'hurdles',\n",
       " 'experiment',\n",
       " 'actual',\n",
       " 'asd',\n",
       " 'yl',\n",
       " 'subjects',\n",
       " 'instrument',\n",
       " 'appr',\n",
       " 'document',\n",
       " 'squares',\n",
       " 'youd',\n",
       " 'stata',\n",
       " 'fourier',\n",
       " 'op',\n",
       " 'normal',\n",
       " 'mcmc',\n",
       " 'ctp',\n",
       " 'results',\n",
       " 'aircraft',\n",
       " 'aluminium',\n",
       " 'valley',\n",
       " 'rawd',\n",
       " 'od',\n",
       " 'populations',\n",
       " 'regress',\n",
       " 'esp',\n",
       " 'real',\n",
       " 'inventory',\n",
       " 'alya',\n",
       " 'values',\n",
       " 'ks',\n",
       " 'dado',\n",
       " 'des',\n",
       " 'sites',\n",
       " 'comp',\n",
       " 'result',\n",
       " 'init',\n",
       " 'licor',\n",
       " 'efa',\n",
       " 'efa',\n",
       " 'heuristics',\n",
       " 'effects',\n",
       " 'rus',\n",
       " 'lambda',\n",
       " 'control',\n",
       " 'pops',\n",
       " 'season',\n",
       " 'type',\n",
       " 'dollar',\n",
       " 'winter',\n",
       " 'sale',\n",
       " 'profit',\n",
       " 'spring',\n",
       " 'summer',\n",
       " 'fall',\n",
       " 'comparison',\n",
       " 'probability',\n",
       " 'application',\n",
       " 'calc',\n",
       " 'recommender',\n",
       " 'treatment',\n",
       " 'useful',\n",
       " 'ppo',\n",
       " 'measures',\n",
       " 'pool',\n",
       " 'rank',\n",
       " 'stii',\n",
       " 'look',\n",
       " 'ins',\n",
       " 'modelling',\n",
       " 'gram',\n",
       " 'sign',\n",
       " 'svm',\n",
       " 'rates',\n",
       " 'calibration',\n",
       " 'mec',\n",
       " 'prop',\n",
       " 'permut',\n",
       " 'norm',\n",
       " 'der',\n",
       " 'cont',\n",
       " 'question',\n",
       " 'learning',\n",
       " 'confused',\n",
       " 'skew',\n",
       " 'miss',\n",
       " 'problem',\n",
       " 'focal',\n",
       " 'zob',\n",
       " 'custom',\n",
       " 'ablin',\n",
       " 'classifier',\n",
       " 'bayesian',\n",
       " 'pleas',\n",
       " 'var',\n",
       " 'mean',\n",
       " 'equ',\n",
       " 'pdf',\n",
       " 'start',\n",
       " 'option',\n",
       " 'linewidth',\n",
       " 'gauss',\n",
       " 'mix',\n",
       " 'peth',\n",
       " 'nhan',\n",
       " 'squared',\n",
       " 'true',\n",
       " 'rund',\n",
       " 'libe',\n",
       " 'sigma',\n",
       " 'estimations',\n",
       " 'like',\n",
       " 'big',\n",
       " 'curve',\n",
       " 'smap',\n",
       " 'cost',\n",
       " 'max',\n",
       " 'perc',\n",
       " 'analysis',\n",
       " 'lava',\n",
       " 'matching',\n",
       " 'subject',\n",
       " 'partition',\n",
       " 'image',\n",
       " 'vector',\n",
       " 'cov',\n",
       " 'cdot',\n",
       " 'import',\n",
       " 'seperately',\n",
       " 'website',\n",
       " 'smooth',\n",
       " 'algo',\n",
       " 'ssas',\n",
       " 'class',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'fail',\n",
       " 'param',\n",
       " 'lem',\n",
       " 'timey',\n",
       " 'clod',\n",
       " 'measures',\n",
       " 'design',\n",
       " 'validation',\n",
       " 'diff',\n",
       " 'factor',\n",
       " 'features',\n",
       " 'tanh',\n",
       " 'sigmoid',\n",
       " 'clf',\n",
       " 'roc',\n",
       " 'msm',\n",
       " 'preference',\n",
       " 'stumped',\n",
       " 'independence',\n",
       " 'cliques',\n",
       " 'gbm',\n",
       " 'caret',\n",
       " 'prob',\n",
       " 'sarim',\n",
       " 'clusters',\n",
       " 'pref',\n",
       " 'outliers',\n",
       " 'seem',\n",
       " 'solv',\n",
       " 'points',\n",
       " 'labels',\n",
       " 'textbooks',\n",
       " 'prob',\n",
       " 'arma',\n",
       " 'parameters',\n",
       " 'intercept',\n",
       " 'unl',\n",
       " 'lare',\n",
       " 'explanation',\n",
       " 'poorman',\n",
       " 'python',\n",
       " 'all',\n",
       " 'car',\n",
       " 'san',\n",
       " 'color',\n",
       " 'black',\n",
       " 'weight',\n",
       " 'size',\n",
       " 'serif',\n",
       " 'bold',\n",
       " 'load',\n",
       " 'fonts',\n",
       " 'both',\n",
       " 'major',\n",
       " 'labels',\n",
       " 'minor',\n",
       " 'estimation',\n",
       " 'study',\n",
       " 'problems',\n",
       " 'factor',\n",
       " 'class',\n",
       " 'relations',\n",
       " 'dat',\n",
       " 'long',\n",
       " 'energies',\n",
       " 'scale',\n",
       " 'manner',\n",
       " 'anova',\n",
       " 'evaluation',\n",
       " 'equ',\n",
       " 'egpc',\n",
       " 'testing',\n",
       " 'vdot',\n",
       " 'measures',\n",
       " 'maximization',\n",
       " 'inject',\n",
       " 'permut',\n",
       " 'cen',\n",
       " 'compound',\n",
       " 'aido',\n",
       " 'how',\n",
       " 'norm',\n",
       " 'left',\n",
       " 'vector',\n",
       " 'valid',\n",
       " 'reaction',\n",
       " 'age',\n",
       " 'group',\n",
       " 'parameters',\n",
       " 'subs',\n",
       " 'quantity',\n",
       " 'form',\n",
       " 'forecast',\n",
       " 'irb',\n",
       " 'logit',\n",
       " 'ppn',\n",
       " 'x0',\n",
       " 'x1',\n",
       " 'subject',\n",
       " 'predict',\n",
       " 'gamm',\n",
       " 'list',\n",
       " 'paper',\n",
       " 'something',\n",
       " 'ash',\n",
       " 'boxes',\n",
       " 'definition',\n",
       " 'facet',\n",
       " 'orden',\n",
       " 'urca',\n",
       " 'products',\n",
       " 'thirteen',\n",
       " 'rabbe',\n",
       " 'trials',\n",
       " 'obvious',\n",
       " 'higher',\n",
       " 'precis',\n",
       " 'differences',\n",
       " 'extractions',\n",
       " 'funt',\n",
       " 'have',\n",
       " 'proportion',\n",
       " 'rate',\n",
       " 'different',\n",
       " 'chi',\n",
       " 'peter',\n",
       " 'gen',\n",
       " 'config',\n",
       " 'nets',\n",
       " 'practice',\n",
       " 'statist',\n",
       " 'well',\n",
       " 'effects',\n",
       " 'residual',\n",
       " 'sarg',\n",
       " 'attributes',\n",
       " 'quantization',\n",
       " 'interpret',\n",
       " 'regressors',\n",
       " 'choices',\n",
       " 'bis',\n",
       " 'rd',\n",
       " 'lmi',\n",
       " 'uh',\n",
       " 'arul',\n",
       " 'match',\n",
       " 'responses',\n",
       " 'prat',\n",
       " 'averages',\n",
       " 'bernoulli',\n",
       " 'outcome',\n",
       " 'dummy',\n",
       " 'quasi',\n",
       " 'covariates',\n",
       " 'layer',\n",
       " 'node',\n",
       " 'dat',\n",
       " 'err',\n",
       " 'it',\n",
       " 'learn',\n",
       " 'coefficients',\n",
       " 'normality',\n",
       " 'ac',\n",
       " 'column',\n",
       " 'pca',\n",
       " 'connected',\n",
       " 'significance',\n",
       " 'theorem',\n",
       " 'feature',\n",
       " 'trees',\n",
       " 'row',\n",
       " 'chart',\n",
       " 'deviation',\n",
       " 'series',\n",
       " 'zou',\n",
       " 'country',\n",
       " 'phi',\n",
       " 'subscript',\n",
       " 'levels',\n",
       " 'as',\n",
       " 'cond',\n",
       " 'id',\n",
       " 'lr',\n",
       " 'gb',\n",
       " 'nb',\n",
       " 'hard',\n",
       " 'graphically',\n",
       " 'dah',\n",
       " 'association',\n",
       " 'vri',\n",
       " 'bin',\n",
       " 'integral',\n",
       " 'cit',\n",
       " 'slopes',\n",
       " 'var',\n",
       " 'msv',\n",
       " 'training',\n",
       " 'question',\n",
       " 'changes',\n",
       " 'constant',\n",
       " 'estimate',\n",
       " 'acer',\n",
       " 'count',\n",
       " 'standard',\n",
       " 'plot',\n",
       " 'len',\n",
       " 'gaf',\n",
       " 'law',\n",
       " 'package',\n",
       " 'veve',\n",
       " 'css',\n",
       " 'ml',\n",
       " 'ethnocentric',\n",
       " 'weak',\n",
       " 'output',\n",
       " 'statist',\n",
       " 'statist',\n",
       " 'er',\n",
       " 'statistically',\n",
       " 'a8',\n",
       " 'writ',\n",
       " 'open',\n",
       " 'sign',\n",
       " 'sde',\n",
       " 'estimates',\n",
       " 'reap',\n",
       " 'inter',\n",
       " 'implementation',\n",
       " 'loses',\n",
       " 'ixs',\n",
       " 'maxima',\n",
       " 'mon',\n",
       " 'significant',\n",
       " 'mrf',\n",
       " 'raster',\n",
       " 'clump',\n",
       " 'param',\n",
       " 'overfitting',\n",
       " 'outcomes',\n",
       " 'tip',\n",
       " 'helpful',\n",
       " 'speedup',\n",
       " 'fiel',\n",
       " 'winsor',\n",
       " 'kern',\n",
       " 'top',\n",
       " 'student',\n",
       " 'slow',\n",
       " 'img',\n",
       " 'system',\n",
       " 'ps',\n",
       " 'parameter',\n",
       " 'control',\n",
       " 'spearman',\n",
       " 'others',\n",
       " 'trim',\n",
       " 'elas',\n",
       " 'error',\n",
       " 'an',\n",
       " 'replacement',\n",
       " 'filter',\n",
       " 'bootstrap',\n",
       " 'analy',\n",
       " 'neighbour',\n",
       " 'tutorials',\n",
       " 'preprocessing',\n",
       " 'comparability',\n",
       " 'mee',\n",
       " 'causation',\n",
       " 'line',\n",
       " 'beg',\n",
       " 'yv',\n",
       " 'ewm',\n",
       " 'networks',\n",
       " 'section',\n",
       " 'graph',\n",
       " 'model',\n",
       " 'cancer',\n",
       " 'sign',\n",
       " 'module',\n",
       " 'fallacy',\n",
       " 'jaynes',\n",
       " 'bayesian',\n",
       " 'mer',\n",
       " 'insights',\n",
       " 'factors',\n",
       " 'grid',\n",
       " 'mung',\n",
       " 'resit',\n",
       " 'district',\n",
       " 'district',\n",
       " 'district',\n",
       " 'station',\n",
       " 'plott',\n",
       " 'rnn',\n",
       " 'properly',\n",
       " 'mu',\n",
       " 'format',\n",
       " 'psi',\n",
       " 'gamm',\n",
       " 'not',\n",
       " 'iv',\n",
       " 'sums',\n",
       " 'befor',\n",
       " 'after',\n",
       " 'accurate',\n",
       " 'user',\n",
       " 'chans',\n",
       " 'tri',\n",
       " 'sours',\n",
       " 'neglect',\n",
       " 'alpha',\n",
       " 'images',\n",
       " 'cont',\n",
       " 'mult',\n",
       " 'evaluators',\n",
       " 'taken',\n",
       " 'triply',\n",
       " 'sattur',\n",
       " '5p',\n",
       " 'cnn',\n",
       " 'ratio',\n",
       " 'aic',\n",
       " 'vif',\n",
       " 'unsc',\n",
       " 'wolfgang',\n",
       " 'cots',\n",
       " 'regression',\n",
       " 'censoring',\n",
       " 'histogram',\n",
       " 'boel',\n",
       " 'interval',\n",
       " 'baseball',\n",
       " 'compra',\n",
       " 'sin',\n",
       " 'gaur',\n",
       " 'gaur',\n",
       " 'descent',\n",
       " 'rigor',\n",
       " 'there',\n",
       " 'steps',\n",
       " 'methods',\n",
       " 'mining',\n",
       " 'idv',\n",
       " 'auto',\n",
       " 'simul',\n",
       " 'aic',\n",
       " 'explained',\n",
       " 'coloni',\n",
       " 'cdf',\n",
       " 'lti',\n",
       " 'forecasts',\n",
       " 'mse',\n",
       " 'shape',\n",
       " 'good',\n",
       " 'poor',\n",
       " 'additivity',\n",
       " 'grateful',\n",
       " 'suss',\n",
       " 'recommendation',\n",
       " 'standard',\n",
       " 'copulas',\n",
       " 'sports',\n",
       " 'un',\n",
       " 'scaling',\n",
       " 'abbrev',\n",
       " 'conv',\n",
       " 'manually',\n",
       " 'pes',\n",
       " 'graphs',\n",
       " 'bill',\n",
       " 'food',\n",
       " 'cloth',\n",
       " 'entertain',\n",
       " 'int',\n",
       " 'cum',\n",
       " 'seasonality',\n",
       " 'prefer',\n",
       " 'functions',\n",
       " 'images',\n",
       " 'hoc',\n",
       " 'b1',\n",
       " 'bx',\n",
       " 'bootstrapping',\n",
       " 'am',\n",
       " 'biggin',\n",
       " 'weights',\n",
       " 'diff',\n",
       " 'collect',\n",
       " 'assumption',\n",
       " 'doing',\n",
       " 'logistics',\n",
       " 'risk',\n",
       " 'peca',\n",
       " 'group',\n",
       " 'uid',\n",
       " 'am',\n",
       " 'reconstructed',\n",
       " 'sdat',\n",
       " 'scenario',\n",
       " 'class',\n",
       " 'stuck',\n",
       " 'acari',\n",
       " 'root',\n",
       " 'account',\n",
       " 'pit',\n",
       " 'column',\n",
       " 'depth',\n",
       " 'known',\n",
       " 'dev',\n",
       " 'scores',\n",
       " 'measurement',\n",
       " 'distinct',\n",
       " 'values',\n",
       " 'align',\n",
       " 'timestep',\n",
       " 'complement',\n",
       " 'balls',\n",
       " 'modeling',\n",
       " 'prai',\n",
       " 'valu',\n",
       " 'step',\n",
       " 'panel',\n",
       " 'town',\n",
       " 'dist',\n",
       " 'etc',\n",
       " 'read',\n",
       " 'average',\n",
       " 'varus',\n",
       " 'frequencies',\n",
       " 'sign',\n",
       " 'events',\n",
       " 'boot',\n",
       " 'cov',\n",
       " 'major',\n",
       " 'suv',\n",
       " 'cac',\n",
       " 'shapes',\n",
       " 'rho',\n",
       " '0m',\n",
       " 'der',\n",
       " 'freedom',\n",
       " 'score',\n",
       " 'abundances',\n",
       " 'arang',\n",
       " 'what',\n",
       " 'deviate',\n",
       " 'weak',\n",
       " 'sets',\n",
       " 'group',\n",
       " 'market',\n",
       " 'init',\n",
       " 'nmd',\n",
       " 'local',\n",
       " 'hyperplane',\n",
       " 'zeroes',\n",
       " 'meaningful',\n",
       " 'regress',\n",
       " 'euclidean',\n",
       " 'default',\n",
       " 'model',\n",
       " 'comply',\n",
       " 'pak',\n",
       " 'areas',\n",
       " 'rco',\n",
       " 'written',\n",
       " 'steps',\n",
       " 'equations',\n",
       " 'peaks',\n",
       " 'new',\n",
       " 'root',\n",
       " 'support',\n",
       " 'beta',\n",
       " 'analys',\n",
       " 'issue',\n",
       " 'amount',\n",
       " 'sla',\n",
       " 'selection',\n",
       " 'items',\n",
       " 'prices',\n",
       " 'und',\n",
       " 'correlation',\n",
       " 'apply',\n",
       " 'setp',\n",
       " 'networks',\n",
       " 'conjugate',\n",
       " 'supress',\n",
       " 'prone',\n",
       " 'adress',\n",
       " 'diagram',\n",
       " 'causality',\n",
       " 'wholes',\n",
       " 'tails',\n",
       " 'bfg',\n",
       " 'use',\n",
       " 'programm',\n",
       " 'hypotheses',\n",
       " 'season',\n",
       " 'density',\n",
       " 'tutorial',\n",
       " 'stud',\n",
       " 'tp',\n",
       " 'lines',\n",
       " 'theory',\n",
       " 'wins',\n",
       " 'once',\n",
       " 'abundance',\n",
       " 'popular',\n",
       " 'prove',\n",
       " 'forecast',\n",
       " 'ap',\n",
       " 'forecast',\n",
       " 'forecast',\n",
       " 'ius',\n",
       " 'rescaling',\n",
       " 'cif',\n",
       " 'bst',\n",
       " 'weighting',\n",
       " 'weight',\n",
       " 'fitr',\n",
       " 'alph',\n",
       " 'partit',\n",
       " 'lat',\n",
       " 'precision',\n",
       " 'slope',\n",
       " 'ear',\n",
       " 'within',\n",
       " 'inexpert',\n",
       " 'ident',\n",
       " 'nans',\n",
       " 'besides',\n",
       " 'pna',\n",
       " 'syst',\n",
       " 'phi',\n",
       " 'benchmark',\n",
       " 'minim',\n",
       " 'mulder',\n",
       " 'gamma',\n",
       " 'small',\n",
       " 'gaussian',\n",
       " 'shrink',\n",
       " 'train',\n",
       " 'italic',\n",
       " 'valu',\n",
       " 'pareto',\n",
       " 'parm',\n",
       " 'vary',\n",
       " 'null',\n",
       " 'member',\n",
       " 'table',\n",
       " 'nast',\n",
       " 'sparc',\n",
       " 'mention',\n",
       " 'els',\n",
       " 'cheat',\n",
       " 'covariance',\n",
       " 'mode',\n",
       " 'part',\n",
       " 'prod',\n",
       " 'diagrams',\n",
       " 'cond',\n",
       " 'muv',\n",
       " 'g1',\n",
       " 'g2',\n",
       " 'cor',\n",
       " 'auc',\n",
       " 'pipeline',\n",
       " 'windsor',\n",
       " 'txi',\n",
       " 'lag',\n",
       " 'for',\n",
       " 'pseudo',\n",
       " 'improv',\n",
       " 'improv',\n",
       " 'cases',\n",
       " 'loud',\n",
       " 'bins',\n",
       " 'objet',\n",
       " 'comp',\n",
       " 'learn',\n",
       " 'boosting',\n",
       " 'combinations',\n",
       " 'to',\n",
       " 'aler',\n",
       " 'equation',\n",
       " 'misinterpret',\n",
       " 'sergio',\n",
       " 'issues',\n",
       " 'pred',\n",
       " 'mathematically',\n",
       " 'publish',\n",
       " 'smote',\n",
       " 'corel',\n",
       " 'independent',\n",
       " 'lemma',\n",
       " 'uncertainties',\n",
       " 'http',\n",
       " 'faces',\n",
       " 'width',\n",
       " 'quarters',\n",
       " 'rate',\n",
       " 'nabl',\n",
       " 'fundament',\n",
       " 'loss',\n",
       " 'individuals',\n",
       " 'learn',\n",
       " 'redi',\n",
       " 'dun',\n",
       " 'keg',\n",
       " 'invert',\n",
       " 'overal',\n",
       " 'error',\n",
       " 'data',\n",
       " 'ret',\n",
       " 'rng',\n",
       " 'gam',\n",
       " 'nich',\n",
       " 'requirements',\n",
       " 'mmo',\n",
       " 'result',\n",
       " ...]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words find by lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words Found by Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19742"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at', 'march', 'leverage', 'acceptable', 'kin']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total New Words found  By load_glove method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31333"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stem) + len(lemma) + len(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incredible we found embeddings for 30k words a lot of them we using stemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# new things learned \n",
    "1. dealing with contractions \n",
    "2. dealing with punctuations \n",
    "3. dealing with quotes (not implemented)\n",
    "\n",
    "\n",
    "conslusion : The preprocessing steps will change for each dataset . The best thing to do is lowercase the words \n",
    "get the initail dataset coverage and out of vocab words . Then check for contractions, punck. \n",
    "\n",
    "The glove embeddings has abt. 3000 digit and most of the punctuations in the word embeddings.\n",
    "\n",
    "Then check for if there still words that can  be found in the word embedings the do mispelling corrections,\n",
    "then check for acrnims\n",
    "\n",
    "## The Work FLow\n",
    "\n",
    "\n",
    "The first thing to do is know what words are present in your embeddings. Seperate them by\n",
    "\n",
    "1. words \n",
    "2. chars \n",
    "3. punctuations \n",
    "4. non-ascii\n",
    "5. numbers \n",
    "\n",
    "\n",
    "Then apply the following preprocessing steps \n",
    "\n",
    "\n",
    "1. while using embeddings adding space to punctuactions works the best eg it's to it ' s\n",
    "2. dealing with numbers by adding # doubles word coverage \n",
    "3. mapping misspelled words didnt help much in my case \n",
    "4. mapping acronims\n",
    "\n",
    "\n",
    "\n",
    "Best results obtained are total text coverage of 96.71 and unique word coverage is  36.62%.\n",
    "\n",
    "\n",
    "Then the next thing to do is filter out the words which are  oov.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
