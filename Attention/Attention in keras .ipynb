{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Attention Mechanisms in keras \n",
    "\n",
    "1. Simple Attention \n",
    "2. self Attention\n",
    "3. Attention with context \n",
    "4. Additive Attention \n",
    "5. Scaled Attention \n",
    "6. MultiHeadAttention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import * \n",
    "from keras.models import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Attention : Bahdanau et a\n",
    "    \n",
    "research papper : https://arxiv.org/abs/1409.0473    \n",
    "    \n",
    "#### Explaination\n",
    "\n",
    "input - shape : (batch_size,step_size,lstm_outdim):\n",
    "\n",
    "\n",
    "output - shape : (batch_size,step_size,1)\n",
    "\n",
    "\n",
    "W = weight matrix of shape (lstm_outdim X 1)\n",
    "\n",
    "B = bias vector of shape (step_size X 1) (optional)\n",
    "\n",
    "\n",
    "#### Steps for calculating attetnion \n",
    "\n",
    "Ignoring the batch_size,performing attention using only a dense layer and optional bias \n",
    "\n",
    "\n",
    "op1 = input X W     :output (step_size X 1)\n",
    "\n",
    "op2 = tanh(op1)\n",
    "\n",
    "op3 = sigmoid(op2)\n",
    "\n",
    "final_op = op3 * input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 200, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 300])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# \n",
    "class Attention(Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        \n",
    "        self.W = self.add_weight(\n",
    "                                shape=(input_shape[-1],1),\n",
    "                                initializer=\"he_normal\",\n",
    "                                dtype = \"float32\",    \n",
    "                                trainable = True,\n",
    "                                name = \"W\"\n",
    "                                )\n",
    "        self.B = self.add_weight(\n",
    "                                shape=(input_shape[1],1),\n",
    "                                initializer=\"ones\",\n",
    "                                trainable = True,\n",
    "                                name = \"B\"\n",
    "                                )           \n",
    "        \n",
    "        super(Attention, self).build(input_shape)    \n",
    "        \n",
    "\n",
    "    def call(self, x,bias = False):\n",
    "\n",
    "    \n",
    "        op1 = tf.matmul(x,self.W)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if bias:\n",
    "            op1 = tf.add(op1,self.B)\n",
    "        \n",
    "        \n",
    "        op2 = tf.tanh(op1)\n",
    "    \n",
    "        op3 = tf.math.softmax(op2)\n",
    "        \n",
    "        final_op = x * op3\n",
    "        \n",
    "        return tf.reduce_sum(final_op,axis = 1)\n",
    "    \n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()\n",
    "    \n",
    "        \n",
    "x = tf.constant(np.random.random((100,200,300)),dtype = 'float32')\n",
    "print(x.shape)        \n",
    "Attention()(x,bias = True).shape  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self attention\n",
    "\n",
    "\n",
    "Formula a = softmax(Ws2 * tanh ( Ws1 * X.T))\n",
    "\n",
    "here we introduce ws2 matrix \n",
    "\n",
    "Earlier the attention was 1D now we make it 2D\n",
    "this helps focous on the multile components of a \n",
    "sentence \n",
    "\n",
    "\n",
    "x - hidden states of \n",
    "\n",
    "Here r is the hyperparameter\n",
    "\n",
    "H = (batch_size,step,dim)\n",
    "Ws2 = (r,da)\n",
    "Ws1 = (da,dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "op1 = Ws1 * H.T : output (da,step)\n",
    "\n",
    "op2 = tanh(op1) : output (da,step)\n",
    "\n",
    "op3 = Ws2 * op2 : output (r,step)\n",
    "\n",
    "softmax is performed along the second dimension \n",
    "that along the columnn . look at the shape of \n",
    "op3 its (r,batch_size) where r is the hyperparametr\n",
    ". its the parts being extracted , the parts the network thinks are important. So each column \n",
    "holds attention for one input sentence which.\n",
    "This weights need to sum up to 1. They act as probability saying which word in the sentence is more important.\n",
    "\n",
    "A = softmax(op3) - will need to look into this \n",
    "\n",
    "\n",
    "M = A * H :output (r * step)\n",
    "\n",
    "M is embedding matrix.\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 200, 300)\n",
      "tracking <tf.Variable 'WS1:0' shape=(60, 300) dtype=float32, numpy=\n",
      "array([[-0.22197285, -0.11108026, -0.06221244, ...,  0.2962583 ,\n",
      "         0.12028784,  0.03872958],\n",
      "       [ 0.0748764 ,  0.18606624, -0.10900351, ...,  0.28110692,\n",
      "        -0.03352141, -0.10248536],\n",
      "       [-0.09674872, -0.15980174,  0.12203795, ...,  0.0649488 ,\n",
      "        -0.2368743 , -0.15283988],\n",
      "       ...,\n",
      "       [ 0.19190738,  0.04186773,  0.24856421, ...,  0.10236433,\n",
      "         0.19191793, -0.17187218],\n",
      "       [ 0.01786709,  0.00816062,  0.1528886 , ...,  0.2369453 ,\n",
      "        -0.01376432,  0.07767013],\n",
      "       [ 0.26486418,  0.20375934,  0.00499254, ..., -0.14374104,\n",
      "         0.19917867, -0.27636725]], dtype=float32)> Ws1\n",
      "tracking <tf.Variable 'WS2:0' shape=(30, 60) dtype=float32, numpy=\n",
      "array([[-0.27931112,  0.24099696, -0.35254106, ...,  0.10023117,\n",
      "        -0.18901902,  0.21257728],\n",
      "       [ 0.4144494 ,  0.0030632 , -0.33869252, ...,  0.24227178,\n",
      "         0.22474116, -0.35860145],\n",
      "       [ 0.1715061 , -0.07784823, -0.04774049, ...,  0.18269247,\n",
      "         0.41528332,  0.31116933],\n",
      "       ...,\n",
      "       [ 0.0912438 , -0.36877403, -0.3012516 , ..., -0.26473635,\n",
      "         0.40924382,  0.36647916],\n",
      "       [-0.41275653, -0.1780039 , -0.14771926, ...,  0.21862096,\n",
      "         0.15219164,  0.00936767],\n",
      "       [-0.17956862, -0.01490635,  0.34467047, ...,  0.43465853,\n",
      "        -0.08636281,  0.06571996]], dtype=float32)> Ws2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# \n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self,batch_size,u,r,da):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        H = (batch_size,step,dim) or\n",
    "        H = (batch_size,dim) - selecting this one \n",
    "        Ws2 = (r,da) \n",
    "        Ws1 = (da,dim)  \n",
    "        \"\"\"\n",
    "        \n",
    "        self.da = da\n",
    "        self.r = r\n",
    "        self.u = u\n",
    "        init = tf.initializers.he_uniform()\n",
    "        self.d = u\n",
    "        \n",
    "        \n",
    "        self.Ws1 = tf.Variable(initial_value=init(shape=(self.da, u), dtype=\"float32\"),trainable=True,name=\"WS1\")\n",
    "        \n",
    "        self.Ws2 = tf.Variable(initial_value = init(shape = (self.r,self.da),dtype = \"float32\"),trainable = True,name = \"WS2\")\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "\n",
    "       \n",
    "\n",
    "    \n",
    "        op1 = tf.matmul(self.Ws1,tf.transpose(tf.reshape(x,(-1,self.u))))\n",
    "        \n",
    "        \n",
    "    \n",
    "        op2 = tf.tanh(op1)\n",
    "       \n",
    "        \n",
    "        op3 = tf.matmul(self.Ws2,op2) # output shape : (30,200)\n",
    "        \n",
    "        op3 = tf.reshape(op3,(100,200,-1))\n",
    "        \n",
    "         \n",
    "        \n",
    "    \n",
    "        attention_weights = tf.math.softmax(op3,1)\n",
    "        \n",
    "    \n",
    "#         print(attention_weights.shape)\n",
    "\n",
    "        final_op = tf.matmul(tf.transpose(tf.reduce_sum(attention_weights,0)),tf.transpose(x))\n",
    "        \n",
    "        \n",
    "        #final output should be batch_size X r X U\n",
    "        \n",
    "        \n",
    "        return tf.transpose(final_op),attention_weights\n",
    "    \n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()\n",
    "    \n",
    "        \n",
    "x = tf.constant(np.random.random((100,200,300)),dtype = 'float32')\n",
    "print(x.shape)        \n",
    "\n",
    "_,att = SelfAttention(100,300,30,60).call(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0000004>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(att[0,:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention With Context \n",
    "Paper : Hierarchical Attention Networks for Document Classification\n",
    "\n",
    "### This Attention gets better score than simple mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#my implementation of Attention with context \n",
    "\n",
    "class AttentionContext(Layer):\n",
    "    \n",
    "    \n",
    "    def _init__(self):\n",
    "        super(AttentionContext,self).__init__()\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        \n",
    "        '''\n",
    "        we need to initialize 2 matrices and 1 bias vector.\n",
    "        The extra  matrix is the context which is multiplied to the energies \n",
    "        \n",
    "        Here the matrix W will have shape (dim x dim) where dim is the feature dim.\n",
    "        \n",
    "        The final output need to remain same as the simple attention to achieve this \n",
    "        the vector W is converted into matrix \n",
    "        \n",
    "        The bias will also have different dimensions from the original attention mechanism\n",
    "        but will will still be a vector \n",
    "        \n",
    "        \n",
    "        The U vector which is the main change here is called the context vector \n",
    "        responsiable for  finding out important information from the energies \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.W = self.add_weight(\n",
    "                                shape = (input_shape[-1],input_shape[-1]),\n",
    "                                initializer = \"he_normal\",\n",
    "                                dtype = 'float32',\n",
    "                                trainable = True,\n",
    "                                name = \"W\"\n",
    "                                )\n",
    "        \n",
    "        self.B = self.add_weight(\n",
    "                                shape = (input_shape[-1],),\n",
    "                                initializer = \"ones\",\n",
    "                                dtype = 'float32',\n",
    "                                trainable = True,\n",
    "                                name = \"B\"\n",
    "                                )\n",
    "        \n",
    "        self.U = self.add_weight(\n",
    "                                shape = (input_shape[-1],),\n",
    "                                initializer = \"he_normal\",\n",
    "                                dtype = 'float32',\n",
    "                                trainable = True,\n",
    "                                name = \"B\"\n",
    "                                )\n",
    "        \n",
    "        \n",
    "    # I am not implementing mask \n",
    "    def call(self,x,bias = True):\n",
    "            \n",
    "            \n",
    "            assert len(x.shape) == 3\n",
    "            \n",
    "            energies = K.squeeze(K.dot(x, K.expand_dims(self.W)),axis = -1)  # output (batch_size,steps,dims)\n",
    "            \n",
    "#             print(energies.shape)   \n",
    "            \n",
    "            if bias:\n",
    "                \n",
    "                energies += self.B     # output (batch_size,steps,dims)       \n",
    "            \n",
    "            energies = K.tanh(energies)\n",
    "            \n",
    "            print(energies.shape) \n",
    "            \n",
    "            context_vector = K.dot(energies,K.expand_dims(self.U))# output (batch_size,steps,1)\n",
    "            \n",
    "            \n",
    "            attention_weights = K.softmax(context_vector,axis = 1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            context = x * attention_weights \n",
    "\n",
    "            \n",
    "            return   K.sum(context,axis = 1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 200, 300)\n"
     ]
    }
   ],
   "source": [
    "input = tf.constant(np.random.random((100,200,300)),dtype = \"float32\")            \n",
    "        \n",
    "att = AttentionContext() \n",
    "att.build(input.shape)\n",
    "context = att.call(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 300])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the build function we dont need to specify the input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190, 2000, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'attention_context_41/Sum:0' shape=(190, 300) dtype=float32>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tf.constant(np.random.random((190,2000,300)),dtype = \"float32\")            \n",
    "AttentionContext()(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additive attention \n",
    "\n",
    "### formula energies e  = V * tanh(W1 * q + W2 * v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (100, 200, 300)\n",
      "Q: (100, 10)\n",
      "V: (100, 200, 10)\n",
      "ADD: (100, 200, 10)\n",
      "Score: (100, 200, 1)\n",
      "context_vector: (100, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 300), dtype=float32, numpy=\n",
       "array([[-0.10166631, -0.5937331 ,  0.6985316 , ..., -1.2327119 ,\n",
       "        -2.0880919 ,  0.46112052],\n",
       "       [ 0.2643503 ,  0.224652  ,  0.72448575, ..., -0.71220255,\n",
       "         0.9383941 ,  0.44116318],\n",
       "       [ 0.14608662,  0.32352874, -0.03091997, ..., -0.06793836,\n",
       "        -0.31154707, -0.10095207],\n",
       "       ...,\n",
       "       [-0.22122407,  0.07794187, -0.26526845, ..., -0.6523165 ,\n",
       "         0.4636118 ,  0.35176542],\n",
       "       [ 0.0133392 , -0.1013723 , -0.03963846, ..., -0.3964976 ,\n",
       "         0.08073253, -0.10223395],\n",
       "       [ 0.1357487 ,  0.19757317,  0.07662726, ..., -0.11371756,\n",
       "         0.18536097, -0.5483618 ]], dtype=float32)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#with attention \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    \n",
    "      def __init__(self, units):\n",
    "            super(BahdanauAttention, self).__init__()\n",
    "            self.units = units\n",
    "    \n",
    "    \n",
    "      def build(self,input_shape):   \n",
    "            \n",
    "            print('input_shape',input_shape)\n",
    "    \n",
    "            self.W1 = self.add_weight(\n",
    "                                     shape = (input_shape[-1],self.units),\n",
    "                                     initializer = 'he_normal',\n",
    "                                     dtype = 'float32',\n",
    "                                     trainable = True,\n",
    "                                     name = \"W1\"\n",
    "                                    )\n",
    "        \n",
    "            self.W2 = self.add_weight(\n",
    "                                      shape = (input_shape[-1],self.units),\n",
    "                                      initializer = 'he_normal',\n",
    "                                      dtype = 'float32',\n",
    "                                      trainable = True,\n",
    "                                      name = \"W2\"\n",
    "                                     )\n",
    "            self.V1 =  self.add_weight(shape = (self.units,1),\n",
    "                                     initializer = 'he_normal',\n",
    "                                     dtype = 'float32',\n",
    "                                     trainable = True,\n",
    "                                     name = \"V\"\n",
    "                                    )\n",
    "            \n",
    "            \n",
    "#             print('w2',self.W2.shape)\n",
    "                                     \n",
    "\n",
    "      def call(self, query, values):\n",
    "    \n",
    "    \n",
    "            '''\n",
    "            value  - (batch_size,steps,dim)\n",
    "            query  - (batch_size,1,dim) - > expanded query\n",
    "\n",
    "            dense layer only changes the last layer \n",
    "\n",
    "            op1 =  W1 * query  =>  (batch_size,1,units)\n",
    "\n",
    "            op2 = W2 * values =>  (batch_size,steps,units)  \n",
    "\n",
    "            ADD =  op1 + op2 during this op1 will be broadcasted to match shape of op2 => (batch_size,steps,units)\n",
    "\n",
    "            op3 = tanh(ADD)\n",
    "\n",
    "            energies = V * op3 => (batch_size,steps,1)\n",
    "\n",
    "            normalizerd_energies/ attention_weights = softmax(energises) => (batch_size,steps,1)\n",
    "\n",
    "            The reason we normalzie is to maked the dot product of long sequences have similar magnited of dot of small vectors \n",
    "\n",
    "            context_vectors = x * attention_weights  =>(batch_size,steps,dims)\n",
    "\n",
    "            return context_vectors \n",
    "\n",
    "\n",
    "            '''\n",
    "    \n",
    "    \n",
    "            assert len(query.shape) == 2\n",
    "            assert len(values.shape) == 3\n",
    "    \n",
    "\n",
    "#             expanded_query = tf.expand_dims(query, axsi = 1)\n",
    "            \n",
    "            Q = K.dot(query,self.W1) # output: (batch_size,units)\n",
    "            \n",
    "            print('Q:',Q.shape)\n",
    "            \n",
    "            V = K.dot(values,self.W2) # output: (batch_size,steps,units)\n",
    "            \n",
    "            print('V:',V.shape)\n",
    "            \n",
    "               \n",
    "            ADD = K.expand_dims(Q,axis = 1) + V # output : (batch_size,steps,units)\n",
    "            \n",
    "            print('ADD:',ADD.shape)\n",
    "                \n",
    "            score = K.dot(ADD,self.V1)\n",
    "            \n",
    "            print('Score:',score.shape)\n",
    "\n",
    "    \n",
    "            # attention_weights shape == (batch_size, max_length, 1)\n",
    "            attention_weights = K.softmax(score, axis=1)\n",
    "\n",
    "            # context_vector shape after sum == (batch_size, hidden_size)\n",
    "            context_vector = attention_weights * values\n",
    "            context_vector = K.sum(context_vector, axis=1)\n",
    "\n",
    "            \n",
    "            print('context_vector:',context_vector.shape)\n",
    "            \n",
    "            return context_vector\n",
    "\n",
    "\n",
    "values = tf.constant(np.random.normal(0,1,(100,200,300)),dtype = \"float32\")  \n",
    "query = tf.constant(np.random.normal(0,1,(100,300)),dtype = \"float32\") \n",
    "activation_layer = BahdanauAttention(10)\n",
    "\n",
    "activation_layer.build(values.shape)\n",
    "\n",
    "activation_layer.call(query,values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled dot product attention\n",
    "\n",
    "\n",
    "\n",
    "We have 3 input vectors queries , keys and values. Queries and keys have same dimenstions \n",
    "We require 3 matrices Wq,Wk,Wq.\n",
    "\n",
    "Q = Wq * q \n",
    "K = Wk * k\n",
    "V = Wv * v\n",
    "attention =  sigmoid((Q * K) / sqrt(d)) * V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 64)\n",
      "(100, 200, 64)\n"
     ]
    }
   ],
   "source": [
    "class scaled_dot_product_attention(Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self,dim):\n",
    "        \n",
    "        super(scaled_dot_product_attention,self).__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        input_shape = (batch_size,steps,dims)\n",
    "        \n",
    "        steps = dmodel\n",
    "        \n",
    "        dims = dk\n",
    "        \n",
    "        matirx Wq,Wk,Wv\n",
    "        \n",
    "        shape of all is same dmodel x dk for Wq and Wk\n",
    "        and dmodel x dv for Wv\n",
    "        \n",
    "        we take dk == dv but they can be different.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.Wq = self.add_weight( \n",
    "                                  shape = (input_shape[-1],self.dim),\n",
    "                                  initializer = \"he_normal\",\n",
    "                                  dtype = \"float32\",\n",
    "                                  trainable = True,\n",
    "                                  name = \"Wq\"  \n",
    "                                 ) \n",
    "        \n",
    "        self.Wk = self.add_weight( \n",
    "                                  shape = (input_shape[-1],self.dim),\n",
    "                                  initializer = \"he_normal\",\n",
    "                                  dtype = \"float32\",\n",
    "                                  trainable = True,\n",
    "                                  name = \"Wk\"  \n",
    "                                 ) \n",
    "        \n",
    "        \n",
    "        self.Wv = self.add_weight( \n",
    "                                  shape = (input_shape[-1],self.dim),\n",
    "                                  initializer = \"he_normal\",\n",
    "                                  dtype = \"float32\",\n",
    "                                  trainable = True,\n",
    "                                  name = \"Wv\"  \n",
    "                                 ) \n",
    "        \n",
    "        \n",
    "        print(self.Wv.shape)\n",
    "    \n",
    "        \n",
    "    def call(self,query,key,value):\n",
    "        \n",
    "        '''\n",
    "        For classification tast query == value == key \n",
    "        key and value can be the same input.\n",
    "        \n",
    "        query shape = (batch_size,steps,dims)\n",
    "        '''\n",
    "        \n",
    "        input_shape = query.shape\n",
    "        \n",
    "        Q = tf.matmul(query,self.Wq) #output: (batch_size,steps,step)\n",
    "        \n",
    "        K = tf.matmul(query,self.Wk) #output: (batch_size,steps,step)\n",
    "        \n",
    "        V = tf.matmul(query,self.Wv) #output: (batch_size,steps,step)\n",
    "        \n",
    "        t1 =tf.divide(tf.matmul(Q,K,transpose_b = True),np.sqrt(input_shape[-1]))\n",
    "        \n",
    "        t1 = tf.math.softmax(t1,axis = 1)\n",
    "        \n",
    "        attention  = tf.matmul(t1,V)\n",
    "        \n",
    "        \n",
    "        print(attention.shape)\n",
    "        \n",
    "        \n",
    "x = tf.constant(np.random.normal(0,1,(100,200,300)),dtype = \"float32\")  \n",
    "# query = tf.constant(np.random.normal(0,1,(100,300)),dtype = \"float32\") \n",
    "activation_layer = scaled_dot_product_attention(64)\n",
    "\n",
    "activation_layer.build(x.shape)\n",
    "\n",
    "activation_layer.call(x,x,x)        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 200, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'multi_head_attention_2/MatMul_5:0' shape=(100, 200, 300) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self,head,dim):\n",
    "        \n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.head = head\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        input_shape = (batch_size,steps,dims)\n",
    "        \n",
    "        dims = dmodel\n",
    "        \n",
    "        dk = units \n",
    "        \n",
    "        matirx Wq,Wk,Wv\n",
    "        \n",
    "        shape of all is same dmodel x dk for Wq and Wk\n",
    "        and dmodel x dv for Wv\n",
    "        \n",
    "        we take dk == dv but they can be different.\n",
    "        \n",
    "        \n",
    "        output of the embedding 100,75,300\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.Wq = self.add_weight( \n",
    "                                  shape = (input_shape[0][-1],self.dim * self.head),\n",
    "                                  initializer = \"he_normal\",\n",
    "                                  dtype = \"float32\",\n",
    "                                  trainable = True,\n",
    "                                  name = \"Wq\"  \n",
    "                                 ) \n",
    "        \n",
    "        self.Wk = self.add_weight( \n",
    "                                  shape = (input_shape[1][-1],self.dim * self.head),\n",
    "                                  initializer = \"he_normal\",\n",
    "                                  dtype = \"float32\",\n",
    "                                  trainable = True,\n",
    "                                  name = \"Wk\"  \n",
    "                                 ) \n",
    "        \n",
    "        \n",
    "        self.Wv = self.add_weight( \n",
    "                                  shape = (input_shape[2][-1],self.dim * self.head),\n",
    "                                  initializer = \"he_normal\",\n",
    "                                  dtype = \"float32\",\n",
    "                                  trainable = True,\n",
    "                                  name = \"Wv\"  \n",
    "                                 ) \n",
    "        \n",
    "        self.Wo = self.add_weight( \n",
    "                                  shape = (self.head* self.dim,input_shape[0][-1]),\n",
    "                                  initializer = \"he_normal\",\n",
    "                                  dtype = \"float32\",\n",
    "                                  trainable = True,\n",
    "                                  name = \"Wo\"  \n",
    "                                 )  \n",
    "        \n",
    "#         print(self.Wv.shape)\n",
    "\n",
    "        super(MultiHeadAttention,self).build(input_shape) \n",
    "    \n",
    "        \n",
    "    def call(self,x):\n",
    "        \n",
    "        '''\n",
    "        For classification tast query == value == key \n",
    "        key and value can be the same input.\n",
    "        \n",
    "        query shape = (batch_size,steps,dims)\n",
    "        '''\n",
    "        \n",
    "        query,key,value = x\n",
    "        \n",
    "        Q = tf.matmul(query,self.Wq) #output: (batch_size,steps,step)\n",
    "        \n",
    "        K = tf.matmul(query,self.Wk) #output: (batch_size,steps,step)\n",
    "        \n",
    "        V = tf.matmul(query,self.Wv) #output: (batch_size,steps,step)\n",
    "        \n",
    "        t1 =tf.divide(tf.matmul(Q,K,transpose_b = True),np.sqrt(self.dim))\n",
    "        \n",
    "        t1 = tf.math.softmax(t1,axis = 1)\n",
    "        \n",
    "        head  = tf.matmul(t1,V)\n",
    "        \n",
    "        context_vector = tf.matmul(head,self.Wo)\n",
    "        \n",
    "        \n",
    "#         print(head.shape)\n",
    "#         print(self.Wo.shape)\n",
    "\n",
    "        print(context_vector.shape)\n",
    "        \n",
    "        return context_vector\n",
    "    \n",
    "x = tf.constant(np.random.normal(0,1,(100,200,300)),dtype = \"float32\")  \n",
    "# query = tf.constant(np.random.normal(0,1,(100,300)),dtype = \"float32\") \n",
    "# activation_layer = MultiHeadAttention(3,64)\n",
    "\n",
    "# activation_layer.build(x.shape)\n",
    "\n",
    "# activation_layer.call(x,x,x)        \n",
    "        \n",
    "MultiHeadAttention(3,64)([x,x,x])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
