{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d2dbdb3-6c74-4f96-9865-2951dfd653ce",
    "_uuid": "bb41ad86b25fecf332927b0c8f55dd710101e33f"
   },
   "source": [
    "# Implementing Different Meta Emdedding \n",
    "\n",
    "The dataset used here is ***jlgsaw-toxic-comment-classification-challenge*** it's a multi-label problem. Where a comment can be in one more of the 6 different class.\n",
    "\n",
    "Meta-Embeddings:\n",
    "1. ConCatenation \n",
    "2. Average \n",
    "3. DME\n",
    "4. PME\n",
    "\n",
    "\n",
    "Glove and Paragrams have been used for the demo purposes.\n",
    "\n",
    "\n",
    "Using the final implementation : Concatenation + Attention I was able to get 98% on the test set.\n",
    "\n",
    "\n",
    "Note: I have not used any special preprocessing steps here just the tokeizer as I just wanted to show code for implementing different meta-embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "2f9b7a76-8625-443d-811f-8f49781aef81",
    "_uuid": "598f965bc881cfe6605d92903b758778d400fa8b"
   },
   "outputs": [],
   "source": [
    "\n",
    "#imports \n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd,gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,Reshape,multiply,Lambda\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,Concatenate,Layer,InputSpec\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import *\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c297fa80-beea-464b-ac90-f380ebdb02fe",
    "_uuid": "d961885dfde18796893922f72ade1bf64456404e"
   },
   "source": [
    "We include the GloVe word vectors in our input files. To include these in your kernel, simple click 'input files' at the top of the notebook, and search 'glove' in the 'datasets' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "66a6b5fd-93f0-4f95-ad62-3253815059ba",
    "_uuid": "729b0f0c2a02c678631b8c072d62ff46146a82ef"
   },
   "outputs": [],
   "source": [
    "#specify \n",
    "\n",
    "path = '../input/'\n",
    "comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "TRAIN_DATA_FILE=f'{path}{comp}train.csv.zip'\n",
    "TEST_DATA_FILE=f'{path}{comp}test.csv.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "98f2b724-7d97-4da8-8b22-52164463a942",
    "_uuid": "b62d39216c8d00b3e6b78b825212fd190757dff9"
   },
   "source": [
    "Set some basic config parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "2807a0a5-2220-4af6-92d6-4a7100307de2",
    "_uuid": "d365d5f8d9292bb9bf57d21d6186f8b619cbe8c3"
   },
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b3a8d783-95c2-4819-9897-1320e3295183",
    "_uuid": "4dd8a02e7ef983f10ec9315721c6dda2958024af"
   },
   "source": [
    "Read in our data and replace missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "ac2e165b-1f6e-4e69-8acf-5ad7674fafc3",
    "_uuid": "8ab6dad952c65e9afcf16e43c4043179ef288780"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "54a7a34e-6549-45f7-ada2-2173ff2ce5ea",
    "_uuid": "e8810c303980f41dbe0543e1c15d35acbdd8428f"
   },
   "source": [
    "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79afc0e9-b5f0-42a2-9257-a72458e91dbb",
    "_uuid": "c292c2830522bfe59d281ecac19f3a9415c07155"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8c4f6a3-3a19-40b1-ad31-6df2690bec8a",
    "_uuid": "e1cb77629e35c2b5b28288b4d6048a86dda04d78"
   },
   "source": [
    "I my only going to use glove and paragram from creating meta-embeddings. An additional way to increase performance is to preprocess the data to increase the coverage of the vector embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common method to get embedding weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### the below fun\n",
    "def get_matrix(path,tokenizer,type = \"glove\"):\n",
    "    \n",
    "    \n",
    "    ##### loading the embeddings from the files:\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    if type == \"glove\":\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in tqdm(open(path, encoding=\"utf8\", errors='ignore')))\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in tqdm(open(path, encoding=\"utf8\", errors='ignore')) if len(o)>100)\n",
    "    \n",
    "    print('done reading vectors....')\n",
    "    \n",
    "    #calculating the mean and std.\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    \n",
    "    #calculating the shape \n",
    "    embed_size = all_embs.shape[1]\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index) + 1)\n",
    "    \n",
    "    #defining the embedding_matirx \n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "\n",
    "    #loading the vectors.    \n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "    del all_embs,embeddings_index\n",
    "\n",
    "\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7370416a-094a-4dc7-84fa-bdbf469f6579",
    "_uuid": "20cea54904ac1eece20874e9346905a59a604985"
   },
   "source": [
    "Use these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Glove Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [05:16, 6935.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done reading vectors....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210337/210337 [00:00<00:00, 694256.97it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_embedding = get_matrix('../input/embeddings/glove-840B-300d.txt',tokenizer,type = 'glove')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Para Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1703756it [03:59, 7101.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done reading vectors....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210337/210337 [00:00<00:00, 729967.91it/s]\n"
     ]
    }
   ],
   "source": [
    "para_embedding = get_matrix('../input/embeddings/paragram-300-sl999.txt',tokenizer,type = 'glove')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Meta Embedding \n",
    "\n",
    "1. Concatanation Matrix \n",
    "2. Average Matrix \n",
    "3. CDME_BLOCK\n",
    "4. PME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "62acac54-0495-4a26-ab63-2520d05b3e19",
    "_uuid": "574c91e270add444a7bc8175440274bdd83b7173"
   },
   "outputs": [],
   "source": [
    "cancatnation_matrix = np.concatenate([glove_embedding,para_embedding],axis = -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size * 2 , weights=[cancatnation_matrix])(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)\n",
    "x1 = GlobalMaxPool1D()(x)\n",
    "x2 = GlobalMaxPool1D()(x)\n",
    "x = Concatenate(axis=-1)([x1, x2])\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      "143613/143613 [==============================] - 1903s 13ms/step - loss: 0.0548 - acc: 0.9806 - val_loss: 0.0467 - val_acc: 0.9825\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_t, y, batch_size=64, epochs=1, validation_split=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Averaging \n",
    "For average meta embedding the embed_size remain the same embed_size = 300\n",
    "This technique reduces the time but also reduces the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_matrix = (cancatnation_matrix[:,:300] + cancatnation_matrix[:,300:]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size , weights=[average_matrix])(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)\n",
    "x1 = GlobalMaxPool1D()(x)\n",
    "x2 = GlobalMaxPool1D()(x)\n",
    "x = Concatenate(axis=-1)([x1, x2])\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_t, y, batch_size=64, epochs=2, validation_split=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f1aeec65-356e-4430-b99d-bb516ec90b09",
    "_uuid": "237345510bd2e664b5c6983a698d80bac2732bc4"
   },
   "source": [
    "# DME\n",
    "For dme we have a make use of a seperate block which takes in three parameters (input , maxlen and n_embed)\n",
    "\n",
    "where \n",
    "        1. n_embed = the no. of embeddings used in our case 2 glove and paragram\n",
    "        2. maxlen = maxlen of each input sample \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dme\n",
    "from keras.layers import Activation\n",
    "# from keras.layers import multiply, Lambda\n",
    "# import keras.backend as K\n",
    "\n",
    "def DME_Block(inp, maxlen,n_emb):\n",
    "    \"\"\"\n",
    "    # inp = tensor of shape (?,maxlen,embedding dim,n_emb)) n_emb is number of embedding matrices\n",
    "    # out = tensor of shape (?,maxlen,embedding dim)\n",
    "    \"\"\"\n",
    "   \n",
    "    x = Reshape((maxlen,-1))(inp)\n",
    "    x = LSTM(n_emb,return_sequences = True)(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    \n",
    "    x = Reshape((maxlen,1,n_emb))(x)\n",
    "    \n",
    "    x = multiply([inp, x])\n",
    "    \n",
    "    out = Lambda(lambda x: K.sum(x, axis=-1))(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lambda_1/Sum:0' shape=(32, 100, 600) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tf.constant(np.random.rand(32,100,600,3),dtype = 'float32')\n",
    "\n",
    "DME_Block(input,100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size * 2 , weights=[cancatnation_matrix])(inp)\n",
    "print(x)\n",
    "x = DME_Block(x,maxlen,2)\n",
    "x = SpatialDropout1D(0.3)(x)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)\n",
    "x1 = GlobalMaxPool1D()(x)\n",
    "x2 = GlobalMaxPool1D()(x)\n",
    "x = Concatenate(axis=-1)([x1, x2])\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PME Projection Meta Embedding(PME)\n",
    "\n",
    "1. Concatenate different embedding matrix and feed it thought the embedding layer \n",
    "2. then use a linear layer to project the vector dimension to a lower space \n",
    "3. then use a rlu thats it the ouptut then goes through a LSTM or GRU Model (classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size * 2 , weights=[cancatnation_matrix])(inp)\n",
    "x = Dense(300,activation = 'relu')(x)\n",
    "x = SpatialDropout1D(0.3)(x)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)\n",
    "x1 = GlobalMaxPool1D()(x)\n",
    "x2 = GlobalMaxPool1D()(x)\n",
    "x = Concatenate(axis=-1)([x1, x2])\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1650s 11ms/step - loss: 0.0599 - acc: 0.9792 - val_loss: 0.0486 - val_acc: 0.9825\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1640s 11ms/step - loss: 0.0460 - acc: 0.9827 - val_loss: 0.0482 - val_acc: 0.9830\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_t, y, batch_size=64, epochs=2, validation_split=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Attention + Concatanation Meta-Embedding \n",
    "\n",
    "This model got me the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        \n",
    "        self.trainable_weights = [self.W]\n",
    "        \n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        \n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "            \n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        \n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        \n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        \n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0d4cb718-7f9a-4eab-acda-8f55b4712439",
    "_uuid": "dc51af0bd046e1eccc29111a8e2d77bdf7c60d28"
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size * 2 , weights=[cancatnation_matrix])(inp)\n",
    "# x = Dense(300)(x)\n",
    "# x = Relu()(x)\n",
    "# x = SpatialDropout1D(0.3)(x)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "# x1 = GlobalMaxPool1D()(x)\n",
    "# x2 = GlobalMaxPool1D()(x)\n",
    "x3 = AttentionWeightedAverage()(x)\n",
    "\n",
    "# x = Concatenate(axis=-1)([x1, x2,x3])\n",
    "x = Dense(50, activation=\"relu\")(x3)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "333626f1-a838-4fea-af99-0c78f1ef5f5c",
    "_uuid": "c1558c6b2802fc632edc4510c074555a590efbd8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_t, y, batch_size=64, epochs=2, validation_split=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "28ce30e3-0f21-48e5-af3c-7e5512c9fbdc",
    "_uuid": "e59ad8a98ac5bb25a6bddd72718f3ed8a7fb52e0"
   },
   "outputs": [],
   "source": [
    "y_test = model.predict([X_te], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv.zip')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv('only_attention.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "617e974a-57ee-436e-8484-0fb362306db2",
    "_uuid": "2b969bab77ab952ecd5abf2abe2596a0e23df251"
   },
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
